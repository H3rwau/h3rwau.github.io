[
  {
    "title": "index",
    "content": "--- title: 这里是H3rwau的私人笔记、博客、随笔 config: toc: false --- 人生当中成功只是一时的，失败却是主旋律。但是如何面对失败，却把人分成了不同的样子。 有的人会被失败击垮，有的人能够不断地爬起来继续向前。 我想真正的成熟并不是追求完美，而是直面自己的缺憾，这才是生活的本质。 罗曼罗兰说过的，这个世上只有一种真正的英雄主义，那就是认清生活的真相，并且仍然热爱它。 不要畏惧失败，不要在舒适区，勇敢去做，去尝试，去挑战，去成长。 ![](../assets/obito.jpg)",
    "url": "/index.html",
    "lang": ""
  },
  {
    "title": "读研这三年的感悟",
    "content": "--- title: 读研这三年的感悟 --- 一、未雨绸缪，不要走一步看一步，提前规划是非常重要的。 二、把要做的事情分维度等级，分成重要的，紧急的，以及这两个维度交叉的。我在这方面吃了亏，总是着急的把紧急的事情做完，而重要的却总是被忽略。 三、任何一件事情，不要自己考虑非常周到考虑到比较完美的时候才去做，而是要尽快开始，边做边想，边做边改，这样效率会高很多。并且人是不可能在预先准备阶段就把所有事情都考虑周全的，所以不要害怕犯错，错了就改，改了就继续做。先做一个垃圾出来，再在垃圾上改，最后才能做出一个精品。 四、不要把事情想的太复杂，要尽量简化，尽量简单。当然把事情做简单是非常难的。需要自己有足够的经验，才能把事情做的简单。 五、出来混最重要的是出来。 六、不要让自己闭门造车了，多问别人，多交流，多学习，多看别人是怎么做的，怎么想的，这样你才能学到东西，才能进步。",
    "url": "/随笔/读研这三年的感悟.html",
    "lang": ""
  },
  {
    "title": "优质博客列表",
    "content": "--- title: 优质博客列表 --- 优质博客",
    "url": "/随笔/优质博客列表.html",
    "lang": ""
  },
  {
    "title": "C++核心技术：C++惯用法",
    "content": "--- title: C++核心技术：C++惯用法 --- C++里有个惯用法，可以利用拷贝构造函数和交换函数(swap)来实现赋值运算符，在有了移动构造函数之后，这个惯用法还有一个额外的好处，既可以用一个赋值函数同时完成拷贝赋值和移动赋值两个动作。 c++ //例如，一个string类的赋值运算符可以这样实现: class string{ public : ..... String& operator=(String rhs){ rhs.swap(this);//不需要考虑自复制的情况了 return this; } void swap(String& rhs)noexcept{ using std::swap; swap(data,rhs.data); swap(size,rhs.size); swap(capacity, rhs.capacity); } private: unique ptr<char[]> data {}; sizet size{}; sizet capacity{}; }; 但是对于左值的赋值来说，会在有重复赋值时每次都去分配内存，产生不必要的开销 对于上面的String类，这样实现会更好 c++ class string { public: string& operator=(const String& rhs){ if(this != &rhs){ if(capacity<rhs.size ){ data =makeunique<char[]>(rhs.size+ 1); capacity=rhs.size ; } memcpy(data .get(),rhs.data.get()，rhs.size+ 1); size =rhs.size ; } return this; } String& operator=(String&& rhs)noexcept{ String(std::move(rhs)).swap(this); return this; } //如果使用频率很高，推荐进行这样的优化",
    "url": "/c++/C++核心技术：C++惯用法.html",
    "lang": ""
  },
  {
    "title": "C++核心技术：STL的查漏补缺",
    "content": "--- title: C++核心技术：STL的查漏补缺 --- c++ mp.insert({key,value}); mp.emplace(key,value); mp.tryemplace(key,value的构造参数) ; //性能最好 mp.insertorassign(key,value); mp[key] = value; c++ auto it = partition(c.begin(),c.end(),[] ( int x){return x< 100;});//将满足第三个参数条件的元素放到容器的前半部分 sort(c.begin(), it); c++ copy(c.begin(),c.end(), ostreamiterator<typename C::valuetype>(cout,\" \"));//通过typename C::valuetype来获得容器的元素类型 //或者在已知道类型时使用 <类型> c++ deque<int> d; copy(c.begin(),c.end(), backinserter(d)); c++ vector<int> d(c.size())//一定要保证目标元素存在 copy(c.begin(),c.end(), d.begin()); c++ transform(v.begin(), v.end(), v.begin(), []( int x){return x+2;});//自映射，在自身上面进行修改 transform(dq.begin(), dq.end(), backinserter(ls),[]( int x){return x+2;});//将映射结果插入到ls尾部 accumulate( C++98) 和reduce (C++ 17) 对于removeif，把不满足谓词的元素向前移，返回的迭代器指向位置之前的元素都不满足谓词，后续接erase操作来真正删除满足谓词的那些元素 从c++20起，有更简单的eraseif使用 c++ eraseif(s, [] (int n){return n%2!=0;}); c++ fill(begin(a),end(a),1); filln(first, n ,....);//在c++中，进行char和byte赋值完全可以代替memset //iota可以填充递增的值 iota(begin(a),end(a),1);// 1 2 3 4 ..... //generate 可以替代上述几个算法 generate(begin(a),end(a),[n=1] () mutable{return n++;}); c++ copybackward(c.begin(),c.begin()+4,c.end());//后四项用前四项来替代 //第三个参数指向的位置不应该在前面的区间内 //移动方法 c2.resize(c1.size()); copy(moveiterator(c1.begin()), moveiterator(c1.end()), c2.begin()); //或者 c2.reserve(c1.size()); copy(moveiterator(c1.begin()), moveiterator(c1.end()), backinserter(c2)); //还可以使用move move(c1.begin(),c1.end(),backinserter(c2)); //简单通用高效的方法 c2.insert(c2.end(),moveiterator(c1.begin()), moveiterator(c1.end())); copyn(begin(c),n,...) copyif()比起copy多了第四个参数，一个一元谓词 c++ it = find(c.begin(), c.end(), 0); it = findif(c.begin(), c.end(), []( int n) {return n==0}); //类似的还有count和countif allof/anyof/noneof //第三个参数接谓词 c++ //当我们数据有序时，只需要知道元素是否存在时可以使用 binarysearch(c.begin(),c.end(),0); lowerbound upperbound equalrange c++ //C++里与排序相关的功能不止sort(全排序)一种: //如果你需要根据某个标准来把数据分成两组，那partition 就够了。 //如果你需要找出序列里的前n项，保证前n项和后续元素的相对顺序(而不用对前n项或后面的元素进行排序)，那可以用nthelement。 //如果你需要找出序列里的前n项，并对前n项进行排序，那可以用partialsort。 //只有在你确实需要对所有元素都进行排序时，才需要使用sort。 //额外地，如果你不仅需要排序，还希望等价的元素(!compare(a，b)&&!compare(b，a))保持其原有的顺序，应当使用stablesort。 c++ std::randomdevice rd; std::mt19937 g(rd()); // 随机数引擎:基于梅森缠绕器算法的随机数生成器 std::shuffle(v.begin(), v.end(), g); // 打乱顺序，重新排序（随机序列） c++ minmax(a,b)直接返回最小值和最大值的引用 c++ std::execution::seq; //传统的顺序执行策略 std::execution::par; //并行执行策略，表示希望使用多个线程 std::execution::unseq; //无顺序执行策略，向量化执行 std::execution::parunseq; //并行无顺序，同时多线程和向量化 double res = reduce(execution::par, v.begin(),v.end()); //gcc 编译中需要 -ltbb c++ using std::ranges::copy; using std::ranges::sort; int a[]={1,2,4,3,7,6,5,2}; sort(a); copy(a,ostreamiterator<int>(cout,\" \"));",
    "url": "/c++/C++核心技术：STL的查漏补缺.html",
    "lang": ""
  },
  {
    "title": "C++核心技术：位图",
    "content": "--- title: C++核心技术：位图 --- bitset | (constructor) | constructs the bitset (public member function) | | ------------------------------------------------------------ | ------------------------------------------------------------ | | operator==operator!=(removed in C++20) | compares the contents (public member function) | | Element access | | | [operator[\\]](https://en.cppreference.com/w/cpp/utility/bitset/operatorat) | accesses specific bit (public member function) | | test | accesses specific bit (public member function) | | allanynone | checks if all, any or none of the bits are set to true (public member function) | | count | returns the number of bits set to true (public member function) | | Capacity | | | size | returns the number of bits that the bitset holds (public member function) | | Modifiers | | | operator&=operator\\|=operator^=operator | performs binary AND, OR, XOR and NOT (public member function) | | operator<<=operator>>=operator<> | performs binary shift left and shift right (public member function) | | set | sets bits to true or given value (public member function) | | reset | sets bits to false (public member function) | | flip | toggles the values of bits (public member function) | | Conversions | | | tostring | returns a string representation of the data (public member function) | | toulong | returns an unsigned long integer representation of the data (public member function) | | toullong(C++11) | returns an unsigned long long integer representation of the data (public member function) |",
    "url": "/c++/C++核心技术：位图.html",
    "lang": ""
  },
  {
    "title": "C++核心技术：协程",
    "content": "--- title: C++核心技术：协程 --- 从概念上讲，有栈协程和goroutine基本是一个概念，都是由用户自行调度的、有自己的独立的栈空间的运行单元。这样的却带你当然就是栈的空间占用和切换栈的开销了。 而无栈协程自己没有独立的栈空间，每个协程只需要一个很小的栈帧(一般而言需要分配到堆上)，空间占用小，也没有栈的切换开销。 C++20的协程是无栈的。部分原因是有栈协程可以使用纯库方式实现，而无栈协程需要一点编译器魔法帮忙。毕竟协程里面的局部变量都是要放到堆上而不是栈上。",
    "url": "/c++/C++核心技术：协程.html",
    "lang": ""
  },
  {
    "title": "C++核心技术：基础内容",
    "content": "--- title: C++核心技术：基础内容 --- 函数的声明或者定义前面不管加不加extern说明，函数都具有外部链接性，它们可以跨翻译单元使用。 函数前加上static说明，指示函数具有内部链接性，符号名不视为公开，只能在当前翻译单元里使用。 对象可以有名字（变量名），也可以没有名字（动态对象和字符串字面量） 在 C++ 中，static 的全局变量具有内部链接性（internal linkage），而不是外部链接性（external linkage）。这意味着 static 的全局变量只能在定义它的翻译单元（通常是源文件）中访问，不能在其他翻译单元中访问。 静态存储器和线程局部存储期的对象内容初始化是0。 自动存储期和动态存储期的对象内容不确定 字符字面量的类型是int而不是char sizeof('A')等同于sizeof(int) const表示其值不可更改，volatile表示其值可能会以意外的方式更改（因而会禁止优化操作）。volatile并没有多线程同步的语义，仅仅使用它是不够的，我们还需要原子量和新的内存模型。 c++中可以在头文件用const定义常量代替宏使用 C++可以有两个同名函数，c++编译器会为他们生成不同的符号名称，称为名字重整 C++的规则是临时对象不能匹配非const的引用 dynamiccast:这是C里面不存在的转型方式，用来在带有虚函数的“动态对象’继承树里进行指针或引用的类型转换。比如，如果有shape指针ptr，我们可以使用 dynamiccast<circle>(ptr)尝试把它转型成circle\\。系统会进行需要的类型检查，并在转型成功时返回一个非空指针，返回空指针则表示失败(如当ptr实际指向的不是 circle，而是Rectangle)。对于引用，转型失败则会抛异常。通常认为这种转型方式是安全的。 staticcast:这是“静态”转型方式，用在一些被认为较安全的场景下。你可以使用它在不同的数值类型之间进行转换，如从long到int，或者从 long long 到double--当转换有可能有精度损失时，就不能使用隐式类型转换，而得明确使用转型了。你也可以使用它把 void转换成实际类型的指针(如int\\)。你还可以用它把基类的指针转换成派生类的指针，前提条件是你能确认这个基类的指针确实指向一个派生类的对象。显然，对于最后一种场景，staticcast不如dynamic cast 安全，但由于 static cast 不需要进行运行期的检査，它的性能比dynamiccast 要高，在很多情况下是个空操作。 constcast:这种转型方式有可能不安全。它的目的是去掉一个指针或引用类型的cv限定，如从 const char 转换到 char\\。这种转型的一种常见用途是把一个 C++的指针传递到一个const不正确的C接口里去。比如，C接口需要字符串参数，不会对其进行修改，那形参应该使用const char，但实际有时使用了char\\。这在C里没有问题，因为字符串字面量的类型退化之后就是char\\，但C++里字符串字面量退化后是 constchar\\(参见第10页)。注意这种转型只是为了“欺骗”类型系统，让代码能通过编译。如果你通过constcast操作指针或引用去修改一个const 对象，这仍然是错误的，是未定义行为，可能会导致奇怪的意外结果。 reinterpretcast:这是对数据进行“重新解释”的最不安全的转型方式，用来在不相关的类型之间进行类型转换，如把指针转换成uintptrt。这种转换有可能得到错误的结果，比如，在存在多继承的情况下，如要把基类指针转成派生类指针，使用staticcast和使用reinterpretcast 可能会得到不同的结果:前者会进行偏移量的调整，而后者真的只是简单粗暴的硬转而已，因此结果通常是错的。又如，根据C++的严格别名规则，如果你用char或byte之外的类型的指针访问非该类型的对象(如通过int访问double对象 )，会导致未定义行为。",
    "url": "/c++/C++核心技术：基础内容.html",
    "lang": ""
  },
  {
    "title": "期值",
    "content": "--- title: C++核心技术：并发编程 --- 在操作系统看来，我们编译完可执行的C++程序，在单次运行的时候就是一个进程。而每个进程里可以有一个或多个线程: 每个进程有自己的独立地址空间，不与其他进程分享;一个进程里可以有多个线程，彼此共享同一个地址空间。 堆内存、文件、套接字等资源都归进程管理，同一个进程里的多个线程可以共享使用。每个进程占用的内存和其他资源，会在进程退出或被杀死时返回给操作系统。 因此，并发编程可以有多种不同的方式，如: 单核或多核系统上的多线程编程(一般使用共享内存来通信) 单核或多核系统上的多进程编程(一般使用消息机制来通信) 跨多个系统的分布式编程(一般使用网络协议来通信) 这些系统之间的区别很大，而我们主要关注多核系统(当然也覆盖了更简单的单核系统)里的多线程编程。如开头所说，这是目前的程序员必须面对的实际场景，而现代C++对这种方式的开发具有较完整的支持，开发效率上相比其他方式也有一定的优势(虽然在安全性上有所欠缺)。从概念上来说，这些不同形式的并发仍有一些共同的关注点。我们需要理解的基本概念是: 线程--基本的执行单元，是可以独立执行的指令序列，通常在某一时刻会占用一个处理器核。对于多线程的编程模型,“线程”当然是最自然的术语;对于其他编程模型，这里的“线程”也可以理解成其他相对应的执行单元概念。 共享数据--可能被多个线程访问的数据。对于不是原子量的共享数据，我们需要确保:在修改共享数据时，没有其他线程在同时修改或读该数据;否则即会导致数据竞争(datarace)。数据竞争是并发编程中最常见的问题，后果是未定义行为。 锁--一种同步机制，持有锁意味着我们有权利执行某项操作，如对共享数据的读或写。对共享数据提供锁，要求持有锁才能访问共享数据，是一种非常简单的避免数据竞争的方式。但反过来，等待锁也往往是并发编程的性能瓶颈所在。 通知--一种抽象的同步机制，用来通知其他线程发生了某个事件。根据具体的并发环境，通知可能携带额外的数据，也可能没有。 数据同步--共享数据可能同时存在于多个不同的地方，因此需要某种机制来同步多份数据。数据同步可能会带来程序员意料之外的延迟。即使在多核共享内存这样的简单系统里，都因为有存储层次结构而存在同步延迟 考虑到并发编程需要一种不同的思维模式，这一变化是一个不小的挑战。在某种程度上，有点像从经典力学的思维模式切换到相对论--这不完全是个比喻，因为并发编程里的某些难点，真是因为光速有限造成的! 目前的 C++标准提供了不止一种互斥量。我们先看最简单、也最常用的mutex。它可以默认构造，不可拷贝(或移动)，不可赋值。mutex主要提供的方法是: lock:锁定，锁已经被其他线程获得时则阻塞执行 trylock:尝试锁定，获得锁之后返回 true，在锁被其他线程获得时返回 false unlock:解除锁定(只允许在已获得锁时调用) 你可能会想到，如果一个线程已经锁定了某个互斥量，再次锁定会发生什么?对于mutex，回答是危险的未定义行为。你不应该这么做。如果需要在同一线程对同一个互斥最多次加锁，就要用到递归锁 recursive mutex了。除了允许同一线程可以无阻塞地多次加锁外(也必须有对应数量的解锁操作)，recursive mutex的其他行为和 mutex 一致。 除了mutex和recursive mutex，C++标准库还提供了: timedmutex:允许锁定超时的互斥量 recursivetimedmutex:允许锁定超时的递归互斥量 sharedmutex:允许共享和独占两种获得方式的互斥量(读写锁) sharedtimedmutex:允许共享和独占两种获得方式且允许锁定超时的互斥量 另外，<mutex>头文件中也定义了锁的 RAII 帮助类，如我们上面用过的 1ock guard。为了避免手动加/解锁的麻烦，以及在有异常或出错返回时发生漏解锁，我们一般应当使用 1ockguard,而不是手工调用互斥量的 1ock 和 unlock 方法。比 1ock guard 更复杂的是 uniquelock,它除了具有自动加/解锁的功能外，还额外支持可移动、构造时延迟加锁、手动加/解锁等操作。在需要这些额外功能时，就可以使用它(注意相比 1ock guard，它的额外开销更大) 条件变量的正确用法 c++ 使用正确惯用法的代码要复杂不少: void work(condition variable& cv,mutex& cvmut,bool& result ready,int& result){ this thread::sleep for(2s); result =42; { lockguard guard{cvmut}; result ready=true; } cv.notifyone(); } int main(){ condition variable cv; mutex cv mut; bool result ready =false; int result; scopedthread th{work,ref(cv),ref(cv mut),ref(result ready),ref(result)}; cout<<\"I am waiting now\\n\"; unique lock lock{cv mut}; cv.wait(lock,[&]{ return resultready;}); cout<<\"Answer:\"<<result<<'\\n'; } 不要在没有条件时使用wait c++ int work(){ thisthread::sleepfor(2s); return 42; } int main(){ auto fut = async(launch:async,work); cout<<\"I am waitting now\\n\"; cout<<\"Answer:\"<<fut.get()<<'\\n'; } 调用async函数模板可以获得一个期值(future)；launch::async是运行策略，告诉async应当在新线程里异步调用目标函数. async函数模板可以根据参数来推导出返回类型，比如这个是future< int> 使用sharedfuture可以解决future只能调用一次get的问题，可以用移动构造也可以调用future的成员函数share来生成。 c++ int work(promise<int> prom){ thisthread::sleepfor(2s); prom.setvalue(42); } int main(){ promise<int> prom; auto fut = prom.getfuture(); scopethread th{work,std::move(prom)}; cout<<\"I am waitting now\\n\"; cout<<\"Answer:\"<<fut.get()<<'\\n'; } c++ int work(){ thisthread::sleepfor(2s); return 42; } int main(){ packagedtask<int()> task{work}; auto fut = task.getfuture(); scopethread th{std::move(task)};//构造thread对象需要将task移进去 thisthread::sleepfor(1s); cout<<\"I am waitting now\\n\"; cout<<\"Answer:\"<<fut.get()<<'\\n'; } 如果软件和硬件可以对指令进行重排，那我们之前使用锁的代码有没有问题呢? 必须没有问题。软件和硬件允许重排都是为了提升软件的性能，而不是跟程序员作对。对于锁这样的已经沿用了多年的机制，我们必须保证它的行为不会在现代C++里突然发生变化。正式来讲，锁具有获得-释放语义:这也是这两个词的来源，获得(acquire)锁和释放(release)锁。这两个术语的基本含义是: 获得语义是内存操作的一个属性，当前线程所有在该操作后面的内存读写不允许被重排到该操作之前。 释放语义是内存操作的一个属性，当前线程所有在该操作前面的内存读写不允许被重排到该操作之后。 原子操作有三类： 读：在读取的过程中，读取位置的内容不会发生任何改变。 写：在写入的过程中，其他执行线程不会看到部分写入的结果。 读-修改-写：读取内存，修改数值，然后写回内存，整个操作的过程中不会有其他写入操作插入，其他执行线程不会看到部分写入的结果。原子量的++和--操作就是这种。 内存序： memoryorderrelaxed:宽松内存序，只提供基本保证 memoryorderconsume:不鼓励使用 memoryorderacquire:获得操作，在读取某原子量时，当前线程所有后续的读写操作都不可重排到该操作之前，并且其他线程在释放同一个原子量之前的所有内存写入都对当前线程可见。 memoryorderrelease:释放操作，在写入某原子量时，当前线程所有之前的读写操作都不可重排到该操作之后，并且当前线程所有之前的内存写入都对获取同一个原子量的其他线程可见。 memoryorderacqrel:获得释放操作，一个读-修改-写操作同时具有获得语义和释放语义，也就是它前后的任何读写操作都不允许跟该操作重排，并且其他线程在释放同一个原子量之前的所有内存写入都对当前线程可见，当前线程所有之前的内存写入都对获取同一个原子量的其他线程可见。 memoryorderseqcst：序列一致性，对于读操作相当于获取，对于写操作相当于释放。，它是所有原子量的默认内存序。 那我们是不是应该多用atomic 呢？并不是： 如果你不确定能不能用好 atomic，那就别用。优先用高层抽象（如期值），或者大家较为熟知的概念（如线程和锁）。C++里的很多机制是提供给需要极致性能的底层库/框架开发者的。 如果你不确定该用哪种内存序，那就用默认的序列一致性。 尽量不要碰宽松内存序，那一般是留给专家的，陷阱特别多。 下面我们再来说说一些实际可以用atomic 的地方。 我们可以使用 atomic<boo1>来做多线程同步标识，比如用来通知线程应当结束运行。 最简单的用法是在检查的地方使用获得语义（stopflag.load（memoryorderacquire）），在通知的地方使用释放语义（stopflag.store（true,memoryorderrelease））。不过，如果这种检查较为频繁，并且线程退出完全不涉及其他的内存操作或原子量的话，你也可以考虑使用宽松内存序。 我们可以使用 atomic<Obj>来管理一个对象的延迟初始化。当指针值不空的时候，使用指针的人就知道对象已经初始化完成。此时，使用获得-释放语义非常合适：检查使用获得语义，通知使用释放语义。 我们可以使用 atomic<int>或其他整数类型的原子量在多线程中进行计数。这样，我们不需要使用互斥量就能安全地进行计数。一般情况下我们应当使用内存序 memory-orderacqrel，但在不检查结果的情况使用 memoryorderrelaxed 也很合理。比如，以libc++的sharedptr的实现里可以看到，它对引用计数增一使用了 addfetch（1,memoryorderrelaxed），对引用计数减一则使用了 addfetch（-1, memoryorderacqrel）。指定不同的内存序究竟有没有差异，则因平台而异。 我们可以使用原子量的 compareexchangeweak之类的方法来实现无锁编程里的CAS操作。实现并发队列就需要这样的技巧。",
    "url": "/c++/C++核心技术：并发编程.html",
    "lang": ""
  },
  {
    "title": "C++核心技术：异步编程",
    "content": "--- title: C++核心技术：异步编程 --- 基本含义是 其操作的执行过程中当前执行线程仍能继续处理其他任务。 但是异步操作并不一定意味着新的(用户)线程。只要不妨碍当前线程的继续执行，都可以看作异步操作。 同步”的编程接口意味着在完成所需操作之前程序会阻塞。 “异步”的接口则行为有所不同： 允许不进行阻塞，而是让用户代码查询操作的状态， 或通过回调的方式通知用户代码某个操作已经完成。 但是，除了原始、性能一般的 select之外，不同操作系统提供的编程接口也大相径庭，有epoll、IOCP、kqueue、iouring 等，让本来就困难的异步代码变得越发麻烦⋯… 异步编程麻烦，那为什么我们要进行异步编程呢？它的主要好处是高效的资源利用，比如，可以在一个线程里管理成千上万个网络连接。这样可以提高吞吐量，降低延迟，并减少CPU 占用。 c++ void doA(){ doB(); doC(); } //如果doA不需要看到doB和doC的结果,也不需要按照这个顺序执行，可以使用asio这样写 asio::iocontext context; void doA(){ context.post(doB); context.post(doC); } 最简单的方式是在main里调用context.post(doB);和context.run()，那样run会阻塞地运行发布到context的所有任务，直到没有任务时退出。 我们可以想象简单的iocontext实现 c++ class iocontext{ public: using taskt = function<void()>; void post(taskt task) { taskqueue.push(std::move(task)); } void run(){ for (;;) { taskt task; if (taskqueue. empty)) { break; } task = std::move(taskqueue.front()); taskqueuepop(); task(); } private: queue<taskt> taskqueue; }; 实际的iocontext可以支持在多个线程里同时运行。 异步的定时器回调代码： c++ void sayHello(){ cout << \"Hello, world! \\n\"; } void doA(){ auto t = makeshared<asio::steadytimer>(context, 2s); t-›asyncwait([t] (const errorcode& /ec/) { sayHello(); });//添加延迟执行的任务 } //尤其要注意，下列代码是错误的 void doA(){ auto t = asio::steadytimer(context, 2s); t.asyncwait([t] (const errorcode& /ec/) { sayHello(); }); } 如果使用这种形式的代码，运行时我们会看到 sayHello 直接运行了，没有等待两秒钟。这是因为t在析构时会直接运行传入的函数对象。这同时也是异步编程里一种常见的惯用法——==对于会超出作用域的对象，我们需要用 sharedptr 在堆上创建，并用lambda表达式捕获下来，以确保在异步执行的回调结束执行之前，用到的对象不会消失。== 另外可以注意到，我们这里异步回调有一个参数 const errorcode& ec，用来表示回调的原因。对于 steadytimer的回调，它可以表示成功，也可以表示操作被中断。当 steadytimer 对象在激发之前被提前析构，或者被主动 cancel 时，ec 就会被设成某个错误码，我们可以在回调里进行检查： c++ t->asyncwait([t] (const errorcode& /ec/) { if (ec == asio::error::operation aborted) { return; sayHello(); });",
    "url": "/c++/C++核心技术：异步编程.html",
    "lang": ""
  },
  {
    "title": "C++核心技术：智能指针",
    "content": "--- title: C++核心技术：智能指针 --- sharedptr里有两个指针，其中一个指向控制块，一个指向真正的对象 sharedptr的复制：复制两个指针，然后在控制块的引用计数+1 假设我们使用容器v来存储智能指针，那对于uniqueptr，每次执行pushback操作时必须使用一个右值，如下面： v.pushback(createShape()); //createShape通过makeunique返回智能指针 v.pushback(std::move(uptr));//移动之后我们就不应该再使用uptr了 如果可以找到一个对象来明确负责另外一个对象的生存期，那么uniqueptr就足够了；只有在做不到这点时，才考虑使用sharedptr。 使用sharedptr的场景： 消息有多个并发处理者 异步代码需要把当前对象的进一步处理注册到某个异步回调上 写时复制的对象，允许同时读和修改的树形结构 weakptr的解决环形引用的问题，通过在控制块里放置一个独立的弱引用计数。 通过weakptr的lock可以得到sharedptr，如果返回指针为空，那么代表指向对象已经失效 引用计数的增减是原子操作，这在多线程环境里很容易导致性能问题。如果按值传递sharedptr，就会发生这类问题。 不过解决这个问题的正确方式是 在无关生存期的函数接口中不要使用智能指针：使用普通的引用(保证对象存在)或指针(对象不存在，指针为空)就好 Facebook在rocksDB里sharedptr从值传参改成传普通引用/指针,该优化在某项基准测试中使得每秒查询次数提升了原来的4倍。 使用sharedptr时，我们常常需要创建局部变量来增加引用计数，以防止被引用的对象在使用期间因全局sharedptr被修改而被销毁。如果使用这种用法的话，要确保这样的代码放在不会反复执行的地方，以免影响性能。 还是这句话，在无关生存期的函数接口中不要使用智能指针：使用普通的引用(保证对象存在)或指针(对象不存在，指针为空)就好 反过来，如果一个函数确实就是要管理对象的所有权，那传参/返回值就该用智能指针，如下： c++ uniqueptr<Obj> factory();//生产Obj void sink(uniqueptr<Obj>);//消费Obj，转移所有权 void reseat(uniqueptr<Obj>&);//可能修改指针 void thinko(const uniqueptr<Obj>&);//这种情况就不需要使用智能指针了，因为不涉及到管理对象的所有权,这样使用智能指针没什么意义，完全可以用对象引用或者指针代替 c++ sharedptr<Obj> factory();//确知生产结果需要共享 void share(sharedptr<Obj>);//将保留引用计数 void reseat(sharedptr<Obj>&);//可能修改指针 void mayshare(const sharedptr<Obj>&);//可能保留引用计数 出于性能考虑，在最终保存智能指针时，我们都应该用右值形式如： saved = std::move(ptr); uniqueptr的删除器是模板的第二个参数，但是sharedptr则没有将删除器当作模板参数。 这个区别的原因是：uniqueptr追求的是低开销，和裸指针开销相似。而sharedptr本来就需要为控制块额外分配内存，再增加删除操作的信息也不是什么问题。 对于sharedptr来说，尽可能使用makeshared来创造智能指针，因为可以一次性分配控制器和对象的内存。 当我们继承了enablesharedfromthis的时候，可以利用sharedfromthis()来从this指针获得一个sharedptr。这样的类内部有一个指向自己的weakptr，但这个weakptr需要外部创建sharedptr的时候来进行初始化，因此我们在构造函数内部不能立即开始执行利用到sharedfromthis()的函数，否则sharedfromthis会失败。 1.11:永远不要用原始指针(T)或引用(T&)来转移所有权 F.7:对于一般用途，使用原始指针(T)或引用(T&)作为参数，而不是智能指针。 C.12:不要把可拷贝或可移动类型的数据成员变成const或引用。 R.3：原始指针(T)不代表所有权",
    "url": "/c++/C++核心技术：智能指针.html",
    "lang": ""
  },
  {
    "title": "C++核心技术：现代C++的一些重要改进",
    "content": "--- title: C++核心技术：现代C++的一些重要改进 --- C++14在标准库中新加了不少字面量运算符 c++ using namespace std; int main() cout <<\"i i = \" << 1i 1i <<'\\n'; cout<<\"Waiting for 500ms\\n\"; this thread::sleep for(500ms); cout<<\"Hello world\"s.substr(0,5)<<'\\n'; cout<<\"Hello world\"sv.substr(6)<<'\\n'; 输出是: ii=(-1,0) Waiting for 500ms Hello world 二进制使用 0b1110011 当需要使用枚举时，默认情况应该选择枚举类 enum class Color:uint8t{red=1,green,blue}; c++20前的chrono c++ auto t1 = chrono::steadyclock::now(); cout<<\"hello\\n\"; auto t2 = chrono::steadyclock::now(); cout<<(t2-t1)/1ns<<\" ns has elapse\\n\"; c++20后的chrono 可以直接输出cout<<(t2-t1) 且可以直接输出systemclock::now() 输出的是UTC时间 c++ using namespace std; int main() auto seed = randomdevice{}(); //生成随机种子 cout<<\"Seed is \"<< seed << '\\n'; mt19937 engine{seed};//使用种子初始化随机引擎 uniformintdistribution dist{1, 1000}; vector<int> v(100); generate(v.begin(),v.end(),[&]{ return dist(engine);}); cout << v<< '\\n'; } 使用shuffle替代shufflerandom c++ auto seed = randomdevice{}(); //生成随机种子 cout<<\"Seed is \"<< seed << '\\n'; mt19937 engine{seed};//使用种子初始化随机引擎 vector<int> v(100); iota(v.begin(),v.end(),1); shuffle(v.begin(),v.end(),engine);",
    "url": "/c++/C++核心技术：现代C++的一些重要改进.html",
    "lang": ""
  },
  {
    "title": "C++核心技术：移动语义",
    "content": "--- title: C++核心技术：移动语义 --- 右值分为纯右值和将亡值： 纯右值：返回非引用类型的表达式，如x++,x+1,makeshared。以及除字符串字面量之外的字面量，如42，true 将亡值：右值对象的成员，返回右值引用的表达式 如std::move()",
    "url": "/c++/C++核心技术：移动语义.html",
    "lang": ""
  },
  {
    "title": "C++核心技术：视图",
    "content": "--- title: C++核心技术：视图 --- 在不关心所有权时，我们可以考虑视图 stringview只保存一个const char 和一个sizet c++ 不要这样使用stringview stringview name = string(\"C++\");//string是一个临时对象 原先如果传参const string &确实对string很友好，但是对字符串字面量就不友好了，如果传参一个字符串字面量，涉及到临时string对象的构造，将字符串字面量的内容全部复制进去，然后函数返回还得销毁这个临时对象 stringview不是string，但是它的成员函数和string非常相似 同样有data,size,begin,end,find等方法。 不同的点在于： 不能修改字符串的内容,data返回的是const char 。 没有cstr()函数，并且它的data函数完全不保证返回的字符串是零结尾的。 substr成员函数返回的是一个新的stringview，而非string。 额外有成员函数removeprefix和removesuffix 如果用一个stringview对象sv去调用原型为f(const string &)的函数，我们需要写f(string(sv))； std::stringview 提供多种构造函数，用于创建对字符串的非拥有视图： cpp复制std::stringview sv; // 空视图，data() = nullptr，size() = 0 c++ cpp复制const char cstr = \"Hello\"; std::stringview sv1(cstr); // 长度自动计算（直到 '\\0'） std::stringview sv2(cstr, 3); // 指定长度（\"Hel\"） cpp复制std::string str = \"Hello\"; std::stringview sv(str); // 视图指向 str 的数据 std::stringview svpart(str, 1, 3); // 从索引 1 开始，长度 3（\"ell\"） cpp复制char arr[] = {'H', 'e', 'l', 'l', 'o'}; std::stringview sv(arr, 5); // 视图指向 arr，需手动指定长度 cpp复制std::stringview svoriginal = \"Hello\"; std::stringview svcopy(svoriginal); // 复制视图 std::stringview svsub(svoriginal, 1, 3); // 从索引 1 开始，长度 3（\"ell\"） cpp复制std::string str = \"Hello\"; std::stringview sv(str.begin(), str.end()); // 视图指向整个字符串 ------ std::stringview 提供与 std::string 类似的接口，但不可修改底层数据。 data(): 返回指向底层数据的指针（可能非空终止）。 size() / length(): 返回视图长度。 maxsize(): 返回可支持的最大长度。 empty(): 检查视图是否为空。 operator[](sizet pos): 返回索引处的字符（不检查边界）。 at(sizet pos): 返回索引处的字符（越界时抛出 std::outofrange）。 front(): 返回首字符。 back(): 返回尾字符。 removeprefix(sizet n): 从头部移除 n 个字符。 removesuffix(sizet n): 从尾部移除 n 个字符。 substr(sizet pos = 0, sizet count = npos) : 返回子视图。 cpp复制std::stringview sv = \"Hello\"; std::stringview sub = sv.substr(1, 3); // \"ell\" compare(stringview other): 返回比较结果（类似 strcmp）。 支持运算符：==, !=, <, >, <=, >=。 find(char ch, sizet pos = 0): 查找字符。 rfind(char ch, sizet pos = npos): 反向查找。 findfirstof(stringview chars, sizet pos = 0): 查找任意字符首次出现。 findlastof(stringview chars, sizet pos = npos): 查找任意字符最后一次出现。 findfirstnotof, findlastnotof: 类似逻辑。 begin() / end(): 返回迭代器。 cbegin() / cend(): 返回常量迭代器。 rbegin() / rend(): 返回反向迭代器。 ------ cpp复制#include <iostream> int main() { // 初始化 std::stringview sv1 = \"Hello\"; // 自动推导长度 std::stringview sv2(\"World\", 3); // \"Wor\" std::string str = \"Hello World\"; std::stringview sv3(str.data() + 6, 5); // \"World\" // 成员函数 std::cout << sv1.substr(1, 3) << std::endl; // \"ell\" sv1.removeprefix(1); // 视图变为 \"ello\" std::cout << sv1.find('l') << std::endl; // 1（第一个 'l' 的索引） return 0; } ------ 生命周期管理：stringview 不管理内存，需确保底层数据在其生命周期内有效。 空终止符：data() 返回的指针可能不以 \\0 结尾，直接传递给 C 函数需谨慎。 性能优化：适合作为函数参数替代 const std::string&，避免不必要的拷贝。 C++ //假设有一个通用的打印整数序列的函数 void print(span<int> sp){ for(int n:sp){ cout<<n<<' '; } cout<<'\\n'; } //以下容器都是可以传参给print的 array a{1,2,3,4,5}; int b[]{1,2,3,4,5}; vector v{1,2,3,4,5}; //而不提供连续存储的容器不能这么用，比如list span< char>允许更改底层数据，和stringview不一样 如果不需要修改内容其实应该使用span< const int> begin,end,front,back,size,empty,data,operator[] 也有一些特有的成员函数 sizebytes:用于字节数计算的序列大小 first：开头若干项组成的新的span，不修改自身 last:结尾若干项组成的新spanm subspan:类似substr 且span的长度可以在编译器确定 span是语法糖，程序员需要保证底层数据合法存在，不要释放后使用 到了C++20,真正有了std::ranges::view这个概念 需要包含< ranges>头文件 c++ //std::views::transform是一个映射视图 std::cout<<(a|std::views::transform([] (int n){return nn;})); c++ //过滤操作 std::cout<<(a|std::views::filter([] (int n){return n%2==0;})); c++ //反转操作 std::cout<<(a|std::views::reverse); c++ //对于包含多个子元素的容器 std::map<int,string> mp{{1,\"one\"},{2,\"two\"},{3,\"three\"}}; std::cout<<(mp|std::views::values);",
    "url": "/c++/C++核心技术：视图.html",
    "lang": ""
  },
  {
    "title": "atomic",
    "content": "--- title: atomic --- atomic是一个类模板，在我的认知中，更愿意把它看做是一个拓展封装类，封装一个原有的类型，并拓展新的api给用户，好比shareptr之于原生指针，适配器queue之于容器deque。 atomic也是如此，当我们写下std::atomic<int>的时候，意味着将int拓展成原子类型，将int类型的++，--等都变成原子操作，同时拓展了诸如fetchadd，fetchsub等原子加减方法供用户使用。 我们可以先对原子变量下一个初步的定义：==即某些成员方法是原子操作的对象==。 atomic提供的常见方法： store：原子写操作。 load：原子读操作。 compareexchangestrong：传入[期望原值]和[设定值]， ==当前值与期望值相等时，修改当前值为设定值，返回true==； ==当前值与期望值不等时，将期望值修改为当前值，返回false==。 compareexchangeweak：同上，但允许偶然出乎意料的返回（比如在字段值和期待值一样的时候却返回了false），比strong性能更好些，但需要循环来保证逻辑的正确性。 exchange：交换2个数值，并保证整个过程是原子的。 原子操作和高层（编程语言层级在C以上，姑且叫它高层）代码的实现并没有关系，哪怕 ++i 或是 i++ 在高层只是一条单独的语句，当翻译成底层代码（汇编）的时候，就需要更多指令来完成。 !image-20241005153016083 原子操作的定义：即在高层代码的一个原子操作，不论在底层是怎么实现的，有多少条指令，在底层这些指令执行期间，都不会受到其它线程或者读写任务的干扰。 我自己对原子操作的定义是，其底层的诸多指令会被捆绑在一起完成，不受其它线程影响，那么具体该怎么才能实现这种捆绑呢。 加锁，这是大家最容易想到的方法，也是最简单的方法。先不说底层汇编，在高层语言解决实际的业务问题，我们需要某段代码在同一个时间只允许一个进程/线程访问，也就是临界区，一般会lock住mutex来进入临界区，完成业务后unlock mutex来退出临界区。 但是互斥锁mutex有个严重的问题，就是效率不高，如果有两个线程t1和t2，当t1拿到了互斥锁，t2也想要这个锁，发现这个锁已经被t1占有了，t2不会在t1身边徘徊等锁，而是直接陷入阻塞态，直到t1释放了锁，t2才会被唤醒，进入到就绪队列等待调度，并再次去尝试获得锁。这将会消耗大量的资源在重复的“睡眠”，“等待”，“唤醒”的操作中。 其次，在保存和恢复上下文的过程中，还会存在cache失效的可能。 !image-20241005153212262 为什么需要内存序的根本原因，我们利用内存序可以限制cpu对指令执行顺序的重排程度，防止单线程指令的合理重排在多线程的环境下出现顺序上的错误。 volatile变量的意义在于每次读写都会从内存读或者写内存，解决的是编译器重排的问题。 volatile只能保证涉及每个volatile变量的代码的相对顺序不会被编译器重排，至于volatile变量的代码和其他非volatile变量的代码之间的相对顺序并不保证，且无法保证cpu不会继续重排你的代码。 但是c++11的真正atomic操作，是有内存序参数的，用于避免cpu的重排，==可以说atomic+内存序两者叠加才真正在lock-free（免锁）情况下实现了高层代码顺序和底层代码执行顺序的统一。== 要理解不同的内存序，不妨从几种硬件层面的内存模型来入手，会更好理解。 内存一致性模型（memory consistency model）用于描述多线程对共享存储器的访问行为，在不同的内存一致性模型里，多线程对共享存储器的访问行为有非常大的差别。 顺序存储模型（强定序模型） 完全存储定序（TSO） x86 CPU 就是这种内存模型，为了提高效率，其在L1缓存之前加了一个store buffer，因此写数据指令执行时，会先把更新后的数据放到store buffer里后立刻返回执行下一条指令，store buffer的数据则会慢慢被写到L1 cache中，如果有多条写操作指令，会严格按照FIFO的次序执行。 但无论是否FIFO，总之store buffer的存在已经导致MESI的同步被破坏，写指令立刻返回，后续的指令（比如读操作）可能在store buffer数据还没更新到所有cache和内存之前就执行，这就会出现store-load乱序。 部分存储定序（PSO） 宽松内存模型（RMO） ==c++在执行atomic操作时，传入的不同内存序参数，就是在告诉你，它会模拟上述哪一种内存模型来处理代码执行顺序。== memory的总体分类和内存序的对应： memoryorderseqcst: 这是所有atomic操作内存序参数的默认值，语义上就是要求底层提供顺序一致性模型，不存在任何重排，可以解决一切问题，但是效率最低。 memoryorderrelease/acquire/consume: 提供release、acquire或者consume, release语意的一致性保障 它的语义是：我们允许cpu或者编译器做一定的指令乱序重排，但是由于tso, pso的存在，可能产生的store-load乱序store-store乱序导致问题，那么涉及到多核交互的时候，就需要手动使用release, acquire去避免这样的这个问题了。简单来说就是允许大部分写操作乱序（只要不影响代码正确性的话），对于乱序影响正确性的那些部分，程序员自己使用对应的内存序代码来控制。 memoryorderrelaxed: 这种内存序对应的就是RMO，完全放开，让编译器和cpu自由搞，很容易出问题，除非你的代码是那种不论怎么重排都不影响正确性的逻辑，那么选择这种内存序确实能提升最大性能。 综上，最实用的还是memoryorderrelease和memoryorderacquire这两种内存序，兼顾了效率和代码的正确性。 memoryorderrelease 如果用了这种内存序，保证在本行代码之前，有任何写内存的操作，都是不能放到本行语句之后的。 本行代码之前的写操作不能执行在本行代码之后 也就是可以让程序员可保证一段代码的写顺序。 假设我们还是希望a=1的执行在b=2之前（对于所有共享ab的线程来说都是一致的），可以这样实现。 c++ //-将下面代码用release控制 int a = 0; int b = 0; void func1(){ a = 1; b = 2; } //------------------------------分割线----------------------------- int a = 0; std::atomic<int> b(0); void func1(){ a = 1; b.store(2,std::memoryorderrelease);//-a的写操作不会重排到b的写操作之后 } memoryorderacquire 如果用这种内存序，保证在本行代码之后，有任何读内存的操作，都不能放到本行语句之前。 本行代码之后的读操作不能执行在本行代码之前 也就是可以让程序员可保证一段代码的读顺序。 对于线程2而言，我们无法保证cout<<a<<endl;会不会重排到while(b !=2);之前，所以可以这样修改代码 c++ int a = 0; int b = 0; void func1(){ a = 1; b = 2; } void func2(){ while(b != 2); cout<<a<<endl; } //------------------------------分割线----------------------------- int a = 0; std::atomic<int> b(0); void func1(){ a = 1; b.store(2,std::memoryorderrelease);//-a的写操作不会重排到b的写操作之后 } void func2(){ while(b.load(std::memoryorderacquire) != 2); cout<<a<<endl;//-a的读操作不会重排到b的读操作之前 } 那么内存屏障是什么，简单来说，就是我们希望上述的代码在逻辑上更纯粹，我们希望a和b就是纯纯的两个非原子int，而不是让b变成原子变量来保证执行顺序。 ==内存屏障就可以想象成用一个匿名的原子变量来保证执行顺序，不需要让b变成原子变量了==，代码如下： c++ int a = 0; int b = 0; void func1(){ a = 1; std::atomicthreadfence(std::memoryorderrelease); b = 2; } void func2(){ while(b != 2); std::atomicthreadfence(std::memoryorderacquire); cout<<a<<endl; } //使用release屏障，相当于写操作a=1不会重排到b之后。 //使用acquire屏障，相当于读操作cout<<a不会重排while(b !=2)之前。 //和刚刚实现了一样的功能。 lock-free不能简单理解成无锁，因为本身CAS就是一个自旋锁的机制，我感觉无锁和有锁更像是互斥锁和自旋锁的区别，或者说悲观锁和乐观锁的区别。 在执行 i++的时候，互斥锁觉得本线程在执行汇编的三条语句时，一定会被其它线程干扰，所以干脆在i++之前就加锁，自增后解锁，代码如下： c++ int i; mutex m; void func1(){ lockguard<mutex> lock(m); i++; } atomic则是利用CAS的机制，我先判断 i 是不是=旧值，如果=旧值说明没被其它线程干扰，于是 i 更新成newvalue，这就有点乐观的意思了，因为atomic优先觉得本线程是没有被其它线程干扰的，大不了compare不成功，就不更新新的值呗。",
    "url": "/c++/atomic.html",
    "lang": ""
  },
  {
    "title": "C++11新特性有哪些",
    "content": "--- title: C++11新特性有哪些 --- 类型推导： auto关键字，允许编译器根据初始化表达式推导变量类型。 decltype 分析表达式并得到它的类型，却不实际计算表达式的值。 基于范围的 for 循环：提供了一种更简洁的遍历容器的方法。 lambda 表达式：允许在需要的地方定义匿名函数。 智能指针（如 std::uniqueptr 和 std::sharedptr）：提供了自动内存管理。 右值引用和移动语义：优化资源的移动操作，高效的将资源从一个对象转移到另一个对象，减少拷贝的开销。 nullptr: 空指针，明确表示空指针的关键字",
    "url": "/c++/C++11新特性有哪些.html",
    "lang": ""
  },
  {
    "title": "C++17的新特性",
    "content": "--- title: C++17的新特性 --- C++17 是继 C++14 之后的一个重要版本，引入了许多新特性和改进，显著提升了代码的简洁性、性能和表达能力。以下是 C++17 的主要新特性及其用途： --- 用途：将元组、结构体或数组的元素解包到多个变量。 示例： cpp std::pair<int, std::string> getData() { return {42, \"hello\"}; } auto [num, str] = getData(); // num=42, str=\"hello\" 用途：在条件语句中声明变量，限制其作用域。 示例： cpp if (auto it = map.find(key); it != map.end()) { // 仅在此块内使用 it } 用途：允许在头文件中直接定义静态成员变量，避免重复定义。 示例： cpp class MyClass { public: inline static int sharedValue = 42; // C++17 支持类内初始化 }; 用途：简化可变参数模板的递归展开。 示例（求和）： cpp template <typename... Args> auto sum(Args... args) { return (... + args); // 折叠表达式展开：args1 + args2 + ... } 用途：在编译期根据条件选择代码分支（模板编程）。 示例： cpp template <typename T> auto process(T val) { if constexpr (std::ispointerv<T>) { return val; // 仅当 T 是指针时编译 } else { return val; // 否则直接返回 } } --- 用途：表示可能存在的值（替代 nullptr 或特殊标记值）。 示例： cpp std::optional<int> find(int key) { if (key == 42) return 42; else return std::nullopt; // 无值 } 用途：类型安全的联合体（可存储多种类型的值）。 示例： cpp std::variant<int, std::string> data; data = \"hello\"; // 存储字符串 std::cout << std::get<std::string>(data); // 安全访问 用途：通过执行策略（std::execution::par）并行化标准库算法。 示例： cpp #include <execution> std::vector<int> vec = {3, 1, 4, 1, 5}; std::sort(std::execution::par, vec.begin(), vec.end()); // 并行排序 用途：跨平台文件路径和目录操作。 示例： cpp #include <filesystem> namespace fs = std::filesystem; fs::path p = \"data.txt\"; if (fs::exists(p)) { std::cout << \"文件大小：\" << fs::filesize(p) << \"字节\"; } --- 用途：自动推导模板参数，简化对象声明。 示例： cpp std::pair p(42, \"hello\"); // 自动推导为 std::pair<int, const char> 用途：简化嵌套命名空间的声明。 示例： cpp namespace A::B::C { // 等价于 namespace A { namespace B { namespace C { // ... }}} 用途：抑制未使用变量的警告或强制返回值检查。 示例： cpp [[nodiscard]] int compute() { return 42; } compute(); // 编译器警告：未处理返回值 13.stringview --- | 类别 | 特性 | 核心优势 | | -------------- | -------------------------- | ------------------------------ | | 语法简化 | 结构化绑定、if初始化 | 代码更简洁，作用域更清晰 | | 元编程增强 | 折叠表达式、constexpr if | 模板代码更易编写和维护 | | 标准库扩展 | std::optional、并行算法 | 功能更强大，性能更优 | | 类型安全 | std::variant | 替代传统联合体，避免未定义行为 | C++17 通过以上特性显著提升了开发效率和代码质量，建议在支持 C++17 的编译器（如 GCC 7+、Clang 5+、MSVC 19.1+）中尝试使用。",
    "url": "/c++/C++17的新特性.html",
    "lang": ""
  },
  {
    "title": "C++中的多线程同步机制",
    "content": "--- title: C++中的多线程同步机制 --- 互斥锁（Mutex） 互斥锁是最常用的同步机制之一，它可以确保在任意时刻只有一个线程能够访问共享资源。通过调用std::mutex类的lock()和unlock()方法，可以将对共享资源的访问保护起来。 条件变量（Condition Variable） 条件变量是一种机制，用于线程间的通信和同步。它允许一个或多个线程等待某个特定条件的发生，并在条件满足时被唤醒。 原子操作 原子操作是一种不可分割的操作，不会被中断。C++11引入了原子操作库，其中定义了一些原子类型，如std::atomicint。 读写锁（Reader-Writer Lock）：通过std::sharedmutex和std::sharedlock、std::uniquelock提供，用于在读多写少的情况下提高并发性。",
    "url": "/c++/C++中的多线程同步机制.html",
    "lang": ""
  },
  {
    "title": "C++如何实现一个单例模式",
    "content": "--- title: C++如何实现一个单例模式 --- 在C++中实现单例模式，主要是确保一个类只有一个实例，并提供一个全局访问点, C++实现单例模式需要满足以下几点要求： 私有化构造函数：将类的构造函数定义为私有，防止外部通过new关键字创建多个实例。 静态实例：在类内部提供一个静态私有实例，这个实例将作为整个程序的唯一实例。 静态公有访问方法：提供一个公有的静态方法，通常称为getInstance，用于获取类的唯一实例。 删除拷贝构造函数和赋值操作符：为了防止通过拷贝或赋值来创建新的实例，需要将拷贝构造函数和赋值操作符定义为私有或删除。 单例模式有懒汉式和饿汉式两种实现。 懒汉式 类实例只有在第一次被使用时才会创建，这个时候需要注意多线程下的访问，需要利用互斥锁来加以控制。 饿汉式 类实例在类被加载时就进行创建。 c++ //c++11 ：方法一 class mySingleton { public: static mySingleton &getInstance() { static mySingleton inst; return inst; } mySingleton() { } mySingleton(const mySingleton &) = delete; mySingleton &operator=(const mySingleton &) = delete; //成员函数 private: mySingleton() { } private: //成员 }; //利用模板加callonce生成单例模板，记得加友元 template <typename T> class Singleton { public: static sharedptr<T> getInstance() { static onceflag on; callonce(on, [&]() { minstance = sharedptr<T>(new T()); }); return minstance; } Singleton(const Singleton<T> &) = delete; Singleton &operator=(const Singleton<T> &) = delete; Singleton() {}; protected: Singleton() {}; private: static sharedptr<T> minstance; }; template <typename T> sharedptr<T> Singleton<T>::minstance = nullptr; class CartMgr : public Singleton<CartMgr> { public: friend class Singleton<CartMgr>; // 添加商品到购物车 void add(const string &itemName, int quantity) { cart[itemName] += quantity; } // 查看购物车 void print() const { for (const auto &item : cart) { cout << item.first << \" \" << item.second << endl; } } private: CartMgr() = default; // 购物车存储商品和数量的映射 map<string, int> cart; };",
    "url": "/c++/C++如何实现一个单例模式.html",
    "lang": ""
  },
  {
    "title": "C++怎么实现多态",
    "content": "--- title: C++怎么实现多态 --- C++ 的多态分为编译时多态（也被称为静态多态）和运行时多态（也被称为动态动态） 编译时多态 函数重载（Function Overloading）:允许在同一作用域内声明多个同名函数，只要它们的参数列表不同。 运算符重载（Operator Overloading）: 允许为自定义类型定义或修改运算符的行为。 模板（Templates）: 允许创建泛型类和函数，它们可以在多种数据类型上使用。 ==编译时多态在编译期间就确定了具体的函数或类型，由编译器根据函数的签名或模板实例化来选择正确的函数或实例。== 运行时多态: 运行时多态主要通过虚函数和抽象类实现，父类中定义声明虚函数，子类实现对虚函数的重写。由虚函数表和虚函数指针在运行时确定调用哪个函数。 虚函数（Virtual Functions）: 虚函数在基类定义，派生类可以重写这些虚函数。 抽象类（Abstract Classes）: 包含至少一个纯虚函数的类，不能被实例化，但可以作为其他类的基类。 运行时多态实现原理： 运行时多态涉及到虚函数表（vtable）和虚函数指针（vptr）。当类包含虚函数时，编译器会自动为该类创建一个虚函数表，表中包含类中所有虚函数的地址。 子类的虚函数表继承了父类的虚函数表，但会使用自己重写的虚函数将虚函数表中对应的虚函数进行覆盖。 当通过基类指针或引用调用虚函数时，程序会根据对象的实际类型在运行时查找正确的函数地址并调用相应的函数，实现多态。",
    "url": "/c++/C++怎么实现多态.html",
    "lang": ""
  },
  {
    "title": "C++构造函数有几种，分别什么作用",
    "content": "--- title: C++构造函数有几种，分别什么作用 --- 在C++中，构造函数有几种不同的类型，每种都有其特定的作用： 默认构造函数：没有参数的构造函数，用于创建对象的默认实例。 参数化构造函数：带参数的构造函数，允许在创建对象时初始化成员变量。 拷贝构造函数：以同一类的实例为参数的构造函数，用于复制已有对象。 移动构造函数：以同一类的实例的右值引用为参数，用于利用即将销毁的对象的资源。 转换构造函数：允许将其他类型或值隐式转换为当前类类型的实例。 委托构造函数：一个构造函数调用另一个构造函数来完成初始化，可以是同一个类的其他构造函数。 初始化列表构造函数：使用成员初始化列表来初始化成员变量，这是最高效的初始化方式。 常量构造函数：声明为const的构造函数，可以用于创建常量对象。 constexpr构造函数：允许在编译时初始化对象，用于定义和初始化字面量类型的对象。 每种构造函数的使用场景不同，例如： 默认构造函数用于快速创建对象，而不需要显式提供任何初始化参数。 参数化构造函数提供了灵活性，允许在创建对象时定制其状态。 拷贝构造函数和移动构造函数分别用于对象的复制和移动，是实现资源管理的关键。 转换构造函数和委托构造函数提供了更灵活的对象初始化方式。 初始化列表构造函数是C++中推荐的成员初始化方式，因为它可以提高效率。",
    "url": "/c++/C++构造函数有几种，分别什么作用.html",
    "lang": ""
  },
  {
    "title": "C++的四种强制类型转换",
    "content": "--- title: C++的四种强制类型转换 --- C++中的四个强制类型转换的关键字： 1、staticcast，2、constcast，3、reinterpretcast，4、dynamiccast 这应该四种中是最常见的。用法为 staticcast<type-id> (expression)。 该运算符把 expression 转换为 type-id 类型，但==没有运行时类型检查来保证转换的安全性。== staticcast 可以用于指针之间的转换，但要求两者之间有明确的继承关系或者是基本类型之间的转换。 主要用法如下： （1）用于类层次结构中基类（父类）和派生类（子类）之间指针或引用的转换。 进行上行转换（把派生类的指针或引用转换成基类表示）是安全的； 进行下行转换（把基类指针或引用转换成派生类表示）时，由于没有动态类型检查，所以是不安全的。 （2）用于基本数据类型之间的转换，如把int转换成char，把int转换成enum。这种转换的安全性也要开发人员来保证。 （3）把空指针转换成目标类型的空指针。 （4）把任何类型的表达式转换成void类型。 最常用的应该还是基本数据类型之间的转换： 上边的 staticcast 不能将 const int 转成 int，constcast 就可以，用法为 constcast<type-i> (expression)。如下面代码 也就是说，constcast<>里边的内容必须是引用或者指针，就连把 int 转成 int 都不行。 reinterpretcast 主要有三种强制转换用途： 1、改变指针或引用的类型 2、将指针或引用转换为一个足够长度的整形 3、将整型转换为指针或引用类型。 用法为 reinterpretcast <type-id> (expression)。 type-id 必须是一个指针、引用、算术类型、函数针或者成员指针。它可以把一个指针转换成一个整数，也可以把一个整数转换成一个指针（先把一个指针转换成一个整数，在把该整数转换成原类型的指针，还可以得到原先的指针值）。 我们映射到的类型仅仅是为了故弄玄虚和其他目的，这是所有映射中最危险的。(这句话是C++编程思想中的原话)。因此, 你需要谨慎使用 reinterpretcast。 用法为 dynamiccast<type-id> (expression)。 几个特点如下： （1）其他三种都是编译时完成的，==dynamiccast 是运行时处理的==，==运行时要进行类型检查==。 （2）不能用于内置的基本数据类型的强制转换 （3）dynamiccast 要求 <> 内所描述的目标类型必须为指针或引用。dynamiccast 转换如果成功的话返回的是指向类的指针或引用，转换失败的话则会返回 nullptr （4）在类的转换时，在类层次间进行上行转换（子类指针指向父类指针）时，dynamiccast 和 staticcast 的效果是一样的。在进行下行转换（父类指针转化为子类指针）时，dynamiccast 具有类型检查的功能，比 staticcast 更安全。 向下转换的成功与否还与将要转换的类型有关，即要转换的指针指向的对象的实际类型与转换以后的对象类型一定要相同，否则转换失败。在C++中，编译期的类型转换有可能会在运行时出现错误，特别是涉及到类对象的指针或引用操作时，更容易产生错误。Dynamiccast操作符则可以在运行期对可能产生问题的类型转换进行测试。 （5）使用 dynamiccast 进行转换的，基类中一定要有虚函数，否则编译不通过（类中存在虚函数，就说明它有想要让基类指针或引用指向派生类对象的情况，此时转换才有意义）。这是由于运行时类型检查需要运行时类型信息，而这个信息存储在类的虚函数表中，只有定义了虚函数的类才有虚函数表",
    "url": "/c++/C++的四种强制类型转换.html",
    "lang": ""
  },
  {
    "title": "c++的好处",
    "content": "--- title: c++的好处 --- 1.贴近硬件 使用原生的指令和类型，高性能 方便使用新的硬件（包括gpu,fpga等） 2.零开销抽象（不用抽象机制就没有开销） 类、继承、模版、类型别名。。。 将来：完全的类型和资源安全，概念、模块、协程、契约、静态反射",
    "url": "/c++/c++的好处.html",
    "lang": ""
  },
  {
    "title": "C++的类型推导",
    "content": "--- title: C++的类型推导 --- auto，用于通过一个表达式在编译时确定待定义的变量类型，auto 所修饰 的变量必须被初始化，编译器需要通过初始化来确定auto 所代表的类型，即必须要定义变 量。 若仅希望得到类型，而不需要（或不能）定义变量的时候应该怎么办呢？ C++11 新增了decltype 关键字，用来在编译时推导出一个表达式的类型。",
    "url": "/c++/C++的类型推导.html",
    "lang": ""
  },
  {
    "title": "C++程序内存分区",
    "content": "--- title: C++程序内存分区 --- 很多情况下，提到 C++ 程序的内存分区时，会简化为下面五个主要区域 栈（Stack）: 用于存储局部变量和函数调用的上下文。栈的内存分配是自动的，由编译器管理。 堆（Heap）: 用于动态内存分配。程序员可以使用 new、malloc 等操作符或函数从堆上分配内存，并使用 delete、free 释放内存。 全局/静态存储区（Global/Static Storage）: 存储全局变量和静态变量，包括： 数据段：存储初始化的全局变量和静态变量。 BSS 段 ：存储未初始化的全局变量和静态变量。 常量区（Constant Data）: 存储程序中的常量数据，如字符串字面量。此外，如果有定义了虚函数，常量区还会存储虚表。 代码段（Code Segment 或 Text Segment）: 存储程序的可执行代码和函数的二进制指令。",
    "url": "/c++/C++程序内存分区.html",
    "lang": ""
  },
  {
    "title": "C++面向对象三大特性",
    "content": "--- title: C++面向对象三大特性 --- 面向对象编程具有封装、继承和多态三个主要特性： 封装：将客观事物封装成为抽象的类, 类把自己数据和方法进行隐藏，仅对外公开接口来和对象进行交互，防止外界干扰或不确定性访问。 继承：指一个类（称为子类或派生类）可以从另一个类（称为父类或基类）中继承属性和行为的能力。通过继承，子类可以重用父类的代码，并且可以在不修改父类的情况下添加新的功能或修改已有的功能。继承使得代码具有层次性和可扩展性，能够建立起类之间的层次关系。 多态：多态是指同一个操作作用于不同的对象时，可以有不同的解释和行为。多态性允许以统一的方式处理不同类型的对象，从而提高了代码的可扩展性和可维护性。在C++中，多态性通常通过虚函数（virtual functions）来实现。通过基类中定义虚函数，并在派生类中重写该函数，可以实现运行时的动态绑定。",
    "url": "/c++/C++面向对象三大特性.html",
    "lang": ""
  },
  {
    "title": "constexpr",
    "content": "--- title: constexpr --- 编译器会将constexpr函数视为内联函数！所以在编译时若能求出其值，则会把函数调用替换成结果值。 类的构造函数也可以使用constexpr关键字 constexpr还能修饰类的构造函数，即保证传递给该构造函数的所有参数都是constexpr，那么产生的对象的所有成员都是constexpr。该对象是constexpr对象了，可用于只使用constexpr的场合。 注意constexpr构造函数的函数体必须为空，所有成员变量的初始化都放到初始化列表中。 c++ using namespace std; class Test { public: constexpr Test(int num1, int num2) : mnum1(num1), mnum2(num2) { } public: int mnum1; int mnum2; }; int main(void) { constexpr Test t1(1, 2); enum e { x = t1.mnum1, y = t1.mnum2 }; return 0; } （1）const 和 constexpr 变量之间的主要区别在于：==const 变量的初始化可以延迟到运行时==，而 ==constexpr 变量必须在编译时进行初始化==。所有 constexpr 变量均为常量，因此必须使用常量表达式初始化。 （2）constexpr和指针 在使用const时，如果关键字const出现在星号左边，表示被指物是常量；如果出现在星号右边，表示指针本身是常量；如果出现在星号两边，表示被指物和指针两者都是常量。 ​ 与const不同，在constexpr声明中如果定义了一个指针，限定符constexpr仅对指针有效，与指针所指对象无关。 constexpr是一种很强的约束，更好的保证程序的正确定语义不被破坏；编译器可以对constexper代码进行非常大的优化，例如：将用到的constexpr表达式直接替换成结果, 相比宏来说没有额外的开销。 c++ using namespace std; int gtempA = 4; const int gconTempA = 4; constexpr int gconexprTempA = 4; int main(void) { int tempA = 4; const int conTempA = 4; constexpr int conexprTempA = 4; /1.正常运行，编译通过/ const int &conptrA = tempA; const int &conptrB = conTempA; const int &conptrC = conexprTempA; /2.有两个问题：一是引用到局部变量，不能再编译器确定；二是conexprPtrB和conexprPtrC应该为constexpr const类型，编译不过/ constexpr int &conexprPtrA = tempA; constexpr int &conexprPtrB = conTempA; constexpr int &conexprPtrC = conexprTempA; /3.第一个编译通过，后两个不通过，原因是因为conexprPtrE和conexprPtrF应该为constexpr const类型/ constexpr int &conexprPtrD = gtempA; constexpr int &conexprPtrE = gconTempA; constexpr int &conexprPtrF = gconexprTempA; /4.正常运行，编译通过/ constexpr const int &conexprConPtrD = gtempA; constexpr const int &conexprConPtrE = gconTempA; constexpr const int &conexprConPtrF = gconexprTempA; return 0; } 简单的说constexpr所引用的对象必须在编译期就决定地址。还有一个奇葩的地方就是可以通过上例conexprPtrD来修改gtempA的值，也就是说constexpr修饰的引用不是常量，如果要确保其实常量引用需要constexpr const来修饰。 C++ 11标准中，为了解决 const 关键字的双重语义问题，保留了 const 表示“只读”的语义，而将“常量”的语义划分给了新添加的 constexpr 关键字。因此 C++11 标准中，建议将 const 和 constexpr 的功能区分开，即凡是表达“只读”语义的场景都使用 const，表达“常量”语义的场景都使用 constexpr。",
    "url": "/c++/constexpr.html",
    "lang": ""
  },
  {
    "title": "delete 和 free 有什么区别？",
    "content": "--- title: delete 和 free 有什么区别？ --- delete和free都是用来释放动态分配的内存，但它们有不同的使用方式： 语法： delete是C++中的关键字，用于释放由new分配的对象。 free是C语言中的函数，通常包含在<stdlib.h>头文件中，用于释放由malloc分配的内存。 对象销毁： 当使用 delete 释放对象内存时，C++ 编译器会自动调用对象的析构函数，释放与对象相关的资源，并执行对象的清理工作。 free 仅释放内存，不调用析构函数。因此，如果使用 malloc 分配了 C++ 对象的内存，需要手动调用析构函数后再调用 free。 数组处理： 如果是数组，C++提供了delete[]来释放整个数组的内存，而C语言中仍然使用free，没有区分单个对象和数组。 返回值: free 没有返回值，即使内存释放失败，也不会反馈任何信息。 delete 之后，指针会自动置为 nullptr 类型检查: delete 进行类型检查，确保删除的对象类型与 new 分配时的类型一致。 free 不进行类型检查，因为它只处理 void 类型的指针。 总结来说，delete和free都是用来释放动态内存的，但它们分别用于C++和C语言中的内存管理。delete适用于C++对象，会自动调用析构函数；而free适用于C语言分配的内存，不涉及对象的析构。",
    "url": "/c++/delete 和 free 有什么区别？.html",
    "lang": ""
  },
  {
    "title": "function和lamda以及bind",
    "content": "--- title: function和lamda以及bind --- 是一个抽象了函数参数和函数返回值的类模板 把任意函数包装成一个对象，该对象可以保存，传送和复制。 动态绑定，只要修改对象（赋值不同的function对象），实现类似多态的效果 保存普通函数，类的静态成员函数 保存仿函数 保存类成员函数 保存lambda表达式 保存bind返回的函数对象 重载了操作符（）的类 可以有状态，通过成员变量进行存储状态 有状态的函数对象称之为闭包 是一种方便创建匿名函数的语法糖 []捕获列表 值捕获：默认只读，不能修改。 添加mutable之后可读可写，但是并不会修改外部变量的值 引用捕获：可读可写，会修改外部变量的值 []捕获列表本质是将外部变量转变为类的成员变量 ()参数列表 ->指定返回值 编译的时候，将lambda表达式转变为一个函数对象 根据lambda参数列表重载operator()操作符。 用来通过绑定函数以及函数参数的方式生成函数对象的模板函数 提供占位符，实现灵活参数绑定 绑定函数以及函数参数 允许修改参数顺序 function用来描述函数对象的类型； lambda表达式用来生成函数对象（可以访问外部变量的匿名函数） bind也是用来生成函数对象（函数和参数进行绑定生成函数对象）",
    "url": "/c++/function和lamda以及bind.html",
    "lang": ""
  },
  {
    "title": "map 和 unordered_map的区别和实现机制",
    "content": "--- title: map 和 unorderedmap的区别和实现机制 --- map 基于红黑树：std::map 基于一种自平衡的二叉搜索树（通常是红黑树）实现，可以保持元素有序。 有序容器：元素按照键的顺序自动排序，可以通过键值进行有序遍历。 元素访问：提供对元素的快速查找、插入和删除操作，时间复杂度为 O(log n)。 唯一键：每个键都是唯一的，不允许有重复的键。 迭代器稳定性 ：由于基于树结构，迭代器在遍历时是稳定的，即使容器发生插入或删除操作，迭代器指向的元素也不会改变，除非该元素被删除。 unorderedmap 基于哈希表：std::unorderedmap 基于哈希表实现，通过哈希函数将键分布到数组的槽位中。 无序容器：元素在容器中是无序的，不能按键的顺序进行遍历。 元素访问：理想情况下，提供平均时间复杂度为 O(1) 的快速查找、插入和删除操作。最坏情况下，性能可能下降到 O(n)。 允许重复键：实际上，std::unorderedmap 不允许有重复的键，因为哈希表的设计不允许两个元素具有相同的哈希值。如果发生哈希冲突，会通过某种方式（如链表或开放寻址）解决。 迭代器稳定性：由于基于哈希表，迭代器的稳定性不如 std::map。在发生哈希表的重新哈希 （rehashing） 时，迭代器可能会失效。 遍历顺序与创建该容器时输入元素的顺序是不一定一致的，遍历是按照哈希表从前往后依次遍历的。 使用场景 当需要元素有序且对性能有较高要求时，应选择 std::map。 当元素的顺序不重要，且需要快速访问元素时，应选择 std::unorderedmap。 实现机制 std::map 的实现依赖于红黑树的旋转和颜色变换来保持树的平衡，确保操作的时间复杂度。 std::unorderedmap 的实现依赖于一个良好的哈希函数来最小化冲突，并通过解决冲突的机制（如链表或开放寻址）来存储具有相同哈希值的元素。",
    "url": "/c++/map 和 unordered_map的区别和实现机制.html",
    "lang": ""
  },
  {
    "title": "new和malloc有什么区别",
    "content": "--- title: new和malloc有什么区别 --- 一、首先new是c++的的操作符，由编译器实现，可以直接用来分配对象或者数组，并且分配失败会抛出std::badalloc异常。 new表达式一共有三步： 1.调用operator new库函数，分配未类型化的空间，用来保存指定类型的一个对象，而在operator new函数中，会调用malloc函数。 2.运行该类型的构造函数来初始化对象。 3.返回指向对象的指针 然后new分配的对象需要使用delete来进行释放，delete会自动调用对象的析构函数 二、然后再来说malloc，malloc是一个C标准库的函数，它只会分配原始内存，不调用构造函数。返回类型是 void，需要强制类型转换为具体的指针类型。malloc分配的内存需要调用free来进行释放，free不会调用析构函数。malloc分配失败的时候只会返回空指针 malloc具体的底层实现有两个分支， 当我们开辟的空间小于128K的时候，是根据brk函数实现的，brk是将数据段的最高地址指针edata往高地址推，但是这一步只是完成了虚拟地址的分配，这块内存还是没有和物理页对应的，等到进程第一次读写A这块内存的时候，会发生缺页中断，这个时候才会真正分配。 当我们开辟的空间大于128K的时候，是利用的mmap系统调用，从文件映射区中分配一块虚拟内存。而mmap分配的内存是可以单独释放的。 malloc() 在分配内存的时候，会预分配更大的空间作为内存池。 也就是说，当我们调用brk()系统调用申请堆内存，仍然会为我们分配超过128KB字节的内存。即使你 malloc()的内存远小于128k也是一样。 为什么malloc要使用内存池来进行管理呢？ 提高效率：内存池预先分配一大块内存，然后在需要时从中分配小块，减少了频繁调用系统级别的内存分配开销（brk()\\mmap()）「每次调用 sbrk 或 mmap 时，程序需要从用户态切换到内核态，这个切换过程本身就有一定的开销；陷入内核态也涉及到了系统需要更新内存管理的数据结构，如页表和内存分配表，需要额外的时间；再一个，为了保证内存分配的线程安全，内核需要处理同步和锁管理，增加了开销」。 现在你还觉得每次调用malloc都会陷入内核态吗？ 减少了内存碎片，内存池通过组织和管理内存块，能更好地利用内存空间，减少内存碎片，提高内存使用率。 快速释放和重用，跟第一点一样，在池内管理，避免了系统调用的高开销。 看了malloc竟然开辟了内存池，你觉得free会把内存归还给操作系统吗？ 这里直接说结论： 通过 free 释放内存后，堆内存还是存在的，并没有归还给操作系统。当然，当进程退出后，操作系统就会回收进程的所有资源。 new和malloc在C++中都用于动态内存分配，但它们之间有几个关键的区别： 语法层面： new是C++的操作符，可以直接用来分配对象或数组。 malloc是一个函数，通常需要包含头文件<cstdlib>，并且只分配原始内存。 类型安全： new是类型安全的，它会根据分配的对象类型进行正确的内存分配和构造函数调用。 malloc 不是类型安全的，它只分配原始内存，不调用构造函数。返回类型是 void，需要强制类型转换为具体的指针类型。 构造与析构： 使用 new 分配的对象在对象生命周期结束时需要使用 delete 来释放，delete 会自动调用对象的析构函数。 使用 malloc 分配的内存需要使用 free 来释放，free 不会自动调用析构函数，因此如果分配的是对象数组，需要手动调用析构函数。 异常安全性： new在分配失败时会抛出std::badalloc异常。 malloc在分配失败时返回NULL指针。 管理机制： C++中的new和delete通常由编译器实现，可能包含一些额外的内存管理机制。 C语言的malloc和free由C标准库提供，与编译器无关。 总结来说，new和malloc都是动态内存分配的手段，但new提供了类型安全和构造/析构的自动化，而malloc则提供了更底层的内存分配方式，需要手动管理构造和析构。在C++中，推荐使用new来分配对象，以保持类型安全和自动化的资源管理。 new是C++的操作符，由编译器实现，可以直接用来分配对象或数组，并且在分配失败会抛出std::badalloc异常。 new表达式工作步骤有三步 ​ 1.调用operator new库函数，分配未类型化的空间，用来保存指定类型的一个对象，而在operator new函数中，会调用malloc函数 ​ 2.运行该类型的构造函数初始化对象 ​ 3.返回指向对象的指针 malloc是一个库函数，通常需要包含头文件<cstdlib>，并且只分配原始内存，分配失败时会返回NULL。 new是类型安全的，它会根据分配的对象类型进行正确的内存分配和构造函数调用。 malloc 不是类型安全的，它只分配原始内存，不调用构造函数。返回类型是 void，需要强制类型转换为具体的指针类型。 使用 new 分配的对象在对象生命周期结束时需要使用 delete 来释放，delete 会自动调用对象的析构函数。 使用 malloc 分配的内存需要使用 free 来释放，free 不会自动调用析构函数，因此如果分配的是对象数组，需要手动调用析构函数。 总结来说，new和malloc都是动态内存分配的手段，但new提供了类型安全和构造/析构的自动化，而malloc则提供了更底层的内存分配方式，需要手动管理构造和析构。 placementnew 允许在已经分配的内存地址上构造对象 ​ 1.new表达式工作步骤 ​ 1.调用operator new库函数，分配未类型化的空间，用来保存指定类型的一个对象 ​ 2.运行该类型的构造函数初始化对象 ​ 3.返回指向对象的指针 ​ 2.delete表达式工作步骤 ​ 1.调用析构函数，回收对象中数据成员申请资源 ​ 2.调用operator delete的标准库函数释放该对象所用的内存 malloc的底层实现是怎样的？free是怎么回收内存的？ 答： 1、当开辟的空间A小于128K的时候，malloc底层调用brk()函数， brk是将数据段的最高地址指针edata往高地址推，edata+30K 只是完成虚拟地址的分配，A这块内存现在还是没有物理页与之 对应的，等到进程第一次读写A这块内存的时候，发送缺页中断， 这个时候，内核才会分配给A这块内存对应的物理页。也就是说， 如果用malloc分配了A这块内容，然后从来不访问它，那么A对应 的物理页是不会被分配的。 而当开辟的空间大于128K的时候，利用mmap系统调用，从堆 和栈的中间也就是文件映射区分配一块虚拟内存。这样做主要是因为： brk分配的内存需要等到高地址内存释放以后才能释放（比如 先申请A再申请B，所以B相对于A来说是高地址空间，所以在B释放 之前，A是不可能释放的，因为只有一个edata指针，这就是内存 碎片产生的原因。而mmap分配的内存可以单独释放。 2、free回收内存时首先要知道这块内存的地址多大，所以在malloc 返回的地址的前一小段是存在空闲内存块的链表中，存储这一块有 多大的信息。对使用mmap机制分配空间的malloc返回的地址free时， 直接将其对应的虚拟内存和物理内存一起释放。而对brk机制分配 空间的malloc返回的地址free时，并不会立刻释放，默认情况下， 当最高地址空间的空闲内存超过128K时，执行内存紧缩操作(trim) .所以当free之后，发现最高地址空闲内存超过128K时，就会内存 紧缩。 new/delete与malloc/free的区别与联系是什么？(面试常考) 答： 1、malloc开辟空间类型大小需手动计算，new是由编译器自己计算。 2、malloc返回类型为void,必须强转成对应类型指针，而new直接 返回对应类型指针。 3、malloc开辟内存时返回内存地址要检查判空，因为若它可能跟开 辟失败会返回NULL；new则不用判断，因为内存分配失败时，它会抛 出异常bacalloc，可以使用异常机制。 4、无论释放几个空间大小，free只传递指针，而多个对象时，delete 需加[]，(分配的大小是数组空间大小). 5、malloc/free函数只是开辟空间并释放，new/delete则不仅会开辟 空间，并调用构造函数和析构函数进行初始化和清理。 6、new/delete底层是基于malloc/free来实现的。 7、因为new/delete是操作符，它调用operator new/operator delete， 它们可以被重载，在标准库里它有8个重载版本；而malloc/free不可以 重载。 8、对于malloc分配内存后，若在使用过程中内存分配不够或太多，这时 可以使用realloc函数对其进行扩充或者缩小，但是new分配好的内存不能 这样被直观简单的改变。 9、对于malloc申请内存的位置是在堆上分配内存的；但是new申请内存 有一个抽象概念，它为自由存储区，它可以在堆上，也可以在静态存储 区上分配，这主要取决于operator new实现细节，取决于它在哪里为对象 分配空间。",
    "url": "/c++/new和malloc有什么区别.html",
    "lang": ""
  },
  {
    "title": "push_back()和emplace_back()的区别",
    "content": "--- title: pushback()和emplaceback()的区别 --- pushback()和emplaceback()都是C++标准库容器（如std::vector）中用来添加元素的方法，但它们在添加元素的方式上有所不同： pushback()： 语法是container.pushback(value);，传入的是一个值或对象的副本/移动版本。 它接受一个元素的副本或移动该元素，然后将其添加到容器的末尾。 这个方法需要先构造一个元素的副本或移动构造一个临时对象，然后再将其添加到容器中。 emplaceback()： 语法是container.emplaceback(args...);，传入的是构造新元素所需的参数列表。 它使用就地构造（emplace）的方式，直接在容器的内存空间中构造新元素。 这个方法避免了元素的复制或移动操作，因为它直接在容器的末尾空间构造新元素。 性能： 在参数为纯右值（临时对象）时，emplaceback()比pushback()更高效，因为它避免了额外的复制或移动操作。 当构造函数需要大量资源时，emplaceback()的优势更加明显。 陷阱： emplaceback会让隐式转换通过编译，可能产生UB。比如regexes.emplaceback(nullptr); pushback的隐式转换不会通过编译。regexes.emplaceback(nullptr);会发生错误 emplacement比pushback更不安全",
    "url": "/c++/push_back()和emplace_back()的区别.html",
    "lang": ""
  },
  {
    "title": "static关键字和const关键字的作用",
    "content": "--- title: static关键字和const关键字的作用 --- static和const是C++ 中两个常用的关键字, 有以下作用： static 关键字: 用于控制变量和函数的生命周期、作用域和访问权限。 声明静态变量：静态变量的生命周期直到程序结束。当在函数内部声明静态变量时，即使函数执行完了也不会释放它，下次调用该函数时会保留上次的状态。 在类中，被static声明的成员被称为静态成员。 静态成员变量：在类中使用static关键字修饰的成员变量，表示该变量属于类而不是类的实例，所有实例共享同一份数据 静态成员函数：在类内使用static关键字修饰的成员函数，所有对象共享同一个函数；静态成员函数只能访问静态成员变量；静态成员函数调用可以不需要通过创建类的实例，而是直接通过类名调用。 static变量如果被多个线程访问，需要特别注意线程安全问题。 const: 关键字用于定义常量，即一旦初始化后其值不能被修改： 常量变量：声明变量，使变量的值不能修改（只读） 常量成员函数，表示该函数不会修改对象的成员变量 常量指针：可以指向一个 const 类型的值，或者是一个指向 const 值的指针，表明指针指向的值不能通过这个指针被修改。 const变量由于其不可变性，天然具有线程安全性。 有时候static和const 可以组合使用，如static const变量，表示一个静态的常量。 总结来说，static关键字用于创建类的静态成员，而const 关键字用于定义常量。",
    "url": "/c++/static关键字和const关键字的作用.html",
    "lang": ""
  },
  {
    "title": "stdforward完美转发",
    "content": "--- title: stdforward完美转发 --- std::forward是c++ 11引入的一个函数模板，用来实现完美转发，它的作用是根据传入的参数，决定将参数以左值引用还是右值引用的方式进行转发。完美转发是为了解决传递参数时的临时对象被强制转换为左值的问题。 在类型推导的场景下，比如一个 T&&，被称为万能引用，这个类型既可以绑定左值又可以绑定右值 std::forward呢有两个重载形式，一个参数是左值，一个参数是右值， 并且在参数列表中使用了一个removereference获得原始类型， 然后在函数体中，利用了staticcast强转成万能引用的类型，这个时候还会涉及到一个引用折叠的机制，就是两个不同类型的引用 在一起使用，只要存在左值引用，那么折叠的结果就是一个左值引用，只有当两个引用都是右值引用的时候，结果才是一个右值引用。 std::forward基于这点可以实现完美转发 ==使用 std::forward 之前的 int&& 是有名字的变量 t，它是一个左值，而使用 std::forward 返回之后的 int&& 是有个匿名变量，它是一个右值，真正的差距就在这里。== c++ template<class T> T&& forward(typename std::removereference<T>::type& t) noexcept { return staticcast<T&&>(t); } template <class T> T&& forward(typename std::removereference<T>::type&& t) noexcept { return staticcast<T&&>(t); } std::forward是C++11中引入的一个函数模板，用于实现==完美转发==。它的作用是根据传入的参数，==决定将参数以左值引用还是右值引用的方式进行转发。== 传统上，当一个左值传递给一个函数时，参数会以左值引用的方式进行传递；当一个右值传递给一个函数时，参数会以右值引用的方式进行传递。完美转发是为了解决传递参数时的临时对象（右值）被强制转换为左值的问题。std::forward实现完美转发主要用于以下场景：提高模板函数参数传递过程的转发效率。 对于形如T&&的变量或者参数，如果T可以进行推导，那么T&&称之为万能引用。换句话说，对于形如T&&的类型来说，其既可以绑定左值，又可以绑定右值，而这个的前提是T需要进行推导。最常见的万能引用方式如以下两种： c++ template<typename T> void f(T&& param); // 存在类型推导，param是一个万能引用 c++ auto&& var = var1; // 存在类型推导，var是一个万能引用 注意：只有当发生自动类型推断时（例如：函数模板的类型自动推导），T &&才是万能引用。下面一个示例中的&& 并不是一个万能引用，例如： c++ template<typename T> void f( T&& param); // 这里T的类型需要推导，所以&&是一个 universal references template<typename T> class Test { Test(Test&& rhs); // Test是一个特定的类型，不需要类型推导，所以&&表示右值引用 }; 万能引用进行类型推导时需要推导出T&&中的T的真实类型：若传入的参数是一个左值，则T会被推导为左值引用；而如果传入的参数是一个右值，则T会被推导为原生类型（非引用类型）。 对于万能引用来说，条件之一就是类型推导，但是类型推导是万能引用的必要非充分条件，也就是说参数必须被声明为T&&形式不一定是万能引用。示例如下： c++ template<typename T> void func(std::vector<T>&& t); // t是右值引用 调用func时会执行类型推导，但是参数t的类型声明的形式并非T &&而是std::vector &&。 之前强调过，万能引用必须是T &&才行，因此，t是一个右值引用，如果尝试将左值传入，编译器将会报错： c++ std::vector<int> v; fun(v); // 编译错误，不能将左值绑定到右值 形如const T&&的方式也不是万能引用： c++ template<typename T> void f(const T&& t); // t是右值引用 int main() { int a = 0; f(a); // 错误 } 引用折叠是一种特性，允许在模板元编程中使用引用类型的参数来创建新的引用类型。由于存在T&&这种万能引用类型，当它作为参数时，有可能被一个左值/左值引用或右值/右值引用的参数初始化，这需要通过类型推导，推导后得到的参数类型会发生类型变化，这种变化就称为引用折叠。 根本原因是因为C++中禁止reference to reference，所以编译器需要对四种情况（& &、& &&，&& &,&& &&）进行处理，将他们折叠成一种单一的reference。引用折叠的规则如下：如果两个引用中至少其中一个引用是左值引用，那么折叠结果就是左值引用；否则折叠结果就是右值引用。示例如下： c++ using T = int &; T& r1; // int& & r1 -> int& r1 T&& r2; // int& && r2 -> int& r2 using U = int &&; U& r3; // int&& & r3 -> int& r3 U&& r4; // int&& && r4 -> int&& r4 c++ template<typename T> void func(T &&t) { cout << \"hello world\" << endl; } int main() { int a = 1; int &b = a; func(a); // T 推导成 int &; T && ==> int & && ==> int & func(b); // T 推导成 int &; T && ==> int & && ==> int & func(1); // T 推导成 int; T && ==> int && func(std::move(a)); // T 推导成 int &&; T && ==> int && && ==> int && return 0; } 完美转发是为了解决传递参数时的临时对象（右值）被强制转换为左值的问题，std::forward源码如下： c++ template<class T> T&& forward(typename std::removereference<T>::type& t) noexcept { return staticcast<T&&>(t); } template <class T> T&& forward(typename std::removereference<T>::type&& t) noexcept { return staticcast<T&&>(t); } 其内部实现只有一行代码，即staticcast<T&&>(t)使用staticcast<>进行类型转换，与std::move()实现方式类似。结合前面介绍的引用折叠，当接收一个左值作为参数时，std::forward<>()返回左值引用，相应的，当接收一个右值作为参数时，std::forward<>()返回右值引用。 下面给出一个案例没有实现完美转发，如下： c++ template <typename T> void wrapper(T u) { fun(u); } class MyClass {}; void fun(MyClass& a) { std::cout << \"in fun(MyClass&)\\n\"; } void fun(const MyClass& a) { std::cout << \"in fun(const MyClass&)\\n\"; } void fun(MyClass&& a) { std::cout << \"in fun(MyClass &&)\\n\"; } int main(void) { MyClass a; const MyClass b; fun(a); fun(b); fun(MyClass()); std::cout << \"----- Wrapper ------\\n\"; wrapper(a); wrapper(b); wrapper(MyClass()); return 0; } 输出结果： c in func(MyClass&) in func(const MyClass&) in func(MyClass &&) ----- Wrapper ------ in func(MyClass&) in func(const MyClass&) in func(const MyClass&) Process returned 0 (0x0) execution time : 0.253 s Press any key to continue. 最后一行函数调用结果不符合预期，传入的是MyClass &&右值引用，预期调用fun(MyClass&& a)，实际上调用的却是fun(const MyClass& a)。调用wrapper函数时触发拷贝构造，基于右值创建了左值u（即：wrapper函数的参数），u的实际类型是const MyClass，匹配的是fun(const MyClass& a) 使用std::forward实现完美转发 c++ template <typename T> void wrapper(T &&u) { // 万能引用 func(std::forward<T>(u)); // 完美转发 } class MyClass {}; void func(MyClass& a) { std::cout << \"in func(MyClass&)\\n\"; } void func(const MyClass& a) { std::cout << \"in func(const MyClass&)\\n\"; } void func(MyClass&& a) { std::cout << \"in func(MyClass &&)\\n\"; } int main(void) { MyClass a; const MyClass b; func(a); func(b); func(MyClass()); std::cout << \"----- Wrapper ------\\n\"; wrapper(a); wrapper(b); wrapper(MyClass()); return 0; } c++ in func(MyClass&) in func(const MyClass&) in func(MyClass &&) ----- Wrapper ------ in func(MyClass&) in func(const MyClass&) in func(MyClass &&) Process returned 0 (0x0) execution time : 0.210 s Press any key to continue. std::forward()建议仅用于模板函数，对于非模板的，因为不涉及到类型推导，所以使用完美转发是没有意义的。 疑惑 可能有人会说，这不对啊，使用 std::forward 修改之前函数参数就是 int&& 类型，修改之后得到的返回值还是 int&& 类型，这有什么区别吗？ 这里的区别就在于，使用 std::forward 之前的 int&& 是有名字的变量 t，它是一个左值，而使用 std::forward 返回之后的 int&& 是有个匿名变量，它是一个右值，真正的差距就在这里。 具名变量都是左值",
    "url": "/c++/stdforward完美转发.html",
    "lang": ""
  },
  {
    "title": "stdsort的优化实现",
    "content": "--- title: stdsort的优化实现 --- 首先说明快排的partition的pivot的取法 众所周知，快速排序算法的具体时间复杂度取决于递归时数组的分割比例。比如说，如果我们在每个递归中都能将数组分成均等的两份，那么算法的时间复杂度将是最佳的O(nlogn)；而如果我们在每个递归中都倒霉地把数组分成大小1和大小n-1这样的两份，快速排序就退化成了递归版的冒泡排序，时间复杂度为O(n^2)。 比较好的选择：三数取中法，即取三个元素（一般是第一个、最后一个和中间的元素）的中间值 元素数量较小的数组对快排不友好，此时快排需要开栈递归，选pivot，分数组，所以不划算，当元素数量较小时，可以直接用插入排序。 还可以使用预排序检查，在实践中，大部分数组经常出现有序片段，所以这个优化有不错的提升 当元素个数小于16的时候，使用插入排序。 或者超过了最大递归深度，采用堆排序 否则使用快排 初始化：设置递归深度限制2log2n，元素分区大小阈值1016 如果数组大小大于等于阈值，且递归深度没有达到限制，使用快排 快排平均为O(nlog2n)，但是最坏会退化到O(n^2)，最坏情况为已经排序或者接近排序的情况 快排是不稳定的排序 如果数组大小小于阈值，则使用插入排序 在数据规模较小，且相对有序的情况下，插入排序在最好情况下比较高效(O(n)) 插入排序是稳定的排序 如果数组大小大于等于阈值，且递归深度达到限制，使用堆排序 堆排序的时间复杂度是O(nlog2n) 所以std::sort是不是稳定的排序算法？ 数据量小是的 数据量大不是的",
    "url": "/c++/stdsort的优化实现.html",
    "lang": ""
  },
  {
    "title": "STL 容器了解哪些",
    "content": "--- title: STL 容器了解哪些 --- 序列容器 std::vector: 动态数组，提供快速随机访问。 std::deque: 双端队列，提供从两端快速插入和删除的能力。 std::list: 双向链表，提供高效的元素插入和删除。 std::forwardlist: 单向链表，每个元素只存储下一个元素的引用。 std::array: 固定大小的数组，具有静态分配的内存。 关联容器: std::set: 基于红黑树，存储唯一元素的集合, 会默认按照升序进行排序。 std::multiset: 允许容器中有多个相同的元素。 std::map: 基于红黑树，存储键值对的有序映射。 std::multimap: 允许映射中有多个相同的键。 std::unorderedset: 基于哈希表，提供平均时间复杂度为 O(1) 的查找。 std::unorderedmap: 基于哈希表，存储键值对的无序映射。 容器适配器（Container Adapters）: std::stack: 后进先出（LIFO）的栈。 std::queue: 先进先出（FIFO）的队列。 std::priorityqueue: 优先队列，元素按优先级排序。",
    "url": "/c++/STL 容器了解哪些.html",
    "lang": ""
  },
  {
    "title": "STL函数对象",
    "content": "--- title: STL函数对象 ---",
    "url": "/c++/STL函数对象.html",
    "lang": ""
  },
  {
    "title": "STL容器",
    "content": "--- title: STL容器 --- !image-20241002194431509 Element access at： operator[]： front： back： data：直接返回指向底层的数据的指针 Iterators begin： cbegin(C++11)： end： cend (C++11)： rbegin： crbegin(C++11)： rend： crend(C++11)： Capacity empty： size: maxsize:返回最大的可存储的元素数量 reserve： capacity： shrink to fit：将多余的容量压缩到和size一样 Modifiers clear： insert：第一个参数是迭代器pos，第二个参数是数值或者迭代器范围 insertrange(C++23)：不需要写迭代器范围 emplace(C++11)： vec.emplace(vec.begin(), \"World\"); vec.emplace(vec.begin() + 1, 3, 'a'); // 插入 \"aaa\" erase： pushback: emplaceback(C++11) appendrange (C++23) popback resize swap //deque的大小在64位为80字节，十个指针的大小 //deque有一个开始和结束迭代器， //每个分别有4个指针，cur、first、last、node //还有1个map指针，指向中控器(指针数组) //以及中控器指向第一个缓冲区的指针 Dequeiterator<Tp,Tp&,Tp> iterator Tp Mmap; iterator Mstart 和 Mfinish; [ Tp Mcur; Tp Mfirst; Tp Mlast; Mappointer Mnode; ] !image-20241002194915735 element access at operator[] front back iterators begin cbegin (C++11) end cend(C++11) rbegin crbegin(C++11) rend crend (C++11) Capacity empty size maxsize shrink to fit Modifiers clear insert insert range (C++23) emplace(C++11) erase push back emplaceback(C++11) append range (C++23) pop back push front emplace front(C++11) prepend range(C++23) popfront resize swap Element acces at operator[] front back data iterators begin cbegin end cend rbegin crbegin rend crend Capacity empty size max size Operations fill swap Element access front back Iterators begin cbegin (C++11) end cend (C++11) rbegin crbegin (C++11) rend crend (C++11) Capacity empty size maxsize Modifiers clear insert insert range(C++23) emplace(C++11) erase push back emplace back(C++11) append range (C++23) pop back push front emplace front(C++11) prepend range(C++23) popfront resize swap Operations merge splice remove remove if reverse unique sort",
    "url": "/c++/STL容器.html",
    "lang": ""
  },
  {
    "title": "空间配置器的基本原理",
    "content": "--- title: STL空间配置器 --- 为了精密分工，STL将new和delete分别的两阶段动作分开： 内存配置操作由成员函数allocate()负责 内存释放操作由成员函数deallcate()负责 对象构造操作由成员函数construct()负责 对象析构操作由成员函数destroy()负责 内存分配过程中需要考虑的问题： 小块内存带来的内存碎片问题 小块内存频繁申请释放带来的性能问题 内存碎片问题是指;单从分配的角度来看。由于频繁分配、释放小块内存容易在堆中造成外部碎片(极端情况下就是堆中空闲的内存总量满足一个请求，但是这些空闲的块都不连续，导致任何一个单独的空闲的块都无法满足这个请求)。 为了解决该类问题，STL设计了==双层级配置器==，也就是第一级配置器和第二级配置器 第一级配置器直接使用malloc和free； 第二级配置器则视情况采 用不同的策略： ①当配置区块大于128bytes，将其视作足够大，便调用第一级配置器，使用malloc和free； ②==当配置区块小于128bytes，将其视作过小，为降低额外负担，便采用内存池的管理方式== 解释：内存池的思想：一次向heap申请一块很大的内存(内存池)，如果申请小块内存的话就直接到内存池中去要。这样的话，就能够有效的解决上面所提到的问题。 !image-20241002204854585 注意：==如果用户申请的内存大小不是8的倍数，二级配置器会将申请的字节数上调至距离用户申请大小的最近的8的倍数处==。（这里带来了内部碎片问题，和之前谈到的外部碎片问题相比，这个问题我们无法避免） 问题： 内存块为什么必须要以8位单位呢，把1作为单位不就可以避免这个问题了吗？ ——我们的内存块是像链表一样连接起来的，这样就必然需要指针来维护， 32位平台下指针4字节，64位平台下指针8字节，所以内存块最小也要能存放一个指针吧，这样就清楚了为什么是8字节。 二级配置器是以内存池管理的，每次从系统中配置一大块内存，并维护与之对应的自由链表free-list 如果用户申请内存，有相同大小的需要就直接从free-list中分配。 如果用户归还内存时，将根据归还内存快的大小，将需要归还的内存插入到对应free-list的最顶端。 !image-20241002205243006 二级配置器的内存分配 1.freelist列表中有空余内存。如果申请3个字节的内存，则所需空间大小提升为8的倍数，然后去 freelist中查找相应的链表，如果 freelist[i] 不为空，则返回第一个元素，然后把头指针往后移。 ​ 2.freelist列表中没有空余，但内存池不为空。首先检验内存池中的大小是不是比申请的内存大，比如申请208的内存，如果足够，则分配相应内存，将其中一个分配给用户使用，其它的挂在相应的freelist 中。如果内存池不够大，只够几个内存分配，则就分配这几个，把相应的数据返回。如果连一个都不够则执行第三中情况。 freelist列表中没有空余，内存池也不够。调用malloc重新分配内存，分配时会多分配一倍的内存，把相应的内存挂到freelist下，剩余的放到内存池中。 freelist列表中没有空余，内存池也不够，malloc也失败。则调用一级空间配置器，里面会有循环处理，或者抛出异常。 !image-20241002212751875",
    "url": "/c++/STL空间配置器.html",
    "lang": ""
  },
  {
    "title": "STL算法",
    "content": "--- title: STL算法 --- 算法库中的函数都是非成员函数（自由函数、全局函数） 1、分类 1、非修改式的算法：foreach、count、find 2、修改式的算法：copy、removeif、swap 3、排序算法：sort 4、二分搜索：lowerbound、upperbound 5、集合操作：setintersection 6、堆相关的操作：makeheap 7、取最值：max、min 8、未初始化的操作：uninitializedcopy 2、foreach算法 一元函数：函数的参数只有一个。二元函数：函数的参数是两个。 一元谓词/断言：函数的参数只有一个，并且函数的返回类型是bool。 二元谓词/断言：函数的参数有两个，并且函数的返回类型是bool。",
    "url": "/c++/STL算法.html",
    "lang": ""
  },
  {
    "title": "STL迭代器",
    "content": "--- title: STL迭代器 ---",
    "url": "/c++/STL迭代器.html",
    "lang": ""
  },
  {
    "title": "STL适配器",
    "content": "--- title: STL适配器 --- 适配器在STL组件的灵活组合运用功能上，扮演着轴承、转换器的角色。Adapters这个概念，事实上是一种设计模式，在《Design Patterns》一书中对adapter样式的定义如下：将一个类（class）的接口转换成另一个类（class）的接口，使原本因接口不兼容而不能合作的两个类（classes）可以一起运作。 总而言之，就是把一个已经存在的东西，改成一个我们需要的东西。 适配器分为三种： 容器适配器（container adapters）: stack,queue,priorityqueue 仿函数适配器（function adapters）: bind,bind1st,bind2nd 迭代器适配器（iterator adapters）: reverseiterator, insertiterator, ostreamiterator stack,queue,priorityqueue",
    "url": "/c++/STL适配器.html",
    "lang": ""
  },
  {
    "title": "vector 底层原理和扩容过程",
    "content": "--- title: vector 底层原理和扩容过程 --- 底层原理 vector 是 C++ 标准库中的一个容器，可以看作是一个动态数组，它的大小可以根据元素的增加而增长。它通过在堆上分配一段连续的内存空间存放元素，支持时间复杂度为 O（1 ) 的随机访问。 vector 底层维护了三个迭代器和两个变量，这三个迭代器分别指向对象的起始字节位置，最后一个元素的末尾字节和整个 vector 分配空间的末尾字节。两个变量分别是 size 和 capacity ，Size 表示当前存储元素的数量，capacity 表示当前分配空间的大小。当创建一个 vector 对象时，会分配一个初始化大小的空间存放元素，初始化空间可以通过构造函数的参数指定，缺省情况下为 0。当对 vector 容器进行增加和删除元素时，只需要调整末尾元素指针，而不需要移动整个内存块。 扩容机制 当添加元素的数量达到当前分配空间的大小时，vector 会申请一个更大的内存块，然后将元素从旧的内存块拷贝到新的内存块中，并释放旧的内存块。 扩容可能导致原有迭代器和指针失效，扩容完成后，容器返回指向新内存区域的迭代器或指针。 vector 扩容的机制分为固定扩容和加倍扩容。 固定扩容就是在每次扩容时在原容量的基础上增加固定容量。但是固定扩容可能会面临多次扩容(扩容的不够大)的情况，时间复杂度较高。 加倍扩容就是在每次扩容时原容量翻倍，优点是使得正常情况下扩容的次数大大减少，时间复杂度低，缺点是空间利用率低。",
    "url": "/c++/vector 底层原理和扩容过程.html",
    "lang": ""
  },
  {
    "title": "vector和list的区别",
    "content": "--- title: vector和list的区别 --- vector 基于动态数组：std::vector 基于可以动态扩展的数组实现，这意味着它在内存中连续存储元素。 随机访问：提供快速的随机访问能力，可以通过索引快速访问任何元素。 内存分配：通常在内存分配上更紧凑，因为元素紧密排列，没有额外的空间用于链接或指针。 时间复杂度： 元素访问：O(1)，即常数时间复杂度。 插入和删除：在 vector 的末尾是 O(1)，但如果需要在中间插入或删除元素，则可能需要 O(n)，因为可能需要移动后续所有元素。 内存管理：使用连续内存分配，可以利用缓存的优势，提高访问速度。 list 基于双向链表：std::list 是基于双向链表的容器，每个元素通过节点链接到前一个和后一个元素。 非连续存储：元素在内存中不是连续存储的，每个元素包含指向前一个和后一个元素的指针。 时间复杂度： 元素访问：O(n)，需要从头开始遍历到所需位置。 插入和删除：非常快速，特别是当需要在列表中间插入或删除元素时，操作是 O(1)，前提是已经拥有指向待插入或删除元素的迭代器。 内存管理：由于元素间通过指针链接，内存分配可能更分散，但插入和删除操作不需要移动其他元素。 使用场景 std::vector： 当你需要快速随机访问元素时。 当你需要在末尾快速添加或删除元素时。 当你关心内存使用效率时。 std::list： 当你需要在列表中间高效地插入或删除元素时。 当你不需要随机访问元素时。 当你需要一个灵活的容器，可以动态地添加和删除元素而不会引起大量的内存复制或移动。",
    "url": "/c++/vector和list的区别.html",
    "lang": ""
  },
  {
    "title": "volatile关键字",
    "content": "--- title: volatile关键字 --- 在C++中，volatile保证数据的读和写不通过缓存，直接对主存进行操作，并且保证相关操作不会被编译器优化。此外还会保证volatile的变量的指令不会重排。并且volatile是没有原子性的，需要通过原子操作或者锁来实现。 场景：volatile多用于多线程的共享字段（标记位）且常被修改。",
    "url": "/c++/volatile关键字.html",
    "lang": ""
  },
  {
    "title": "什么是内存泄漏, 如何检测和防止？",
    "content": "--- title: 什么是内存泄漏, 如何检测和防止？ --- 如果程序的某一段代码在内存池中动态申请了一块内存而没有及时将其释放，就会导致那块内存一直处于被占用的状态而无法使用，造成了资源的浪费。内存泄漏并不是说物理上的消失掉了，是因为无法使用该区域，在外界看来这块内存就好像被泄漏了一样。 什么操作会导致内存泄漏 忘记释放内存：使用 new 或 malloc 等分配内存后，没有使用 delete 或 free 释放内存。 子类继承父类时，没有将基类的析构函数定义为虚函数。 野指针：指针被赋值为 nullptr 或重新赋值后，丢失了对先前分配内存的引用，导致无法释放。 循环引用：在使用引用计数的智能指针（如 std::sharedptr）时，循环引用会导致引用计数永远不会归零，从而无法释放内存。 使用不匹配的内存释放函数: 使用 delete 释放由 new[] 分配的内存，或使用 delete[] 释放由 new 分配的内存，这可能导致未定义行为。 资源未关闭：对于文件、网络连接等资源，如果没有正确关闭，虽然不直接导致内存泄漏，但会占用系统资源，可能导致资源耗尽。 如何检测：使用工具如Valgrind、AddressSanitizer或Visual Studio的诊断工具来检测内存泄漏。 如何避免 使用智能指针:优先使用 std::uniqueptr、std::sharedptr 等智能指针来自动管理内存。 确保资源释放: 对于手动分配的内存，确保在不再需要时使用 delete 或 free 释放。 内存泄漏检测工具: 在开发和测试阶段，定期使用内存泄漏检测工具检查程序。",
    "url": "/c++/什么是内存泄漏, 如何检测和防止？.html",
    "lang": ""
  },
  {
    "title": "什么是智能指针，C++有哪几种智能指针",
    "content": "--- title: 什么是智能指针，C++有哪几种智能指针 --- 智能指针是C++中用来自动管理动态分配内存的一种模板类，维护着一个指向动态分配对象的指针，并在智能指针对象被销毁时，自动释放该内存，从而避免内存泄漏。 C++有以下几种智能指针： std::uniqueptr：独占式拥有指针，保证同一时间只有一个uniqueptr指向特定内存，适用于不需要共享所有权的场景，如栈上对象的管理。 std::sharedptr：多个sharedptr可以共享同一内存，使用引用计数机制来管理内存的生命周期，适用于多个对象需要共享同一个资源的场景。 std::weakptr：弱引用，不会增加指向对象的共享引用计数，并还可以通过Lock方法提升成为sharedptr，可以用于解决sharedptr可能导致的循环引用问题，但不是说他只用来解决这个问题，根据不同的业务场景，可以灵活使用，比如用它去监视一个对象，不用参与这个对象的生命周期管理，它不拥有所指向的对象。",
    "url": "/c++/什么是智能指针，C++有哪几种智能指针.html",
    "lang": ""
  },
  {
    "title": "什么是构造函数和析构函数？构造函数、析构函数可以是虚函数嘛",
    "content": "--- title: 什么是构造函数和析构函数？构造函数、析构函数可以是虚函数嘛 --- 构造函数 构造函数是创建对象时自动调用的成员函数，它的作用是初始化成员变量，为对象分配资源，执行必要的初始化操作。 特点 函数名必须与类名相同，且没有返回类型； 可以有多个构造函数； 如果没有为类定义一个构造函数，编译器会自动生成一个默认构造函数，它没有参数，也可能执行一些默认的初始化操作。 构造函数不能是虚函数。 虚函数对应一个虚表，需要通过虚函数指针去访问虚表，这个虚函数指针存在对象的内存空间，如果此时构造函数是虚函数，对象还没实例化没有分配内存空间，也就无法调用； 析构函数 析构函数是对象生命周期 结束时自动调用的函数，它的作用是释放对象占用的资源，执行一些必要的清理操作。 析构函数特点： 函数名为 类名 ； 没有参数； 如果没有为类定义一个析构函数，编译器会自动生成一个默认析构函数，执行简单的清理操作。 析构函数可以是虚函数。 虚析构函数可以在运行时实现多态性； 如果基类的析构函数不是虚函数，当通过基类指针去删除派生类对象时，不会调用派生类的析构函数。可能会导致派生类的资源未被正确释放，从而造成资源泄漏",
    "url": "/c++/什么是构造函数和析构函数？构造函数、析构函数可以是虚函数嘛.html",
    "lang": ""
  },
  {
    "title": "什么是菱形继承",
    "content": "--- title: 什么是菱形继承 --- 多继承体系中，当两个派生类继承同一个基类，然后有一个最派生类同时继承这两个派生类时，就形成了菱形继承的结构。这种结构会导致基类的成员在最派生类中出现两次，因为两个派生类各自继承了基类的成员，而最派生类又继承了这两个派生类。菱形继承如果没有适当处理，会导致二义性问题，比如基类的构造函数和析构函数调用顺序问题。为了解决这个问题，可以使用虚继承。虚继承确保了基类只被继承一次，无论在继承链中出现多少次。",
    "url": "/c++/什么是菱形继承.html",
    "lang": ""
  },
  {
    "title": "什么是野指针？如何避免？",
    "content": "--- title: 什么是野指针？如何避免？ --- 什么是野指针 野指针是指“指向已经被释放的或无效的内存地址的指针”。在 C 和 C++ 这类允许直接操作内存地址的语言中，如果指针没有被正确初始化，或者指针所指向的内存已经被释放，那么这个指针就成为了野指针。使用野指针可能会导致程序崩溃、数据损坏或者其他一些不可预测的行为。 在什么情况下会产生野指针？ 在释放后没有置空指针： 使用 delete 或 free 释放了内存后，没有将指针设置为 nullptr，指针仍然指向已释放的内存地址。 返回局部变量的指针 : 如果函数返回了指向其局部变量的指针，一旦函数返回，这些局部变量的生命周期结束，返回的指针成为野指针。 越界访问：指针访问的内存超出了其合法的内存块边界。 函数参数指针被释放。 如何避免野指针 在释放内存后将指针置为 nullptr 。 避免返回局部变量的指针。 使用智能指针（如 std::uniqueptr 和 std::sharedptr ）。 注意函数参数的生命周期，避免在函数内释放调用方传递的指针，或者通过引用传递指针。",
    "url": "/c++/什么是野指针？如何避免？.html",
    "lang": ""
  },
  {
    "title": "内存碎片",
    "content": "--- title: 内存碎片 --- 内存碎片是指在计算机内存管理过程中，由于内存的分配和释放不连续，导致可用内存被分割成许多不连续的小块。这种现象会降低内存的利用效率，并可能导致内存分配失败，即使总的可用内存足够。 外部碎片: 发生在堆内存中（即动态内存分配）。 由于内存块的分配和释放顺序不一致，导致可用空间被分割成许多不连续的小块。 典型的例子是当你释放内存块时，它们之间可能因为保留的内存块而无法合并成一个更大的连续块。 内部碎片: 发生在固定大小的内存块中。 当分配的内存比实际需要的多时，会在每个块的末尾留下未使用的部分。 这通常发生在为了加快分配速度而采取的策略中，比如页分配或块分配策略。 不规则的内存分配和释放: 程序在运行过程中频繁地分配和释放不同大小的内存块，导致内存无法被完整地回收和重用。 不良的内存管理策略: 某些内存管理算法可能导致更多的碎片，例如首次适应（first-fit）和最佳适应（best-fit）策略。 程序生命周期的变化: 长时间运行的程序由于不断的内存分配和释放，容易导致碎片化。 垃圾回收（Garbage Collection）: 自动管理内存，定期整理内存以减少碎片。 常见于语言如 Java 和 Python。 内存池（Memory Pool）: 预分配一个大的内存块，并在其中自行管理内存分配和释放。 适用于频繁分配和释放小内存块的场景。 使用合适的分配策略: 选择合适的分配策略（如次适应（next-fit）、伙伴系统（buddy system））以减少碎片。 改进的内存分配器如 jemalloc 和 tcmalloc 提供了更好的碎片管理。 定期紧缩（Compaction）: 将活跃的内存块移动到一侧，从而合并不连续的空闲块。 这种方法可能需要暂停程序执行（Stop-the-world），因此必须在可控的延迟下实施。 优化程序结构: 优化程序的内存使用模式以降低碎片化。 避免频繁的分配和释放，或者在某个确定的生命周期内集中执行。 通过使用合适的内存管理策略和技术，可以有效地减少内存碎片，提高程序的内存利用效率和稳定性。",
    "url": "/c++/内存碎片.html",
    "lang": ""
  },
  {
    "title": "分配内存时栈和堆在操作系统的底层原理",
    "content": "--- title: 分配内存时栈和堆在操作系统的底层原理 --- 栈是用于管理函数调用，局部变量等的高效的内存区域，由操作系统自动管理 而堆是用于动态分配的内存区域，由程序员手动管理（或者垃圾回收） 栈在操作系统底层实现： 线程创建的时候，操作系统为其创建栈空间 栈的使用：只需要移动栈指针 硬件支持：寄存器 x86 rsp rbp 操作系统管理： 函数调用时，先将参数压栈 执行call指令，将返回地址（返回上一级函数的下一条指令）压栈 创建栈帧，保存旧的rbp，设置新的rbp 为局部变量分配空间 恢复之前的rbp和rsp，ret指令弹出返回地址 为什么栈的分配速度比堆快？ 栈只需要移动指针，堆需要查找空闲内存块 多线程程序中，栈和堆如何隔离？栈是线程私有，堆是进程共享，需要同步机制",
    "url": "/c++/分配内存时栈和堆在操作系统的底层原理.html",
    "lang": ""
  },
  {
    "title": "可重入函数",
    "content": "--- title: 可重入函数 --- 在信号、多线程这些情况下，一个函数在执行过程中，有可能会异步地再重新调用同一个函数。如果重复的函数调用有可能会导致错乱的结果，那么这些函数就是不可重入的。 一个比较典型的不可重入函数例子就是 malloc 函数， malloc 函数必然是要修改静态数据的，为了保证线程安全性， malloc 函数的实现当中会存在加锁和解锁的过程，假如 malloc 执行到加锁之后，解锁之前的时候，此时有信号产生并且递送的话，线程会转向执行信号处理回调函数，假如信号处理函数当中又调用了 malloc 函数，此时就会导致死锁——这就是 malloc 的不可重入性。",
    "url": "/c++/可重入函数.html",
    "lang": ""
  },
  {
    "title": "场景面经题",
    "content": "--- title: 场景面经题 --- 精确算法： 对每个url按照hash拆分到不同的小文件中（分治思维），两个文件中相同的url的hash肯定是一致的，这样就只需要在两个更小的对应hash的文件中比较了。 模糊算法： 利用布隆过滤器的思路，A文件先通过布隆的hash函数映射到位图上，然后B文件再去遍历url通过hash函数映射，如果位图上全1，代表可能是一致的，否则不一致。 场景题：服务器把每条外部请求写入日志文件，每行包括：请求方ip+业务。查看外部请求频次最高的10个ip，请求频次是多少。用shell指令 操作日志文件（去重+计数+排序 11、说一件你做过最棘手的事，并谈谈你是怎么解决的，有什么经验分享 12、平时看c++的书有哪些 9.微信的扫码登陆如何实现? 成就感最高的项目 11.有一百万个不相等的乱序的int，用最快的方法将他们分成相等的两个部分，要求左边部分的每个数都比后边的小 14.场景题:设计一套在高并发场景下的qq号的生成服务，希望数字是单调递增的数字 设计模式，责任链模式 1.ID 用户名的表 如何快速查找 8.设计一个任务队列，如何解决并发问题不用锁怎 么解决 程序上线后性能不好，如何分析问题并且如何调优(重点是如何分析问题，使用什么工具) --------------------------- redis如何实现分布式锁的 除了用setnx还有其他方法吗？ 比如都自增1？ （INCR） 你如何用mysql实现分布式锁 用伪代码+sql语句实现，输入一个字符串返回是否修改成功还是失败 mysql的实现的分布式锁如何防止获取锁的线程突然崩溃了，在代码中反应 在代码中如何保证崩溃之后的事务ACID 如何实现一个中间件？ 你要如何设计？ 如何使用 为什么mysql使用B+树？ 跳表不行吗？ 不也可以一次加载大量磁盘数据进来吗？ 写个sql语句： 获取课程数量大于5的学生且平均成绩大于70的学生ID, 结果倒排 HTTP协议中交换了那些信息？ （SYN+ACK） 面试官说还有？ 502表示什么？ 504表示什么？ 为什么四次挥手有2MSL？ 进程fork读取的是一样的数据吗？ 写时复制呢？ 乐观锁和悲观锁？ 乐观锁如何实现？ redis的数据结构有哪些？ 如果有10个线程我想顺序执行，你用redis如何实现？用什么数据结构为什么？",
    "url": "/c++/场景面经题.html",
    "lang": ""
  },
  {
    "title": "堆区和栈区的区别",
    "content": "--- title: 堆区和栈区的区别 --- 堆 （Heap） 和栈 （Stack） 是程序运行时两种不同的内存分配区域 内存分配： 栈 是由编译器自动管理的，用于存储局部变量和函数调用的上下文信息。 栈上的对象在定义它们的块或函数调用结束后自动销毁。 栈的内存分配和释放速度很快，因为栈的内存管理是连续的，不需要搜索空闲内存。 堆 由程序员手动管理的，用于存储动态分配的对象。 堆上的对象需要程序员手动释放，否则可能导致内存泄漏。 堆的内存分配和释放速度通常比栈慢，因为可能需要搜索合适的内存块，并且涉及内存碎片整理。 大小限制： 栈的大小通常有限制，远小于堆的大小，且在不同系统和编译器中可能不同。 堆的大小通常很大，受限于系统可用内存。 使用场景： 栈主要用于存储函数参数、局部变量等。 堆用于存储生存期不受限于单个函数调用的对象，如使用 new 或 malloc 分配的对象。",
    "url": "/c++/堆区和栈区的区别.html",
    "lang": ""
  },
  {
    "title": "如何在C++中创建和管理线程？",
    "content": "--- title: 如何在C++中创建和管理线程？ --- 在C++中，可以使用标准库中的头文件来创建和管理线程，创建和管理线程步骤如下。 包含头文件 定义线程函数，作为线程的入口点。 创建线程对象，使用std::thread来创建线程。你可以将线程函数传递给std::thread的构造函数来指定线程执行的任务 启动线程：使用join()方法或detach()方法 等待线程结束（可选）：使用join()方法启动线程，主线程将会阻塞直到新线程执行完毕，如果调用detach()方法，新线程将会变成分离状态，主线程不再与其同步，并且不再需要等待线程的结束。",
    "url": "/c++/如何在C++中创建和管理线程？.html",
    "lang": ""
  },
  {
    "title": "左值引用和右值引用的区别",
    "content": "--- title: 左值引用和右值引用的区别 --- 左值是能够取地址的值，而右值是不能够取地址的值。 左值引用使用单个&符号，用来绑定具有持久存储期的变量或者对象 右值引用使用两个&&符号，绑定到右值表达式上，通常是临时对象或者即将销毁的对象。 右值引用的主要目的是通过移动语义来利用这些临时资源，避免不必要的拷贝。通常是与std::move进行搭配使用 在C++中，左值和右值是两种不同类型的表达式，它们分别对应着不同的引用方式： 左值引用： 左值引用使用&符号 左值引用绑定到左值表达式上，即那些具有持久存储期的表达式，如变量或者对象。 它们在内存中有一个持久的地址，可以被赋值和修改。 右值引用： 右值引用使用两个&符号 右值引用绑定到右值表达式上，通常是临时对象或即将销毁的对象，它们没有持久的存储期。 右值引用的主要目的是通过移动语义来利用这些临时资源，避免不必要的复制。 区别总结 绑定对象：左值引用绑定到具有持久状态的对象，而右值引用绑定到临时或即将销毁的对象。 生命周期：左值引用延长了它所引用对象的生命周期，右值引用则表示对一个临时值的引用。 可修改性：左值引用可以被用来修改其所引用的对象，而右值引用通常用于移动语义，不涉及修改。 标准库支持：C++11 标准库中的某些函数和算法（如 std::move）特别设计来与右值引用配合使用。",
    "url": "/c++/左值引用和右值引用的区别.html",
    "lang": ""
  },
  {
    "title": "常量指针和指针常量之间有什么区别",
    "content": "--- title: 常量指针和指针常量之间有什么区别 --- 常量指针\"和 \"指针常量\"是两种不同的概念，它们的区别主要在于它们所指向的数据是否可以被修改，以及它们自己的值是否可以改变。 常量指针是指指针所指向的数据是常量，不能通过这个指针来修改它指向的数据。但是，指针本身的值（即它所指向的地址）是可以改变的。 指针常量是指指针本身的值是常量，一旦被初始化后就不能指向其他地址。但是，它所指向的数据是可以修改的（除非那个数据本身是常量） 也可以同时使用 const 关键字来定义一个指针，既是常量指针又是指针常量，即它所指向的数据不能被修改，同时指针本身的值也不能改变。 总结 常量指针：可以改变指针本身的值，但不能修改它所指向的数据。 指针常量：指针本身的值不能改变，但可以修改它所指向的数据（除非数据本身是常量）。",
    "url": "/c++/常量指针和指针常量之间有什么区别.html",
    "lang": ""
  },
  {
    "title": "异步服务器发送数据使用队列",
    "content": "--- title: 异步使用队列 --- 直接发送的陷阱： 若多个线程或回调同时调用异步发送（如 asyncwrite），会导致多个未完成的异步写操作同时操作同一个 Socket。由于 TCP 是流式协议，操作系统内核不保证多个并发写操作的顺序性，可能引发数据交叉混乱（如 ABC 和 123 可能被接收为 A1B2C3）。 示例场景： 用户 A 的线程发送数据 \"Hello\"，用户 B 的线程同时发送 \"World\"，最终 Socket 实际发送的数据可能是 HWeolrllod。 队列序列化： 将待发送数据按顺序存入队列，确保同一时间只有一个异步写操作在执行。当前写操作完成后，从队列中取出下一个数据继续发送。 ------ 异步无序性： 异步操作完成的顺序不确定。若直接连续调用 asyncwrite，实际发送顺序可能与调用顺序不一致。 示例场景： 依次调用 asyncwrite(\"A\") 和 asyncwrite(\"B\")，但由于网络延迟或调度，B 可能先于 A 发送完成。 队列 FIFO 保证： 发送队列按先进先出（FIFO）顺序处理数据，确保消息严格按照调用顺序发送。 数据生命周期风险： asyncwrite 的缓冲区（如 boost::asio::buffer）必须保证在异步操作完成前有效。若直接传递栈内存或临时对象，可能因作用域结束导致缓冲区被释放，引发内存错误。 错误示例： c++ void SendData() { char buffer[] = \"Hello\"; // 栈内存 asyncwrite(socket, boost::asio::buffer(buffer), [](...){}); // 函数返回后 buffer 被销毁，但异步操作可能仍在进行！ } 队列持有数据所有权： 将待发送数据封装为堆对象（如 std::sharedptr）并存入队列，由队列管理其生命周期，确保异步操作期间数据有效。 ------ 发送速率不匹配： 若数据生产速率远高于网络发送速率，持续触发 asyncwrite 会导致待发送数据在内存中无限堆积，最终内存耗尽。 队列作为缓冲区： 队列大小可设置上限，当队列满时拒绝新数据（或采取其他流控策略），避免资源耗尽。 ------ c++ class Session { std::queue<std::sharedptr<Msg>> sendqueue; // 发送队列 std::mutex queuemutex; // 互斥锁保护队列 bool iswriting = false; // 标记是否正在发送 void Send(std::sharedptr<Msg> msg) { std::lockguard<std::mutex> lock(queuemutex); sendqueue.push(msg); if (!iswriting) { StartAsyncWrite(); // 触发异步发送 } } void StartAsyncWrite() { iswriting = true; auto& msg = sendqueue.front(); asyncwrite(socket, boost::asio::buffer(msg->data), this, self=sharedfromthis() { std::lockguard<std::mutex> lock(queuemutex); sendqueue.pop(); // 发送完成，移除队列头部 iswriting = false; if (!sendqueue.empty()) { StartAsyncWrite(); // 继续发送下一个 } }); } }; 数据入队： 所有发送请求先将数据存入队列，而非直接调用 asyncwrite。 串行发送： 检查当前是否正在发送（iswriting 标记），若空闲则触发异步写操作。 回调处理： 在异步写完成回调中： 移除已发送的队列头部数据。 若队列非空，继续发送下一个数据。 ------ | 场景 | 直接发送 | 队列发送 | | :--------------- | :------------- | :----------------------------- | | 并发写入 | 数据交叉混乱 | 顺序发送，无并发 | | 数据生命周期 | 栈数据可能失效 | 队列持有数据所有权，保证有效性 | | 流量控制 | 内存易失控 | 队列大小可控，支持背压 | | 顺序性 | 无法保证 | 严格 FIFO 顺序 | ------ 通过发送队列管理异步写入操作，本质是将并发操作转化为串行化任务链，从而： 避免数据竞争：同一时间只有一个异步写操作。 保证顺序性：严格按调用顺序发送。 管理资源：安全控制数据生命周期。 支持流量控制：防止内存溢出。 这是异步服务器设计中处理发送逻辑的标准模式，兼顾了性能、安全性和可维护性。",
    "url": "/c++/异步服务器发送数据使用队列.html",
    "lang": ""
  },
  {
    "title": "指针和引用的区别",
    "content": "--- title: 指针和引用的区别 --- 指针是一个存储另一个变量地址的变量，而引用是变量的别名。指针可以指向空，而引用在定义时必须被初始化，并且引用一旦被初始化，不能改变绑定，因为引用底层实现是一个指针常量，而指针可以改变指向，指针需要解引用才可以访问变量。 从概念上来说： 指针是一个存储另一个【变量地址】的变量，它指向内存中的一个位置。 引⽤就是变量的别名，从⼀⽽终，不可变，必须初始化 空状态： 指针可以被初始化为NULL或nullptr，表示它不指向任何地址。 引用在定义时必须被初始化，不能引用NULL或不存在的内存地址。 可变性： 指针： 可以改变指针的指向，使其指向不同的内存地址。 引⽤： ⼀旦引⽤被初始化，它将⼀直引⽤同⼀个对象，不能改变绑定。 操作 指针： 可以通过解引⽤操作符 来访问指针指向的变量的值，还可以通过地址运算符 & 获取变量的地址。 引用： 引⽤在声明时被初始化，并在整个⽣命周期中⼀直引⽤同⼀个变量。不需要使⽤解引⽤操作符，因为引⽤本身就是变量的别名。 用途： 指针： 通常⽤于动态内存分配、数组操作以及函数参数传递。 引⽤： 通常⽤于函数参数传递、操作符重载以及创建别名。",
    "url": "/c++/指针和引用的区别.html",
    "lang": ""
  },
  {
    "title": "智能指针的实现原理是什么",
    "content": "--- title: 智能指针的实现原理是什么 --- 智能指针是利用RAII机制来保证资源的自动释放，从而避免内存泄漏。uniqueptr和sharedptr都是通过析构函数来管理资源的释放。当uniqueptr超出作用域时，会自动调用删除操作符来释放其指向的内存。 std::uniqueptr 通过删除复制构造函数和复制赋值运算符来确保所有权的唯一性，但提供移动构造函数和移动赋值运算符，允许所有权的转移。 而sharedptr维护一个指向原始对象的指针和一个指向控制块的指针，这个控制块通常包含强弱引用计数和删除器，分配器。每当创建一个新的sharedptr或将一个sharedptr赋值给另一个时，引用计数增加。当sharedptr被销毁或通过标准库提供的reset成员函数重置时，引用计数减少。当引用计数降到零时，控制块会释放资源并自我销毁。std::weakptr 是一种不拥有对象的智能指针，它观察 std::sharedptr 管理的对象，但不增加引用计数。它用于解决 std::sharedptr 之间可能产生的循环引用问题，因为循环引用会导致引用计数永远不会达到零，从而造成内存泄漏。 对于引用计数这一变量的存储，是在堆上的，多个sharedptr的对象都指向同一个堆地址。 为什么尽量使用 makeshared ，makeshared只分配一次内存，通过指针构造sharedptr有两次分配。当你使用 std::makeshared 时，这两个步骤被合并为一个原子操作，进行一次内存分配，分配包括对象和控制块的连续内存。如果使用先new，再传递给sharedptr的构造函数时，本来new给对象分配了一个空间，然后构造sharedptr的控制块又分配了一次空间。 智能指针的应用： 自动管理内存分配，延续对象的生命周期，监测对象的存在 作为函数的返回值，函数内部分配了堆的资源，想在类外面使用，并能够在类外面得到正确的释放。 std::uniqueptr uniqueptr代表独占所有权的智能指针，同一时间只能有一个uniqueptr实例指向特定资源。 它通过析构函数来管理资源的释放。当uniqueptr超出作用域时，会自动调用删除操作符来释放其指向的内存。 std::uniqueptr 通过删除复制构造函数和复制赋值运算符来确保所有权的唯一性，但提供移动构造函数和移动赋值运算符，允许所有权的转移。 std::sharedptr sharedptr允许多个指针实例共享对同一资源的所有权，使用引用计数机制来跟踪有多少个sharedptr指向同一资源。 内部维护一个控制块，通常包含引用计数和资源的原始指针。每当创建一个新的sharedptr或将一个sharedptr赋值给另一个时，引用计数增加。 当sharedptr被销毁或通过标准库提供的reset成员函数重置时，引用计数减少。当引用计数降到零时，控制块会释放资源并自我销毁。 std::weakptr std::weakptr 是一种不拥有对象的智能指针，它观察 std::sharedptr 管理的对象，但不增加引用计数。 它用于解决 std::sharedptr 之间可能产生的循环引用问题，因为循环引用会导致引用计数永远不会达到零，从而造成内存泄漏。 智能指针是利用RAII机制来保证资源的自动释放，从而避免内存泄漏。uniqueptr和sharedptr都是通过析构函数来管理资源的释放。当uniqueptr超出作用域时，会自动调用删除操作符来释放其指向的内存。std::uniqueptr 通过删除复制构造函数和复制赋值运算符来确保所有权的唯一性，但提供移动构造函数和移动赋值运算符，允许所有权的转移。而sharedptr维护一个控制块，通常包含引用计数和资源的原始指针。每当创建一个新的sharedptr或将一个sharedptr赋值给另一个时，引用计数增加。当sharedptr被销毁或通过标准库提供的reset成员函数重置时，引用计数减少。当引用计数降到零时，控制块会释放资源并自我销毁。std::weakptr 是一种不拥有对象的智能指针，它观察 std::sharedptr 管理的对象，但不增加引用计数。它用于解决 std::sharedptr 之间可能产生的循环引用问题，因为循环引用会导致引用计数永远不会达到零，从而造成内存泄漏。",
    "url": "/c++/智能指针的实现原理是什么.html",
    "lang": ""
  },
  {
    "title": "深拷贝与浅拷贝",
    "content": "--- title: 深拷贝与浅拷贝 --- 浅拷贝 定义：浅拷贝仅复制对象本身，不复制对象所指向的动态分配的内存。换句话说，它只复制内存中的对象副本，而不复制对象内部指向的任何动态分配的资源。 实现：通常通过复制构造函数或赋值运算符实现。 特点： 速度快，因为只涉及基本数据类型的复制。 如果原始对象包含指针，浅拷贝会导致两个对象尝试管理相同的动态内存，这可能导致多重释放和悬空指针问题。 深拷贝 定义：深拷贝不仅复制对象本身，还递归地复制对象所指向的所有动态分配的内存。这意味着每个对象都有自己的独立资源副本。 实现：通常需要自定义复制构造函数或赋值运算符来确保所有动态分配的资源都被正确复制。 特点： 速度慢，因为需要递归地复制所有资源。 可以安全地使用复制出的对象，而不担心资源管理问题。",
    "url": "/c++/深拷贝与浅拷贝.html",
    "lang": ""
  },
  {
    "title": "移动语义有什么作用，原理是什么",
    "content": "--- title: 移动语义有什么作用，原理是什么 --- 移动语义是 C++11 引入的一项特性，对于大型对象或包含资源的对象（如动态分配的内存、文件句柄等），拷贝构造函数可能会非常昂贵。移动语义允许对象的资源被“移动”到新对象，而不是拷贝，从而节省资源和时间。其主要作用是优化资源的利用，特别是在对象的复制操作中。 移动语义通过移动构造函数和移动赋值运算符实现。在移动构造或移动赋值过程中，源对象的资源被“拿走”，并转移到目标对象，源对象变为无效状态。",
    "url": "/c++/移动语义有什么作用，原理是什么.html",
    "lang": ""
  },
  {
    "title": "简述一下 C++的重载和重写，以及它们的区别和实现方式",
    "content": "--- title: 简述一下 C++的重载和重写，以及它们的区别和实现方式 --- 重载和重写是两种不同的概念，它们都用于实现多态性，但方式和使用场景有所不同。 重载：在同一个类或命名空间中，声明多个同名函数， 但是参数列表不同。编译器根据参数的类型、数量或顺序来区分不同的函数。 重写：重写发生在继承体系中，在子类中，声明一个与父类中虚函数具有相同名称、相同参数列表和相同返回类型的函数，并在子类函数前加上 override 关键字。 区别： 作用域：重载发生在同一个作用域内，而重写发生在继承体系中。 参数列表：对于重载的函数，参数列表必须不同；对于重写的函数，参数列表必须与被重写的函数完全相同。 返回类型：重载函数的返回类型可以不同，但重写函数的返回类型必须与被重写的函数相同（或与之兼容，C++中称为协变返回类型）。 虚函数：重写通常与虚函数一起使用，以实现运行时多态性；而重载是编译时多态性，由编译器在编译期间确定调用哪个函数。 关键字：重写函数可以使用 override 关键字，明确指出该函数是对父类虚函数的重写。 1.重载 重载是在同一个作用域中允许有多个同名的函数，而这些函数的参数列表不同， 允许参数个数不同，参数类型不同，或者两者都不同。编译器会根据这些函数 的不同列表将同名的函数的名称做修饰，从而生成一些不同名称的预处理函数， 来实现同名函数调用时的重载问题。重载没有体现多态性。 2.隐藏 隐藏是指派生类的函数屏蔽了与其同名的基类函数。隐藏规则如下： 如果派生类的函数与基类的函数同名，但是参数不同。此时，不论有无virtual 关键字，基类的函数将被隐藏（注意别与重载混淆，重载是在同一个类中发生） 。如果派生类的函数与基类的函数同名，并且参数也相同，但是基类函数没有 virtual 关键字。此时，基类的函数被隐藏（注意别与覆盖混淆，覆盖有virtual 关键字）。 3.覆盖 覆盖又可以叫做重写，只有重写了虚函数的才能算作是体现了C++多态性。 基类存在某个虚函数，在派生类中对该虚函数保持返回类型不变，参数类型个数 不变，函数名字也不变，然后对函数体进行修改，这就叫覆盖。被覆盖的函数必须 是虚函数。 4.区别 重载最好区分，重载是在一个作用域(类)中 隐藏和覆盖都是在派生类和基类之间产生，被覆盖的函数一定是虚函数，但是 被隐藏的函数不一定是虚函数， 1、基类的函数无论是不是虚函数，在派生类中的函数与基类函数同名，但是参 数不同，基类函数将被隐藏。 2、基类函数此时不是虚函数，在派生类中的函数与基函数同名并且参数相同， 此时基类函数将被隐藏。",
    "url": "/c++/简述一下 C++的重载和重写，以及它们的区别和实现方式.html",
    "lang": ""
  },
  {
    "title": "结构体和类之间有什么区别",
    "content": "--- title: 结构体和类之间有什么区别 --- 在C中，struct 只能包含成员变量，不能包含成员函数。而在 C++ 中，struct 类似于 class，既可以包含成员变量，又可以包含成员函数。 不同点： class 中的成员默认都是 private 的，而 struct 中的成员默认都是 public 的； class 继承默认是 private，struct 继承默认是 public； class 可以用于定义模板函数，而 struct 不行。 实际使用中，struct 我们通常用来定义一些 POD(plain old data) 类型，它是用来描述一种数据类型的特性，主要用于在内存中表示简单的数据结构。",
    "url": "/c++/结构体和类之间有什么区别.html",
    "lang": ""
  },
  {
    "title": "编程语言",
    "content": "--- title: 编程语言 --- 答： 面向对象主要是可以将复杂的系统拆解为各个小的独立的对象，每个对象负责特定的功能。这样做降低复杂性，使得系统更容易维护。 面向对象提供了抽象封装继承多态等机制，可以实现代码的复用，以及对扩展开放对修改关闭的原则保证了系统的稳定以及易维护。 答： 首先说明左值和右值的区别，左值是能够取地址的值，而右值是不能够取地址的值。在c++中可以用取地址符号来判断。而左值引用是单个取地址符号,用来绑定一个左值的，来作为变量的别名，可以直接在引用上操作修改变量，以及避免在参数传递中发生拷贝。const左值引用是一个万能引用，也可以绑定右值。 而右值引用只能用来绑定右值，是两个取地址符号来表示的，右值引用最重要的作用是实现移动语义，通过移动而不是拷贝来避免不必要的资源消耗。而在模版编程中，T && t表示万能引用，可以绑定左值也可以绑定右值，可以搭配std::forward来实现完美转发。 将亡值是指C++11新增的和右值引用相关的表达式，通常指将要被移动的对象、T&&函数的返回值、std::move函数的返回值、转换为T&&类型转换函数的返回值，将亡值可以理解为即将要销毁的值，通过“盗取”其它变量内存空间方式获取的值，在确保其它变量不再被使用或者即将被销毁时，可以避免内存空间的释放和分配，延长变量值的生命周期，常用来完成移动构造或者移动赋值的特殊任务。 答： 空指针解引用，数组越界，除以0，多次释放同一个内存地址，访问已经释放过的内存, dynamiccast失败的时候 答： vector，deque，string等容器添加元素或者删除元素，会导致迭代器失效，因为添加元素的时候，可能会发生扩容现象，这个时候需要重新分配内存，所以迭代器指向的空间还是原来的老空间上。 可以使用一些比较稳定的容器比如list,map,set，当删除元素的时候，其他迭代器并不会失效。第二点是当插入或删除元素后，保持重置迭代器的习惯，比如利用erase返回的迭代器进行迭代。 答： 可重入函数指的是在任何时刻都可以被中断，然后在中断后可以安全地再次被调用。这种函数在被多个线程同时调用时不会引起竞争条件或数据不一致等问题。可重入函数不依赖于不受保护的全局变量或静态数据 不可重入函数就是指执行过程中被中断后再次进入或调用时，会导致未定义行为或者数据不一致问题。比如malloc函数就是不可重入函数，malloc的内部实现维护了全局的内存分布表，在标准C实现中，malloc内部并没有同步机制来管理对这个全局对象的并发访问。 值得注意的是，可重入和线程安全（Thread-safe）是相关但不同的概念： 可重入函数：可以安全地被中断并在中断后重新进入。 线程安全函数：可以安全地被多个线程同时调用。 一个函数可以是可重入的但不是线程安全的，也可以是线程安全的但不是可重入的。线程安全函数通常通过锁（mutex）等同步机制来实现，而可重入函数则通常避免使用任何需要同步的共享资源。 答： C++的内存分区分为 内核空间，栈，映射段，堆，全局静态存储区（数据段,BSS段），代码段。 栈（Stack）: 用于存储局部变量和函数调用的上下文。栈的内存分配是自动的，由编译器管理。 堆（Heap）: 用于动态内存分配。程序员可以使用 new、malloc 等操作符或函数从堆上分配内存，并使用 delete、free 释放内存。 映射段：内存映射区包含文件映射和匿名内存映射，比如mmap系统调用进行映射就是映射这块内存。 全局/静态存储区（Global/Static Storage）: 存储全局变量和静态变量，包括： 数据段：存储初始化的全局变量和静态变量。 BSS 段 ：存储未初始化的全局变量和静态变量。 常量存储区（Constant Data）: 存储程序中的常量数据，如字符串字面量。此外，如果有定义了虚函数，常量区还会存储虚表。 代码段（Code Segment 或 Text Segment）: 存储程序的可执行代码和函数的二进制指令。 答： 利用函数指针和结构体，定义一个通用的结构体，然后这个结构体里有一个函数指针的数据成员，创造对象的时候将这个指针指向需要的函数即可。 答： 传一个对象指针，调用这个对象的成员方法，或者使用一个静态成员指针，再利用这个指针调用对象的成员方法。 答： 内存泄漏是指程序中己动态分配的堆内存未释放或无法释放，造成系统内存的浪费，导致程序运行速度减慢甚至系统崩溃等严重后果。 内存溢出：通俗理解就是内存不够，指程序要求的内存超出了系统所能分配的范围，通常在运行大型软件或游戏时，软件或游戏所需要的内存远远超出了你主机内安装的内存所承受大小，就叫内存溢出。 内存泄露可能会导致内存溢出。 防止内存泄漏的方法：保持良好的习惯来进行delete或者free，采用RAII的思想和智能指针来管理对象，利用valgrind工具来分析内存问题。 new是C++的操作符，由编译器实现，可以直接用来分配对象或数组，并且在分配失败会抛出std::badalloc异常。 new表达式工作步骤有三步 ​ 1.调用operator new库函数，分配未类型化的空间，用来保存指定类型的一个对象，而在operator new中，会调用malloc函数 ​ 2.运行该类型的构造函数初始化对象 ​ 3.返回指向对象的指针 new是类型安全的，它会根据分配的对象类型进行正确的内存分配和构造函数调用。 使用 new 分配的对象在对象生命周期结束时需要使用 delete 来释放，delete 会自动调用对象的析构函数。 总结来说，new和malloc都是动态内存分配的手段，但new提供了类型安全和构造/析构的自动化， placementnew 允许在已经分配的内存地址上构造对象 delete表达式工作步骤有两步 ​ 1.调用析构函数，回收对象中数据成员申请资源 ​ 2.调用operator delete的标准库函数释放该对象所用的内存 malloc是一个库函数，通常需要包含头文件<cstdlib>，并且只分配原始内存，分配失败时会返回NULL。 malloc 不是类型安全的，它只分配原始内存，不调用构造函数。返回类型是 void，需要强制类型转换为具体的指针类型。 使用 malloc 分配的内存需要使用 free 来释放，free 不会自动调用析构函数，因此如果分配的是对象数组，需要手动调用析构函数。 而malloc则提供了更底层的内存分配方式，需要手动管理构造和析构。 malloc的底层实现是怎样的？free是怎么回收内存的？ 答： 1、当开辟的空间A小于128K的时候，malloc底层调用brk()函数，brk是将数据段的最高地址指针edata往高地址推，edata+30K只是完成虚拟地址的分配，A这块内存现在还是没有物理页与之对应的，等到进程第一次读写A这块内存的时候，发送缺页中断， 这个时候，内核才会分配给A这块内存对应的物理页。也就是说，如果用malloc分配了A这块内容，然后从来不访问它，那么A对应 的物理页是不会被分配的。而当开辟的空间大于128K的时候，利用mmap系统调用，从堆和栈的中间也就是文件映射区分配一块虚拟内存。这样做主要是因为： brk分配的内存需要等到高地址内存释放以后才能释放（比如先申请A再申请B，所以B相对于A来说是高地址空间，所以在B释放 之前，A是不可能释放的，因为只有一个edata指针，这就是内存碎片产生的原因。而mmap分配的内存可以单独释放。 2、free回收内存时首先要知道这块内存的地址多大，所以在malloc返回的地址的前一小段的内存块控制块，存储这一块有多大的信息。对使用mmap机制分配空间的malloc返回的地址free时，直接将其对应的虚拟内存和物理内存一起释放。而对brk机制分配空间的malloc返回的地址free时，并不会立刻释放，默认情况下，当最高地址空间的空闲内存超过128K时，执行内存紧缩操作(trim) 答： 1、malloc开辟空间类型大小需手动计算，new是由编译器自己计算。 2、malloc返回类型为void,必须强转成对应类型指针，而new直接返回对应类型指针。 3、malloc开辟内存时返回内存地址要检查判空，因为若它可能跟开辟失败会返回NULL；new则不用判断，因为内存分配失败时，它会抛出异常bacalloc，可以使用异常机制。 4、无论释放几个空间大小，free只传递指针，而多个对象时，delete需加[]，(分配的大小是数组空间大小). 5、malloc/free函数只是开辟空间并释放，new/delete则不仅会开辟空间，并调用构造函数和析构函数进行初始化和清理。 6、new/delete底层是基于malloc/free来实现的。 7、因为new/delete是操作符，它调用operator new/operator delete，它们可以被重载，在标准库里它有8个重载版本；而malloc/free不可以重载。 8、对于malloc分配内存后，若在使用过程中内存分配不够或太多，这时可以使用realloc函数对其进行扩充或者缩小，但是new分配好的内存不能这样被直观简单的改变。 9、对于malloc申请内存的位置是在堆上分配内存的；但是new申请内存有一个抽象概念，它为自由存储区，它可以在堆上，也可以在静态存储区上分配，这主要取决于operator new实现细节，取决于它在哪里为对象分配空间。 答： 当一个对象被创建后，它的每一个成员函数都含有一个系统自动生成的隐含指针this，用以保存这个对象的地址 保证这个对象是普通new分配的，并且要保证delete this后，不会有其他调用this的地方，用来进行对象的自杀。并且delete this不能再析构函数中使用，否则会无限递归。比如智能指针中当引用计数变为0的时候，手动进行delete this. 答： pushback左值会执行拷贝构造函数，而右值会执行移动拷贝,因为pushback的实现 c++ template<typename Ty> // 函数模板的类型推演 + 引用折叠 void pushback(Ty &&val) //Ty CMyString& + && = CMyString& { if (full()) expand(); // move(左值)：移动语义，得到右值类型 (int&&)a // forward:类型完美转发，能够识别左值和右值类型 allocator.construct(last, std::forward<Ty>(val)); last++; } 答：move底层就是用一个staticcast将对象转化成右值 c++ template <typename T> typename removereference<T>::type&& move(T&& t) { return staticcast<typename removereference<T>::type&&>(t); 答： 完美转发是为了在模版编程中解决传递参数时的临时对象（右值）被强制转换为左值的问题。C++11中是用std::forward来实现的，它的作用是根据传入的参数，==决定将参数以左值引用还是右值引用的方式进行转发。== forward的内部实现只有一行代码，即staticcast<T&&>(t), 然后还有一个概念和这个实现有关，也就是模版推导中的引用折叠，引用折叠的规则如下：如果两个引用中至少其中一个引用是左值引用，那么折叠结果就是左值引用；否则折叠结果就是右值引用。所以说T如果是右值，那么转换之后还是右值，如果是左值，转换之后是左值。这样就实现了完美转发 atomic 加内存顺序模型实现,具体看atomic文档 答： 原子性：原子性指的是一个操作是不可分割、不可中断的，要么全部执行并且执行的过程不会被任何因素打断，要么就全不执行。 可见性：可见性指的是一个线程修改了某一个共享变量的值时，其它线程能够立即知道这个修改。 有序性：有序性指的是对于一个线程的执行代码，从前往后依次执行，单线程下可以认为程序是有序的，但是并发时有可能会发生指令重排。 单核CPU下不存在可见性问题，在多核情况下，CPU有时候会直接读cache而不去读内存，而这个时候这个变量的值可能已经被修改了，但是并没有同步到这个cache中，而各个CPU之间的高速缓存是不可见的。 答： 什么是CPU缓存一致性？ CPU缓存是处理器内部用于快速访问数据的临时存储区域。由于CPU速度远高于主内存，缓存能够显著提高数据访问速度。但当多个处理器核心或线程同时访问同一数据时，就可能出现数据不一致的问题。这就是CPU缓存一致性的重要性所在。 想象一下，如果两个处理器核心分别读取了同一内存地址的数据到各自的缓存中，并且其中一个核心修改了该数据，但没有及时通知其他核心，那么其他核心仍然使用着旧的数据，导致数据不一致。缓存一致性协议就是为了解决这个问题而设计的。 其中最著名的是MESI协议（Modified, Exclusive, Shared, Invalid）。 3.1 MESI协议状态 Modified (M): 数据已被修改，与主内存不同。 Exclusive (E): 数据仅存在于当前缓存中，与主内存相同。 Shared (S): 数据存在于多个缓存中，与主内存相同。 Invalid (I): 缓存中的数据无效，需要从主内存或其他缓存中获取。 答： 单例模式需要设置 一个static的变量，通过一个static的getinstance方法返回，并且设置构造函数为私有，拷贝构造函数和赋值函数删除 自动释放的话可以设置atexit绑定destroy方法，用饿汉模式保证线程安全。 也可以用callonce保证只初始化一次 当使用懒汉模式的时候，就需要用双检锁，防止两个线程同时进入，造成new两次。 但是dcl可能会失效，在多处理器多线程状态下，会出现CPU指令重排 new操作的步骤一般为：1.调用malloc开辟未类型化的空间，用来存储对象。2.调用构造函数创建对象。3.返回这个空间的指针 而指令重排可能导致第二个步骤和第三个步骤顺序颠倒。导致有一个线程拿到的指针指向的空间是一个未初始化的空间。 需要利用到原子变量和内存屏障解决。 答： 当定义了虚函数的时候，都要将基类的析构函数设置为虚函数，因为如果不设置为虚函数，在利用基类的指针进行delete时，只会delete掉基类对象，而派生类对象的成员和派生类对象并不会被析构。 当基类的析构函数设置为虚函数时，编译器做了优化，将析构函数看成destructor,只要派生类写了析构函数，就相当于对虚函数进行了重写(进而满足动态多态的五个条件)，体现多态性。之所以编译器可以这么做，是因为析构函数特点：不能被重载，具有唯一性 答： 构造函数和析构函数是特殊的成员函数，在其中访问虚函数时，C++采用静态联编，即在构造函数或析构函数内，即使是使用 “this->虚函数名” 的形式来调用，编译器仍将其解释为静态联编的 “本类名::虚函数名” 。即它们所调用的虚函数是自己类中定义的函数，如果在自己的类中没有实现该函数，则调用的是基类中的虚函数。但绝不会调用任何在派生类中重定义的虚函数。 答： 1）同一个sharedptr被多个线程“读”是安全的； 2）同一个sharedptr被多个线程“写”是不安全的； 3）共享引用计数的不同的sharedptr被多个线程”写“ 是安全的； 4）但同时访问指针管理的对象是不安全的 并且引用计数是线程安全的，而智能指针托管的对象不是线程安全的，在多线程情况需要加锁再操作。 sharedptr发生拷贝的流程: 1）拷贝智能指针指向的资源（非原子操作） 2）增减引用计数（原子操作） 在多线程情况下，可能会有步骤一步骤二分开执行，导致智能指针指向的是一个已经被销毁的对象。 sharedptr 的引用计数机制通过原子操作保证了线程安全，但对象的访问和 sharedptr 实例的直接修改仍需用户同步。理解其线程安全边界是避免并发问题的关键。 c++11总结15——sharedptr在多线程下的安全性问题sharedptr线程安全-CSDN博客 stl容器并不是线程安全的，所以有些时候需要用到加锁，也可以使用原子变量以及cas的机制 C++ STL容器如何解决线程安全的问题？-腾讯云开发者社区-腾讯云 (tencent.com) 正常来说pushback没有发生扩容的情况下是O(1)，发生扩容的时候是O(n)，但是均摊下来是O(1) clear需要遍历vector中的所有元素，并调用析构函数进行销毁，因此时间复杂度是O(n) 答： 原意是指一个操作如果连续执行多次所产生的结果与仅执行一次的效果相同，那么我们就称这个操作是幂等的。 腾讯二面：如何保证接口幂等性？高并发下的接口幂等性如何实现？ - 码农Academy - 博客园 (cnblogs.com) 答：利用 FILE fopen(const char pathname ,const char mode);也可以用open int open(const char path,int flag,......);//第三个参数可以是权限,返回值是文件描述符，-1为失败 int open(const char path,int flag,modet mode);//文件名 打开方式 权限 答： vector底层是通过三个指针和一段连续的空间实现的。start指针指向了空间的开始位置，finish指针指向了最后元素的后一个位置而end of storage是空间的后一个位置。vector实现了pushback,popback，以及insert等操作。 list底层是用一个双向链表来实现的，可实现任何一个位置插入节点，任何一个位置删除节点。 答： 默认构造函数、析构函数、复制构造函数和赋值运算符 答： explicit可以防止内置类型隐式转换,所以在类的构造函数中,最好尽可能多用explicit关键字,防止不必要的隐式转换. 答： 根据变量定义的顺序进行初始化 答： 根据64位机器和32位机器分别是8字节和4字节。 答： 野指针指向的位置是不可知的 答：多线程之间的执行顺序并不确定，并且共享的资源访问可能会出现数据错乱。需要用到互斥和同步等方法。 同步可以使用信号量，条件变量，互斥锁，也可以使用内核socket. C++ 标准模板库（Standard Template Library，STL）是一套功能强大的 C++ 模板类和函数的集合，它提供了一系列通用的、可复用的算法和数据结构。 迭代器是一种模板类的对象，封装了指针并提供了类似指针的功能，用来模拟指针的行为。对不同的对象提供了统一的接口。 答： 新建状态：新建了一个线程对象，该对象就处于新建状态。 就绪状态：线程对象创建后，其他线程调用了该对象的start方法。该状态的线程位于可运行线程池中，变得可以运行，只等待获取cpu的使用权。 运行状态：就绪状态的线程获取了CPU，执行程序代码。 阻塞状态：阻塞状态时线程因为某种原因放弃CPU使用权，暂时停止运行，直到线程进入就绪状态，才有机会转为运行状态。 阻塞的情况分三种： 等待阻塞：运行的线程执行wait方法，该线程会释放占用的所有资源，JVM会把该线程放入等待池中。可以通过notify或notifyAll方法唤醒。同步阻塞：运行的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则JVM会把该线程放入锁池中。其他阻塞：运行的线程执行sleep或join方法或者发出I/O请求时，JVM会把该线程置为阻塞状态。当sleep超时，join等待中止或超时，或I/O完毕，重新进入就绪状态。 死亡状态：线程实行完了，或者因为异常退出了run方法，该线程结束生命周期。 答：原子操作，条件变量，以及线程局部存储变量 答：C++的空类大小是1，因为不同的对象必须要有它自己的地址 答： weakptr拥有指向托管对象的指针以及一个指向智能指针也指向的控制块，但是没有实现解引用运算符和箭头运算符的方法。所以不能直接操作对象，这个控制块存储了usecount，weakcount以及原生指针还有目标对象的删除器。 C++ 智能指针（sharedptr/weakptr）原理分析 - CNHK19 - 博客园 (cnblogs.com) 答： 在函数 f(int a, int b) 中，a 和 b 是函数参数，以传值方式传递。这意味着它们在函数内部是局部变量，存储在栈上。它们的地址关系取决于编译器和具体的调用约定，通常情况下，a 和 b 的地址是连续的，但具体顺序和间隔可能会有所不同。 传值机制：a 和 b 是传值参数，函数接收的是实参的拷贝。 内存位置：它们通常存储在栈上，地址关系由编译器决定，可能连续也可能不连续。 不可预测性：由于栈帧的布局由编译器实现，不同编译器或编译选项可能导致不同的内存布局，因此不应依赖于它们的具体地址关系。 如果关心参数的地址关系，通常是出于调试或学习目的。在实际应用中，不应依赖于这些地址关系来实现功能。 答： 通常拷贝构造函数会执行一个深拷贝的方法，而移动构造我们只需要将指针指向另一块空间即可。 答： 捕获的方式分为两种：“值捕获”和“引用捕获”； lambda表达式的“参数”是在被调用时拷贝，而“被捕获的变量的值”是在lambda表达式创建时拷贝。 理解这一点至关重要，因为lambda表达式捕获的值在创建时被拷贝，因此如果采用“值捕获”的方式，后续对于该变量的修改并不会影响到lambda表达式的计算；但是如果采用“引用捕获”的方式，在lambda表达式被创建后被调用前的这个阶段，如果该值发生了改变，就会影响到lambda表达式的计算结果。 因此，在使用lambda表达式的捕获之前，必须要考虑好这个值是否需要变化，如果需要变化，那么就采用“引用捕获”的方式，如果不需要变化，就可以直接采用“值捕获”的方式。 hash碰撞指的是，两个不同的值（比如张三、李四的学号）经过hash计算后，得到的hash值相同，后来的李四要放到原来的张三的位置，但是数组的位置已经被张三占了，导致冲突。 增加哈希值的位数：通过增加哈希值的位数，可以显著降低碰撞的概率。例如，将哈希值从 32 位增加到 64 位，可以将碰撞的概率降低到几乎可以忽略不计的程度。 使用抗碰撞的哈希函数：一些哈希函数，如 SHA-256 和 SHA-3，被设计为具有很强的抗碰撞性。使用这些哈希函数可以显著降低碰撞的风险。 链地址法：在哈希表中，可以使用链地址法来处理碰撞。具体来说，每个哈希桶维护一个链表，所有哈希值相同的元素都存储在这个链表中。查找时，需要遍历链表来找到目标元素。 开放地址法：另一种处理哈希碰撞的方法是开放地址法。在这种方法中，当发生碰撞时，会尝试在哈希表中寻找另一个空闲的位置来存储元素。常见的开放地址法包括线性探测、二次探测和双重哈希。 再哈希法：当发生碰撞时，可以使用另一个哈希函数来计算新的哈希值，直到找到一个空闲的位置。 unorderedmap底层是使用一个装着list的vector，使用的是链地址法，当size大小到达了bucket的个数时就进行扩容，也就是负载因子到1的时候扩容，可以修改这个阈值。bucket的扩容是 8 64 512 1024 2048 4096 8192 .... 扩容之后进行rehash, 可以使用maxloadfactor函数改变负载因子 在有虚函数的类实例中，this指针调用vptr指针，指向的是vtable(虚函数列表)，通过虚函数列表找到需要调用的虚函数的地址。总体来说虚函数的调用关系是：this指针->vptr(4字节）->vtable ->virtual虚函数。 所以说，static静态函数没有this指针，也就无法找到虚函数了。所以静态成员函数不能是虚函数。他们的关键区别就是this指针。 普通函数（非成员函数）：定义虚函数的主要目的是为了重写达到多态，所以 普通函数声明为虚函数没有意义，因此编译器在编译时就绑定了它。 静态成员函数：静态成员函数对于每个类都只有一份代码，所有对象都可以共 享这份代码，他不归某一个对象所有，所以它也没有动态绑定的必要。 内联成员函数：内联函数本就是为了减少函数调用的代价，所以在代码中直接 展开。但虚函数一定要创建虚函数表，这两者不可能统一。另外，内联函数在 编译时被展开，而虚函数在运行时才动态绑定。 构造函数：这个原因很简单，主要从语义上考虑。因为构造函数本来是为了初 始化对象成员才产生的，然而虚函数的目的是为了在完全不了解细节的情况下 也能正确处理对象，两者根本不能“ 好好相处 ”。因为虚函数要对不同类型的对 象产生不同的动作，如果将构造函数定义成虚函数，那么对象都没有产生，怎 么完成想要的动作呢 友元函数：当我们把一个函数声明为一个类的友元函数时，它只是一个可以访 问类内成员的普通函数，并不是这个类的成员函数，自然也不能在自己的类内 将它声明为虚函数。 将析构函数设置为私有，这样不能生成栈对象 将operator new/operator delete设置为私有，不能生成堆对象 虚基类有以下作用 - 主要用来解决菱形继承时可能发生的对同一基类继承多次而产生的二义性问题 为最远的派生类提供唯一的基类成员，而不重复产生多次复制 这时候虚基指针会指向同一个虚基类 .：成员访问运算符 .：成员指针访问运算符 ::：作用域解析运算符 ?:：条件运算符 sizeof：求字节大小运算符 typeid：类型信息运算符 动态链接的基本思想是把程序按照模块拆分成各个相对独立部分，在程序运行时才将它们链接在一起形成一个完整的程序，而不是像静态链接一样把所有程序模块都链接成一个单独的可执行文件 静态库 静态链接的时候，把库文件打包到程序里面 ，容易部署 难以升级体积大 bash gcc -c add.c 得到add.o ar crsv libadd.a add.o 打包 sudo cp libadd.a /usr/lib 将库文件放在库目录 gcc -o main main.c -ladd 编译时加入库名 动态库 链接的时候，得库文件的位置，在运行时再加载到内存(映射区)，难以部署，容易升级，体积小,dll shell gcc -c add.c -fpic 得到add.o gcc -shared -o libadd.so add.o sudo cp libadd.so /usr/lib gcc -o main main.c -ladd ldd 查看依赖的动态库 版本更新 libadd.so 符号链接(软链接) 删除原软链接 重新建立链接 ln -s sudo ln -s libadd.so.0.0.0 libadd.so extern \"C\" 内联函数分为隐式内联和显示内联，C++中在类内定义的所有函数都自动称为内联函数，类的成员函数的定义直接写在类的声明中时，不需要inline关键字。显示内联是使用Inline关键字 内联函数将调用函数的地方直接展开，.内联是以代码膨胀（复制）为代价，仅仅省去了函数调用的开销，从而提高函数的执行效率。 void\\ realloc (void \\ ptr,sizet size); ptr：初始空间的地址 size：将空间开辟到多大 用于对开辟的空间进行初始化，calloc会将开辟的空间中每个元素初始化为0 void\\ calloc (sizet num,sizet size); num：分配的元素数量 size：每个元素的大小 pthreadmutex 使用了硬件提供的原子操作来实现锁的基本功能，如 compare-and-swap（CAS）等。这些操作确保在多处理器环境中可以安全地执行检查和设置操作。 在 Linux 中，pthreadmutex 常常使用 futex（fast userspace mutex）机制。futex 允许在用户空间执行快速的加锁和解锁操作，而只有在竞争的情况下才会陷入内核。 无竞争路径：如果锁是可用的，线程使用原子操作直接在用户空间获取锁，不需要陷入内核。 竞争路径：如果锁不可用，线程会调用 futex 系统调用进入内核，线程会被挂起，直到锁可用。 为了保证对内存的访问顺序，pthreadmutex 使用内存屏障来防止编译器和 CPU 进行不当的指令重排序。 快速用户区互斥量futex 随着并行的增加，有效的同步和锁机制对性能而言非常重要。如果等待时间短的话，完全可以用自旋锁忙等待，因为如果阻塞进程，陷入内核的开销远远比其大。但如果等待时间长，则会浪费CPU周期。如果有很多竞争，那么阻塞该进程，并仅当锁被释放的时候让内核接触阻塞会更加有效。然而，这却带来了相反的问题：竞争不激烈时，那么不断地内核切换将花销太大。 因此引入了futex，他是linux系统的一个特性，结合了以上两者之所长。它实现了基本的锁，但避免陷入内核，除非它是真的不得不这样做。因为来回切换到内核花销很大，所以这样做可以十分可观的改善性能。一个futex包含两个部分：一个内核服务和一个用户库。内核服务提供一个等待队列，它允许多个进程在一个锁上等待，直到内核对他们解除阻塞。将一个进程放到内核等待队列需要系统调用，而系统调用将陷入内核，这会非常花费时间，我们应该尽量避免他们。因此，在没有竞争时，futex完全在用户空间工作，进程共享一个锁变量，假设锁初始态是释放状态，初始值为1.线程要进入临界区前，通过执行原子操作\"减少并检验\"来获得锁，接下来，这个线程检查结果，看锁是否被释放，如果为处于被锁状态，线程成功获取锁。如果该锁被其他线程持有，则进行系统调用将该线程投入内核等待队列。当一个线程将要退出临界区时，进行原子操作\"增加并检验\"来释放锁，并检查结果，看是否扔有进程阻塞在内核等待队列上。如果有，通知内核可以对等待队列里的一个或个线程解除阻塞。",
    "url": "/c++/编程语言.html",
    "lang": ""
  },
  {
    "title": "编译和解释的区别",
    "content": "--- title: 编译和解释的区别 --- 编译执行:是指程序在执行之前，首先通过编译器将源代码编译为机器代码，然后直接在 CPU 上运行。常见的编译语言如C、C++ 优点:编译后的程序运行速度快，因为机器代码是针对目标平台直接生成的，且不需要在运行时再进行翻译。并且编译器通常会进行指令重排优化代码。 缺点:程序必须针对每个平台重新编译，跨平台性差;另外，编译后生成的机器代码难以调试和逆向工程。 解释执行:解释执行是指源代码不经过编译器的预先编译，而是在运行时通过解释器逐行翻译并执行。常见的解释语言如Python、 Ruby. 优点:跨平台性好，因为代码在每个平台上都是通过相应平台的解释器来运行的，且开发周期更短。 缺点:运行速度较慢，因为每次执行时都需要进行动态翻译和解释。 JVM 采用编译执行和解释执行相结合的方式: 解释执行:JVM 会逐行解释执行字节码，尤其是程序初次运行时，这种方式有助于程序的跨平台性。即时编译(JIT):JVM 引入了即时编译器(Just-In-Time Compiler)，在程序运行时将热代码(经常执行的代码)编译为本地机器码，避免反复解释，提升性能。因此，JVM 实际上是混合使用解释执行和编译执行。 编译器（将高级语言或汇编语言的代码转换成机器认识的代码） 解释器定义：机器不能理解我们用高级语言编写的代码，所以要在程序执行前将高级语言“翻译”为机器语言。 这是一个将源语言程序转化为目标语言程序的过程，它依靠翻译程序来完成。 编译器：将编译型语言（C++，Go）翻译为机器语言。（工作效率高,即时间快、空间省；交互性与动态性差,可移植性差） 解释器：将解释型语言（JavaScript、Python）翻译为机器语言。（工作效率低,即时间慢、空间费；交互性与动态性好,可移植性好） 编译与解释的比较 （1）编译程序会产生目标程序；而解释程序不产生目标程序； （2）编译程序实现起来比较复杂；而解释程序本身实现起来比较简单； （3）编译程序效率比较高；而解释程序运行效率比较低，需对语法、词法、语义等进行检测； 编译是将源代码经过分析后生成语法树，再优化生成中间代码（指令重排），最后生成机器码。编译的结果是生成一个可执行的二进制文件； 而解释也是将源代码经过分析后生成语法树，只不过此后它是基于语法树生成字节码，再根据字节码去执行程序。它并不会生成目标文件，更多的是一个结果。",
    "url": "/c++/编译和解释的区别.html",
    "lang": ""
  },
  {
    "title": "虚函数和纯虚函数的区别",
    "content": "--- title: 虚函数和纯虚函数的区别 --- 虚函数和纯虚函数都用于实现多态。 虚函数 虚函数是在普通函数之前加一个 virtual 关键字 虚函数是在基类中声明的，并且可以在派生类中被重写。 虚函数可以有实现，也就是说，基类中的虚函数可以有一个定义，派生类可以选择提供自己的实现，也可以使用基类的实现。 通过虚函数，可以在基类指针或引用中实现动态绑定，即在运行时确定调用哪个类中的函数实现。 纯虚函数 纯虚函数是在虚函数后面加一个 =0 纯虚函数也是在基类中声明的，但它没有实现，只有声明。 当一个类包含至少一个纯虚函数时，它就成为了一个抽象类，这意味着你不能实例化这样的类，但可以声明这种类型的指针或引用。 区别 是否实现: 虚函数提供函数声明和实现，即提供虚函数的默认实现。 纯虚函数没有函数具体实现，只提供函数声明。 派生类是否实现 派生类可以选择是否覆盖虚函数的默认实现。 当一个类包含至少一个纯虚函数时，派生类必须提供具体实现，否则他们也变成抽象类。 实例化: 包含纯虚函数的类是抽象类，不能被实例化； 而包含虚函数的类不一定是抽象类，可以被实例化，除非它也包含纯虚函数。 目的: 虚函数用于提供一个可以在派生类中被重写的方法实现； 通过纯虚函数，抽象类提供一种接口规范，要求派生类必须提供具体实现。 动态绑定: 虚函数支持动态绑定， 纯虚函数由于没有实现，它们本身不参与动态绑定，但可以作为接口的一部分，影响整个类的多态性。 虚函数和纯虚函数都用于实现多态。虚函数是在普通函数之前加一个 virtual 关键字。虚函数是在基类中声明的，并且可以在派生类中被重写。通过虚函数，可以在基类指针或引用中实现动态绑定，即在运行时确定调用哪个类中的函数实现。而纯虚函数是在虚函数后面加一个 =0 ，纯虚函数也是在基类中声明的，但它没有实现，只有声明。当一个类包含至少一个纯虚函数时，它就成为了一个抽象类，这意味着你不能实例化这样的类，但可以声明这种类型的指针或引用。当一个类包含至少一个纯虚函数时，派生类必须提供具体实现，否则他们也变成抽象。通过纯虚函数，抽象类提供一种接口规范，要求派生类必须提供具体实现。",
    "url": "/c++/虚函数和纯虚函数的区别.html",
    "lang": ""
  },
  {
    "title": "虚函数是怎么实现的",
    "content": "--- title: 虚函数是怎么实现的 --- 虚函数的实现依赖于一种称为虚函数表的机制。 虚函数表的创建: 当一个类包含虚函数时，编译器会自动为这个类创建一个虚函数表。这个表是一个函数指针数组，每个指针指向一个虚函数的实现。 虚函数表指针: 编译器会在对象的内存布局中添加一个隐式的虚函数表指针（通常是一个指向 vtable 的指针），这样每个对象都可以通过这个指针访问到类的虚函数表。 虚函数的声明: 在类中声明虚函数时，可以使用 virtual 关键字。如果一个函数被声明为虚函数，编译器会在类的 vtable 中为这个函数分配一个入口。 重写虚函数: 当从基类继承并创建派生类时，可以在派生类中重写基类的虚函数。重写的函数会替换掉 vtable 中对应的基类实现。 动态绑定: 当通过基类指针或引用调用虚函数时，程序会使用对象的虚函数表指针来查找正确的函数实现。这个过程称为动态绑定或晚期绑定。 调用虚函数: 程序运行时，当调用一个虚函数时，会先通过对象的虚函数表指针找到 vtable，然后在 vtable 中查找对应的函数指针，并调用该函数。 简短来说，每个类都有一个虚表，里面有这个类的虚函数地址；每个对象都有指向它的类的虚表的指针，这个指针称为虚指针。 当调用虚函数时，编译器会调用对象的虚指针查找虚表，通过虚函数的地址来执行相应的虚函数。",
    "url": "/c++/虚函数是怎么实现的.html",
    "lang": ""
  },
  {
    "title": "虚函数表是什么",
    "content": "--- title: 虚函数表是什么 --- 虚函数表是 C++ 中实现运行时多态（动态绑定）的关键机制之一。 虚函数表是一个或多个函数指针的集合，它存储了类中所有虚函数的地址。当类包含虚函数时，编译器会自动为这个类创建一个虚函数表。 虚函数表的主要目的是在运行时能够确定通过基类指针或引用调用的是哪个派生类中的虚函数实现，从而实现动态绑定。 原理 创建虚函数表：当类声明至少一个虚函数时，编译器会为这个类生成一个虚函数表。 虚函数表指针：编译器会为包含虚函数的类的对象添加一个隐藏的虚函数表指针（通常是一个指针或引用），指向类的虚函数表。 调用虚函数：当通过基类指针或引用调用虚函数时，程序会使用对象的虚函数表指针来查找并调用正确的函数实现。",
    "url": "/c++/虚函数表是什么.html",
    "lang": ""
  },
  {
    "title": "说一下lambda函数",
    "content": "--- title: 说一下lambda函数 --- lambda 表达式（也称为匿名函数）是在 C++11 标准中引入的一种方便的函数编写方式。Lambda 允许你在需要一个函数对象的地方快速定义函数的行为，而不需要按照传统方式定义一个完整的函数。 我们主要出于这些情况用 lambda 函数： API：Lambda 函数可以简化回调函数的编写，特别是在使用 STL 时。 简洁：在需要临时使用一个函数但不想定义一个完整函数的情况下。 闭包：Lambda 可以捕获外部变量，形成闭包，使得函数可以访问和操作外部作用域的变量。",
    "url": "/c++/说一下lambda函数.html",
    "lang": ""
  },
  {
    "title": "静态变量和全局变量、局部变量的区别、在内存上是怎么分布的",
    "content": "--- title: 静态变量和全局变量、局部变量的区别、在内存上是怎么分布的 --- 静态局部变量 特点： 作用域：仅限于声明它们的函数或代码块内部。 生命周期：静态局部变量在程序的整个运行期间都存在，只初始化一次（在第一次使用前）。 初始化：在首次进入函数时初始化，并保持值直到程序结束。 使用场景： 当你需要一个仅在函数内部使用，但希望其值在函数调用之间保持不变的变量时。 适用于需要缓存数据以提高性能的情况。 内存分布：静态局部变量存储在全局/静态存储区。 局部变量 特点： 作用域：局部变量仅在声明它们的函数或代码块内部可见。 生命周期：局部变量在函数调用时创建，函数调用结束后销毁。 初始化：必须在使用前显式初始化。 使用场景： 需要临时存储数据，且这些数据只在当前作用域内使用时。 作为循环计数器或中间计算结果。 内存分布：局部变量存储在栈上，与它们所在的作用域（如函数）相关联。 全局变量 特点： 作用域：全局变量在整个程序中都是可见的，可以在任何函数或代码块中访问。 生命周期：全局变量同样具有静态存储期，它们在程序的整个运行期间都存在。 初始化：通常在程序启动时初始化。 使用场景： 当你需要在程序的多个部分共享数据时。 适用于存储配置信息或程序的状态信息。 需要注意全局变量可能导致代码难以测试和维护。 内存分布：全局变量也存储在全局/静态存储区。 静态局部变量的作用域仅限于声明它们的代码块内部，但是在程序的整个运行期间都存在，只初始化一次，它存储在全局静态存储区。而静态全局变量的作用域只在定义了该变量的源文件中。 局部变量的作用域仅限于声明它们的代码块内部，它的生命周期是在函数期间或者代码块，它存储在栈上。 全局变量的作用域在任何函数中都可访问，在程序的整个运行期间都存在，它存储在全局存储区。",
    "url": "/c++/静态变量和全局变量、局部变量的区别、在内存上是怎么分布的.html",
    "lang": ""
  },
  {
    "title": "复杂指令集和简单指令集",
    "content": "--- title: 复杂指令集和简单指令集 --- CISC（复杂指令集）结构主要优点是： 1.指令丰富，功能强大 2.寻址方式灵活。 3.以微程序控制器为核心，指令存储器与数据存储器共享同一个物理存储空间，性能强大。 CISC结构主要缺点是： 1.指令使用率不均衡。 2.不利于采用先进结构提高性能。 3.结构复杂不利于VLSI(超大规模集成电路)实现。 RISC（简单指令集）结构主要优点是： 1.具备结构简单、易于设计 2.指令精简，使用率均衡 3.程序执行效率高 RISC结构主要缺点是： 1.指令数较少，功能不及CISC强大。 2.寻址方式不够灵活。",
    "url": "/操作系统相关/复杂指令集和简单指令集.html",
    "lang": ""
  },
  {
    "title": "系统调用的过程",
    "content": "--- title: 系统调用的过程 --- 系统调用的实现涉及到用户态和核心态之间的切换，以及操作系统内核对请求的处理。以下是系统调用实现的基本步骤： 用户程序发起请求： 用户程序通过执行一个特殊的系统调用指令（如int指令在x86架构中）来发起系统调用请求。这通常涉及到设置一个系统调用号和必要的参数。 中断和上下文切换： 特殊的系统调用指令会触发一个==中断==，导致CPU从用户态切换到核心态。操作系统会保存当前用户程序的状态（上下文），以便之后可以恢复执行。 参数传递： 用户程序在发起系统调用时会传递参数，这些参数在内核中被解析，以便内核知道要执行的具体服务。 内核处理请求： 操作系统内核会根据系统调用号和传递的参数执行相应的服务。这可能涉及到访问文件系统、管理进程、处理I/O请求等。 执行系统调用： 内核中有一个系统调用表，它将系统调用号映射到具体的内核函数。内核会查找这个表，执行对应的函数来处理请求。 返回结果： 系统调用完成后，内核会将结果（如果有的话）返回给用户程序。如果系统调用成功，它会返回一个非负值；如果失败，它会返回一个错误码。 上下文恢复和返回用户态： 内核处理完请求后，会恢复用户程序的执行状态（上下文），并将CPU控制权交还给用户程序。 用户程序继续执行： 用户程序从系统调用指令的下一条指令继续执行，这时它已经得到了操作系统服务的结果。",
    "url": "/操作系统相关/系统调用的过程.html",
    "lang": ""
  },
  {
    "title": "多进程和多线程的使用场景",
    "content": "--- title: 多进程和多线程的使用场景 --- ①需要频繁创建销毁的优先用线程（进程的创建和销毁开销过大） 这种原则最常见的应用就是Web服务器了，来一个连接建立一个线程，断了就销毁线程，要是用进程，创建和销毁的代价是很难承受的 适合使用多进程的情况： 安全，隔离性需求使用进程 CPU 密集型任务：如果应用程序需要大量的 CPU 计算，并且没有太多的 I/O 操作，则使用多进程可能更加适合。这是因为每个进程都有自己的 CPU 时间片，可以并行执行，从而提高整个应用程序的性能。 隔离性：如果应用程序需要隔离不同的任务或数据，以确保它们不会相互影响，则使用多进程可能更加适合。每个进程都有自己的地址空间和资源，因此不同的进程可以独立运行，互相不会干扰。 适合使用多线程的情况： I/O 密集型任务：如果应用程序需要大量的 I/O 操作（例如网络通信、磁盘读写等），则使用多线程可能更加适合。这是因为 I/O 操作通常是阻塞的，一个线程被阻塞时，另一个线程可以继续执行，从而提高整个应用程序的性能。 共享性：如果应用程序需要共享数据，并且需要在不同的任务之间共享数据，那么使用多线程可能更加适合。线程可以在同一地址空间内运行，它们可以访问相同的变量和数据结构，从而可以方便地共享数据。 轻量级任务：如果应用程序需要处理大量的轻量级任务，并且创建进程的开销太大，那么使用多线程可能更加适合。线程比进程更加轻量级，创建和销毁线程的开销也比较小，可以更加高效地处理大量的轻量级任务。 需要注意的是，多进程和多线程都有各自的优缺点，选择合适的技术取决于具体的应用场景和需求。同时，在使用多进程或多线程时，还需要考虑到线程或进程间的同步和通信问题，以确保它们能够正确地协作工作。 多线程和多进程 及其应用场景多进程与多线程的区别以及使用场景?-CSDN博客",
    "url": "/操作系统相关/多进程和多线程的使用场景.html",
    "lang": ""
  },
  {
    "title": "可重入锁",
    "content": "--- title: 可重入锁 --- 可重入锁：当线程获取某个锁后，还可以继续获取它，可以递归调用，而不会发生死锁； 不可重入锁：与可重入相反，获取锁后不能重复获取，否则会死锁（自己锁自己）。",
    "url": "/操作系统相关/可重入锁.html",
    "lang": ""
  },
  {
    "title": "进程有哪些状态",
    "content": "--- title: 进程有哪些状态 --- 就绪态：指进程其他资源已经准备好，只差cpu调度了 运行态：指进程当前正在被cpu执行 阻塞态：指进程由于其他资源还没准备好，所以让出CPU，等待资源的就绪 LINUX中的六种状态： R运行状态（running）：并不意味着进程一定在运行中，它表明进程要么是在运行中要么在运行队列。 S睡眠状态（sleeping)：意味着进程在等待事件完成（这里的睡眠有时候也叫做可中断睡眠（interruptible sleep）） D磁盘休眠状态（Disk sleep）：有时候也叫不可中断睡眠状态（uninterruptible sleep），在这个状态的进程通常会等待IO的结束。即使在睡眠状态,OS也杀不掉。 T停止状态（stopped）：可以通过发送 SIGSTOP 信号给进程来停止（T）进程。这个被暂停的进程可以通过发送 SIGCONT 信号让进程继续运行。 Z僵尸状态（Zombies）：子进程退出并且父进程没有读取到子进程退出的返回代码时，就会产生僵死(尸)进程， 僵死进程会以终止状态保持在进程表中，并且会一直在等待父进程读取退出状态代码。所以，只要子进程退出，父进程还在运行，但父进程没有读取子进程状态，子进程进入Z状态。 因为我们必须得保证一个进程跑完，启动这个进程的父进程或是操作系统必须得知道这个进程退出时，把我们交代得任务完成得怎么样了，成功还是失败了。必须要知道子进程得运行结果。当子进程退出的时候，它的信息不会立即释放，会存在PCB中，没有人读取，这个状态不会被释放掉，这个状态就是僵尸状态。 X死亡状态（dead）：这个状态只是一个返回状态，你不会在任务列表里看到这个状态。当父进程读取子进程的返回结果时，子进程立刻释放资源。死亡状态是非常短暂的，几乎不可能通过ps命令捕捉到。",
    "url": "/操作系统相关/进程有哪些状态.html",
    "lang": ""
  },
  {
    "title": "CPU的cache",
    "content": "--- title: CPU的cache --- cache分为一级cache,二级三级，后面是内存，cache是用来缓存最常用的那些数据，空间换时间的思想，cache的速度越靠近CPU是越快的，cache的出现就是来匹配CPU的速度. 缓存行的四个状态： 需要利用总线嗅探的机制实现了事务串行化（总线嗅探（Bus Snooping）是一种用于监控和管理多处理器系统或多核系统中共享总线数据传输的机制。它允许各个处理器或缓存控制器观察和响应总线上的活动，以确保数据的一致性和有效性。） MESI中每个缓存行都有四个状态，分别是E（exclusive）、M（modified）、S（shared）、I（invalid）。下面我们介绍一下这四个状态分别代表什么意思。 M：代表该缓存行中的内容被修改了，并且该缓存行只被缓存在该CPU中，还没有同步到内存中。这个状态的缓存行中的数据和内存中的不一样，在未来的某个时刻它会被写入到内存中（当其他CPU要读取该缓存行的内容时。或者其他CPU要修改该缓存对应的内存中的内容时（个人理解CPU要修改该内存时先要读取到缓存中再进行修改），这样的话和读取缓存中的内容其实是一个道理）。 E：E代表该缓存行对应内存中的内容只被该CPU缓存，其他CPU没有缓存该缓存对应内存行中的内容，此时缓存内容和内存中是一致的。这个状态的缓存行中的内容和内存中的内容一致。该缓存可以在任何其他CPU读取该缓存对应内存中的内容时变成S状态。或者本地处理器写该缓存就会变成M状态。 S:该状态意味着数据不止存在本地CPU缓存中，还存在别的CPU的缓存中。这个状态的数据和内存中的数据是一致的。当有一个CPU修改该缓存行对应的内存的内容时会使其他缓存行变成 I 状态。 I：代表该缓存行中的内容是无效的。 原子操作：先lock指令，只需阻止其他核心对相关内存空间的访问(对应到MESI协议，就只需要阻止M,E这两个状态发生改变，就可以实现原子操作)，任何改变这两种状态的行为都需要阻塞 E -> I :其他核心修改这个数据 E -> S :其他核心读这个数据 M -> I :其他核心修改这个数据 M -> S :其他核心读这个数据",
    "url": "/操作系统相关/CPU的cache.html",
    "lang": ""
  },
  {
    "title": "线程创建最大数量的计算公式",
    "content": "--- title: 线程创建最大数量的计算公式 --- 我们可以执行 ulimit -a 这条命令，查看进程创建线程时默认分配的栈空间大小，比如我这台服务器默认分配给线程的栈空间大小为 8M，在32位linux系统中，一个进程留给用户使用的只有3G大小，那么假设创建一个线程需要占用 10M 虚拟内存，总共有 3G 虚拟内存可以使用。于是我们可以算出，最多可以创建差不多 300 个（3G/10M）左右的线程。",
    "url": "/操作系统相关/线程创建最大数量的计算公式.html",
    "lang": ""
  },
  {
    "title": "进程和线程之间有什么区别",
    "content": "--- title: 进程和线程之间有什么区别 --- 首先进程是资源分配的基本单位，而线程是程序执行的最小单位 从内存上来说，每个进程都有自己独立的地址空间，创建和销毁的开销比较大，并且进程间切换需要保存和恢复整个进程的状态，进程之间的切换还要切换页表，频繁切换页表还会使得TLB缓存命中率下降，所以上下文切换开销比较高。而线程共享相同的内存空间，只有自己的栈，程序计数器，方法栈，创建销毁开销比较小。由于线程独占的资源比较少所以上下文切换开销比较少。 从通信上来说，进程之间相互隔离，所以进程之间的通信需要使用一些特殊机制IPC，比如管道，消息队列，共享内存等。而线程是共享相同的堆内存空间的，所以可以直接访问共享数据。 从安全性上来说，一个进程的崩溃不会直接影响到其他进程的稳定性。而线程是共享相同的内存空间，一个线程的错误可能会影响整个进程的稳定性。 协程可以理解为是一种轻量级线程，用户态线程。协程的本质就是函数和函数运行状态的组合，就是可以暂停和恢复的函数。其实就是我们可以在堆上模拟出一块空间，然后分配寄存器，再分配一个协程号，这个协程就算创建成功了，这个切换完全是在用户态操作的，不用像线程那样陷入内核态。操作系统是感知不到这个协程的，协程的运行和调度都要由应用程序来完成，一个协程的yield必然对应另一个协程的resume。协程不能通过锁机制来实现同步，多协程是通过协程队列解决竞态问题。协程非常适合高并发，IO密集型的应用，因为可以利用io等待的时间来执行其他任务。 进程是资源分配的基本单位。 线程是程序执行和调度的最小单位，线程是进程的子任务，是进程内的执行单元。 一个进程至少有一个线程，一个进程可以运行多个线程，这些线程共享同一块内存。 资源开销： 进程：由于每个进程都有独立的内存空间，创建和销毁进程的开销较大。进程间切换需要保存和恢复整个进程的状态，因此上下文切换的开销较高。 线程：线程共享相同的内存空间，创建和销毁线程的开销较小。线程间切换只需要保存和恢复少量的线程上下文，因此上下文切换的开销较小。 通信与同步： 进程：由于进程间相互隔离，进程之间的通信需要使用一些特殊机制，如管道、消息队列、共享内存等。 线程：由于线程共享相同的内存空间，它们之间可以直接访问共享数据，线程间通信更加方便。 安全性： 进程：由于进程间相互隔离，一个进程的崩溃不会直接影响其他进程的稳定性。 线程：由于线程共享相同的内存空间，一个线程的错误可能会影响整个进程的稳定性。 对于，线程相⽐进程能减少开销，体现在： 线程的创建时间⽐进程快，因为进程在创建的过程中，还需要资源管理信息，⽐如内存管理信息、⽂件管理信息，⽽线程在创建的过程中，不会涉及这些资源管理信息，⽽是共享它们； 线程的终⽌时间⽐进程快，因为线程释放的资源相⽐进程少很多； 同⼀个进程内的线程切换⽐进程切换快，因为线程具有相同的地址空间（虚拟内存共享），这意味着同⼀个进程的线程都具有同⼀个⻚表，那么在切换的时候不需要切换⻚表。⽽对于进程之间的切换，切换的时候要把⻚表给切换掉，⽽⻚表的切换过程开销是⽐较⼤的； 由于同⼀进程的各线程间共享内存和⽂件资源，那么在线程之间数据传递的时候，就不需要经过内核了，这就使得线程之间的数据交互效率更⾼了； 协程的产生原因： 一是系统线程会占用非常多的内存空间，二是过多的线程切换会占用大量的系统时间。 协程运行在线程之上，当一个协程执行完成后，可以选择主动让出，让另一个协程运行在当前线程之上。协程并没有增加线程数量，只是在线程的基础之上通过分时复用的方式运行多个协程，而且协程的切换在用户态完成，切换的代价比线程从用户态到内核态的代价小很多。 实际上协程并不是什么银弹，协程只有在等待IO的过程中才能重复利用线程，上面我们已经讲过了，线程在等待IO的过程中会陷入阻塞状态，意识到问题没有？ 假设协程运行在线程之上，并且协程调用了一个阻塞IO操作，这时候会发生什么？实际上操作系统并不知道协程的存在，它只知道线程，因此在协程调用阻塞IO操作的时候，操作系统会让线程进入阻塞状态，当前的协程和其它绑定在该线程之上的协程都会陷入阻塞而得不到调度，这往往是不能接受的。 因此在协程中不能调用导致线程阻塞的操作。也就是说，协程只有和异步IO结合起来，才能发挥最大的威力。 协程相当于是一个用户级线程 CPU 寄存器和程序计数器是 CPU 在运⾏任何任务前，所必须依赖的环境，这些环境就叫做 CPU 上下⽂。 进程的上下⽂切换不仅包含了虚拟内存、栈、全局变量等⽤户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。",
    "url": "/操作系统相关/进程和线程之间有什么区别.html",
    "lang": ""
  },
  {
    "title": "并行和并发有什么区别",
    "content": "--- title: 并行和并发有什么区别 --- 并行是在同一时刻执行多个任务。这些任务可以同时进行，每个任务都在不同的处理单元（如多个CPU核心）上执行 并发是在相同的时间段内执行多个任务，任务可能交替执行，通过调度实现。在宏观上相当于是同时执行的 并行是指在同一时刻执行多个任务，这些任务可以同时进行，每个任务都在不同的处理单元（如多个CPU核心）上执行。在并行系统中，多个处理单元可以同时处理独立的子任务，从而加速整体任务的完成。 并发是指在相同的时间段内执行多个任务，这些任务可能不是同时发生的，而是交替执行，通过时间片轮转或者事件驱动的方式。并发通常与任务之间的交替执行和任务调度有关。",
    "url": "/操作系统相关/并行和并发有什么区别.html",
    "lang": ""
  },
  {
    "title": "解释一下用户态和核心态，什么场景下，会发生内核态和用户态的切换？",
    "content": "--- title: 解释一下用户态和核心态，什么场景下，会发生内核态和用户态的切换？ --- 用户态和内核态的区别 用户态和内核态是操作系统为了保护系统资源和实现权限控制而设计的两种不同的CPU运行级别，可以控制进程或程序对计算机硬件资源的访问权限和操作范围。 用户态：在用户态下，进程或程序只能访问受限的资源和执行受限的指令集，不能直接访问操作系统的核心部分，也不能直接访问硬件资源。 核心态：核心态是操作系统的特权级别，允许进程或程序执行特权指令和访问操作系统的核心部分。在核心态下，进程可以直接访问硬件资源，执行系统调用，管理内存、文件系统等操作。 在什么场景下，会发生内核态和用户态的切换 系统调用：当用户程序需要请求操作系统提供的服务时，会通过系统调用(软中断)进入内核态。 异常：当程序执行过程中出现错误或异常情况时，CPU会自动切换到内核态，以便操作系统能够处理这些异常。 中断：外部设备（如键盘、鼠标、磁盘等）产生的中断信号会使CPU从用户态切换到内核态。操作系统会处理这些中断，执行相应的中断处理程序，然后再将CPU切换回用户态。",
    "url": "/操作系统相关/解释一下用户态和核心态，什么场景下，会发生内核态和用户态的切换？.html",
    "lang": ""
  },
  {
    "title": "进程调度算法你了解多少",
    "content": "--- title: 进程调度算法你了解多少 --- 先来先服务(FCFS)：按照请求的顺序进行调度。 这种调度方式简单，但是能导致较长作业阻塞较短作业。 最短作业优先(SJF)：非抢占式的调度算法，按估计运行时间最短的顺序进行调度。 但是如果一直有短作业到来，那么长作业永远得不到调度，造成长作业“饥饿”现象。 最短剩余时间优先(SRTF)：基于最短作业优先改进，按剩余运行时间的顺序进行调度。当一个新的作业到达时，其整个运行时间与当前进程的剩余时间作比较。如果新的进程需要的时间更少，则挂起当前进程，运行新的进程。否则新的进程等待。 优先级调度(PS)：为每个进程分配一个优先级，按优先级进行调度。为了防止低优先级的进程永远等不到调度，可以随着时间的推移增加等待进程的优先级。 时间片轮转(RR)：为每个进程分配一个时间片，进程轮流执行，时间片用完后切换到下一个进程。 多级队列(MQS)：时间片轮转调度算法和优先级调度算法的结合。 将进程分为不同的优先级队列，每个队列有自己的调度算法。",
    "url": "/操作系统相关/进程调度算法你了解多少.html",
    "lang": ""
  },
  {
    "title": "进程间有哪些通信方式",
    "content": "--- title: 进程间有哪些通信方式 --- 管道：是一种半双工的通信方式，数据只能单向流动，而且只能在具有父子进程关系的进程间使用。 命名管道： 类似管道，也是半双工的通信方式，但是它允许在不相关的进程间通信。 消息队列：允许进程发送和接收消息，而消息队列是消息的链表，可以设置消息优先级。 信号：用于发送通知到进程，告知其发生了某种事件或条件。 信号量：是一个计数器，可以用来控制多个进程对共享资源的访问，常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此主要作为进程间以及同一进程内不同线程之间的同步手段。 共享内存：就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的进程通信方式， Socket套接字：是支持TCP/IP 的网络通信的基本操作单元，主要用于在客户端和服务器之间通过网络进行通信。",
    "url": "/操作系统相关/进程间有哪些通信方式.html",
    "lang": ""
  },
  {
    "title": "解释一下进程同步和互斥，以及如何实现进程同步和互斥",
    "content": "--- title: 解释一下进程同步和互斥，以及如何实现进程同步和互斥 --- 进程同步是指多个并发执行的进程之间协调和管理它们的执行顺序，以确保它们按照一定的顺序或时间间隔执行。 互斥指的是在某一时刻只允许一个进程访问某个共享资源。当一个进程正在使用共享资源时，其他进程不能同时访问该资源。 解决进程同步和互斥的问题有很多种方法，其中一种常见的方法是使用信号量和 PV 操作。信号量是一种特殊的变量，它表示系统中某种资源的数量或者状态。PV 操作是一种对信号量进行增加或者减少的操作，它们可以用来控制进程之间的同步或者互斥。 P操作：相当于“检查”信号量，如果资源可用，就减少计数，然后使用资源。 V操作：相当于“归还”资源，增加信号量的计数，并可能唤醒等待的进程。 除此之外，下面的方法也可以解决进程同步和互斥问题： 临界区：将可能引发互斥问题的代码段称为临界区，里面包含了需要互斥访问的资源。进入这个区域前需要先获取锁，退出临界区后释放该锁。这确保同一时间只有一个进程可以进入临界区。 互斥锁（Mutex）：互斥锁是一种同步机制，用于实现互斥。每个共享资源都关联一个互斥锁，进程在访问该资源前需要先获取互斥锁，使用完后释放锁。只有获得锁的进程才能访问共享资源。 条件变量：条件变量用于在进程之间传递信息，以便它们在特定条件下等待或唤醒。通常与互斥锁一起使用，以确保等待和唤醒的操作在正确的时机执行。",
    "url": "/操作系统相关/解释一下进程同步和互斥，以及如何实现进程同步和互斥.html",
    "lang": ""
  },
  {
    "title": "什么是死锁，如何避免死锁？",
    "content": "--- title: 什么是死锁，如何避免死锁？ --- 系统资源的竞争 系统资源的竞争导致系统资源不足，以及资源分配不当，导致死锁。 进程运行推进顺序不合适 进程在运行过程中，请求和释放资源的顺序不当，会导致死锁 死锁是系统中两个或多个进程在执行过程中，因争夺资源而造成的一种僵局。当每个进程都持有一定的资源并等待其他进程释放它们所需的资源时，如果这些资源都被其他进程占有且不释放，就导致了死锁。 死锁只有同时满足以下四个条件才会发生： 互斥条件：一个进程占用了某个资源时，其他进程无法同时占用该资源。 请求保持条件：一个进程因为请求资源而阻塞的时候，不会释放自己的资源。 不可剥夺条件：资源不能被强制性地从一个进程中剥夺，只能由持有者自愿释放。 循环等待条件：多个进程之间形成一个循环等待资源的链，每个进程都在等待下一个进程所占有的资源。 避免死锁：通过破坏死锁的四个必要条件之一来预防死锁。比如破坏循环等待条件，让所有进程按照相同的顺序请求资源。 检测死锁：通过检测系统中的资源分配情况来判断是否存在死锁。例如，可以使用资源分配图或银行家算法进行检测。 解除死锁：一旦检测到死锁存在，可以采取一些措施来解除死锁。例如，可以通过抢占资源、终止某些进程或进行资源回收等方式来解除死锁。 死锁是系统中两个或多个线程在执行过程中，因争夺资源而造成的一种僵局。当每个线程都持有一定的资源并等待其他线程释放它们所需的资源时，如果这些资源都被其他线程占有且不释放，就导致了死锁。 死锁只有同时满足以下四个条件才会发生： 互斥条件：一个进程占用了某个资源时，其他进程无法同时占用该资源。 请求保持条件：一个进程因为请求资源而阻塞的时候，不会释放自己的资源。 不可剥夺条件：资源不能被强制性地从一个进程中剥夺，只能由持有者自愿释放。 循环等待条件：多个进程之间形成一个循环等待资源的链，每个进程都在等待下一个进程所占有的资源。 避免死锁：通过破坏死锁的四个必要条件之一来预防死锁。比如破坏循环等待条件，让所有进程按照相同的顺序请求资源。 检测死锁：通过检测系统中的资源分配情况来判断是否存在死锁。例如，可以使用资源分配图或银行家算法进行检测。 解除死锁：一旦检测到死锁存在，可以采取一些措施来解除死锁。例如，可以通过抢占资源、终止某些进程或进行资源回收等方式来解除死锁。 首先如果要解除死锁的话基本上只能通过终止进程或者进行资源回收的方式来解除死锁，很难人为干预，所以说我们最好是避免死锁。说到死锁的四个条件，互斥条件本来是解决线程安全的，所以说不用去考虑通过去改变互斥条件来避免死锁了。所以说我们需要从请求保持条件，不可剥夺条件，循环等待条件这三个条件来避免。 对于请求保持条件，我们可以一次性去申请需要的所有资源，这样就不存在等待的问题了。 对于不可剥夺条件，就是使线程如果申请不到其他资源的时候，就主动释放掉自己的已经占有的资源。 对于循环等待条件呢，我们可以按顺序去申请资源，这样就不会出现循环等待问题了。每个进程只能按递增顺序申请资源因此每个时刻总有一个进程占据了较高序号的资源，那么它后面继续申请的资源一定是空闲的，这就保证了进程是可以一直向前推进的",
    "url": "/操作系统相关/什么是死锁，如何避免死锁？.html",
    "lang": ""
  },
  {
    "title": "介绍一下几种典型的锁",
    "content": "--- title: 介绍一下几种典型的锁 --- 互斥锁：互斥锁是一种最常见的锁类型，用于实现互斥访问共享资源。在任何时刻，只有一个线程可以持有互斥锁，其他线程必须等待直到锁被释放。这确保了同一时间只有一个线程能够访问被保护的资源。对于互斥锁来说，如果获取锁失败，操作系统会将线程阻塞，把线程id记录到和这个锁相关的队列中，等其他线程释放了这个互斥锁，才会通过这个等待队列唤醒休眠线程。 自旋锁：自旋锁是一种基于忙等待的锁，即线程在尝试获取锁时会不断轮询，直到锁被释放，是完全在用户态的，并且一直占用CPU，但是在其他线程很快就释放锁的情况下，效率较高，因为避免了线程从阻塞态恢复到运行态的这个过程。 其他的锁机制都是基于这两个锁的 读写锁：允许多个线程同时读共享资源，只允许一个线程进行写操作。分为读（共享）和写（排他）两种状态。 悲观锁：认为多线程同时修改共享资源的概率比较高，所以访问共享资源时候要上锁 乐观锁：本质上是一种无锁并发控制，先不管，修改了共享资源再说，如果出现同时修改的情况，再放弃本次操作，可以利用CAS和版本号来实现。 其他的锁都是基于这两个锁的，如果你能确定被锁住的代码执⾏时间很短，就不应该⽤互斥锁，⽽应该选⽤⾃旋锁， 否则使⽤互斥锁。 你会发现乐观锁全程并没有加锁，所以它也叫⽆锁编程 乐观锁虽然去除了加锁解锁的操作，但是⼀旦发⽣冲突，重试的成本⾮常⾼，所以只有在冲突概率⾮常低，且加锁成本⾮常⾼的场景时，才考虑使⽤乐观锁。",
    "url": "/操作系统相关/介绍一下几种典型的锁.html",
    "lang": ""
  },
  {
    "title": "讲一讲你理解的虚拟内存",
    "content": "--- title: 讲一讲你理解的虚拟内存 --- 虚拟内存是指在每一个进程创建加载的过程中，会分配一个连续虚拟地址空间，它不是真实存在的，而是通过映射与实际物理地址空间对应，这样就可以使每个进程看起来都有自己独立的连续地址空间，并允许程序访问比物理内存RAM更大的地址空间, 每个程序都可以认为它拥有足够的内存来运行。 需要虚拟内存的原因： 内存扩展： 虚拟内存使得每个程序都可以使用比实际可用内存更多的内存，从而允许运行更大的程序或处理更多的数据。 内存隔离：虚拟内存还提供了进程之间的内存隔离。每个进程都有自己的虚拟地址空间，因此一个进程无法直接访问另一个进程的内存。 物理内存管理：虚拟内存允许操作系统动态地将数据和程序的部分加载到物理内存中，以满足当前正在运行的进程的需求。当物理内存不足时，操作系统可以将不常用的数据或程序暂时移到硬盘上，从而释放内存，以便其他进程使用。 页面交换：当物理内存不足时，操作系统可以将一部分数据从物理内存写入到硬盘的虚拟内存中，这个过程被称为页面交换。当需要时，数据可以再次从虚拟内存中加载到物理内存中。这样可以保证系统可以继续运行，尽管物理内存有限。 内存映射文件：虚拟内存还可以用于将文件映射到内存中，这使得文件的读取和写入可以像访问内存一样高效。",
    "url": "/操作系统相关/讲一讲你理解的虚拟内存.html",
    "lang": ""
  },
  {
    "title": "你知道的线程同步的方式有哪些？",
    "content": "--- title: 你知道的线程同步的方式有哪些？ --- 线程同步机制是指在多线程编程中，为了保证线程之间的互不干扰，而采用的一种机制。常见的线程同步机制有以下几种： 互斥锁：互斥锁是最常见的线程同步机制。它允许在同一时刻只有一个线程访问被保护的临界区（共享资源） 条件变量：条件变量用于线程间通信，允许一个线程等待某个条件满足，而其他线程可以发出信号通知等待线程。通常与互斥锁一起使用。 读写锁： 读写锁允许多个线程同时读取共享资源，但只允许一个线程写入资源。 信号量：用于控制多个线程对共享资源进行访问的工具。",
    "url": "/操作系统相关/你知道的线程同步的方式有哪些？.html",
    "lang": ""
  },
  {
    "title": "有哪些页面置换算法？",
    "content": "--- title: 有哪些页面置换算法？ --- 常见页面置换算法有最佳置换算法（OPT）、先进先出（FIFO）、最近最久未使用算法（LRU）、时钟算法（Clock） 等。 最近最久未使用算法LRU ：LRU算法基于页面的使用历史，通过选择最长时间未被使用的页面进行置换。 先进先出FIFO算法：也就是最先进入内存的页面最先被置换出去。 最不经常使用LFU ：淘汰访问次数最少的页面，考虑页面的访问频率。 时钟算法CLOCK：Clock算法的核心思想是通过使用一个指针(称为时钟指针)在环形链表上遍历，检查页面是否被访问过, 当需要进行页面置换时，Clock算法从时钟指针的位置开始遍历环形链表。 如果当前页面的访问位为0，表示该页面最久未被访问，可以选择进行置换。将访问位设置为1，继续遍历下一个页面。 如果当前页面的访问位为1，表示该页面最近被访问过，它仍然处于活跃状态。将访问位设置为0，并继续遍历下一个页面如果遍历过程中找到一个访问位为0的页面，那么选择该页面进行置换。 最佳置换算法: 该算法根据未来的页面访问情况，选择最长时间内不会被访问到的页面进行置换。那么就有一个问题了，未来要访问什么页面，操作系统怎么知道的呢?操作系统当然不会知道，所以这种算法只是一种理想情况下的置换算法，通常是无法实现的。",
    "url": "/操作系统相关/有哪些页面置换算法？.html",
    "lang": ""
  },
  {
    "title": "熟悉哪些linux命令",
    "content": "--- title: 熟悉哪些linux命令 --- 文件操作： ls：列出目录内容。 cd：改变当前目录。 pwd：显示当前工作目录。 cp：复制文件或目录。 mv：移动或重命名文件。 rm：删除文件或目录。 touch：创建空文件或更新文件时间戳。 文件内容查看： cat：查看文件内容。 head：查看文件的前几行。 tail：查看文件的后几行，常用于查看日志文件。 文件编辑： vi 或 vim：强大的文本编辑器。 权限管理： chmod：更改文件或目录的访问权限。 chown：更改文件或目录的所有者和/或所属组。 磁盘管理： df：查看磁盘空间使用情况。 网络管理： ifconfig 或 ip addr：查看和配置网络接口。 ping：测试网络连接。 netstat：查看网络状态和统计信息。 ssh：安全远程登录。 nc或者telnet 产生tcp连接 进程管理： ps：查看当前运行的进程。 kill：发送信号给进程。 软件包管理 （根据Linux发行版不同，命令可能有所不同）： apt-get（Debian/Ubuntu）：安装、更新和删除软件包。",
    "url": "/操作系统相关/熟悉哪些linux命令.html",
    "lang": ""
  },
  {
    "title": "Linux中如何查看一个进程，如何杀死一个进程，如何查看某个端口有没有被占用",
    "content": "--- title: Linux中如何查看一个进程，如何杀死一个进程，如何查看某个端口有没有被占用 --- 查看进程： 用 ps 命令查看当前运行的进程，比如 ps aux或者ps -elf 可以列出所有进程及其详细信息。 杀死进程： 首先用 ps 或 top 命令找到进程的PID（进程ID）。 然后用 kill 命令加上进程ID来结束进程，例如 kill -9 PID。\"-9\" 是强制杀死进程的信号。 查看端口占用： 使用 lsof -i：端口号 可以查看占用特定端口的进程。 或者用 netstat -tulnp | grep 端口号，这会显示监听在该端口的服务及其进程ID。",
    "url": "/操作系统相关/Linux中如何查看一个进程，如何杀死一个进程，如何查看某个端口有没有被占用.html",
    "lang": ""
  },
  {
    "title": "说一下 select、poll、epoll",
    "content": "--- title: 说一下 select、poll、epoll --- I/O多路复用通常通过select、poll、epoll等系统调用来实现。 select： select是一个最古老的I/O多路复用机制，它可以监视多个文件描述符的可读、可写和错误状态。然而，但是它的效率可能随着监视的文件描述符数量的增加而降低。因为select使用fdset位图来进行监听，将需要监听的文件描述符放入这个fdset中，让内核来检查是否有⽹络事件产⽣，检查的⽅式很粗暴，就是通过遍历⽂件描述符集合的⽅式，当检查到有事件产⽣后，将此 Socket 标记为可读或可写， 接着再把整个⽂件描述符集合拷⻉回⽤户态⾥，然后⽤户态还需要再通过遍历的⽅法（每个判断if ISSET）找到可读或可写的 Socket，然后再对其处理。所以，对于 select 这种⽅式，需要进⾏ 2 次「遍历」⽂件描述符集合，⼀次是在内核态⾥，⼀次是在⽤户态⾥ ，⽽且还会发⽣ 2 次「拷⻉」⽂件描述符集合，先从⽤户空间传⼊内核空间，由内核修改后，再传出到⽤户空间中。fdset的固定大小是1024，所以监听的数量有限，并且监听和就绪是耦合的。 poll： poll是select的一种改进，它使用轮询方式来检查多个文件描述符的状态，避免了select中文件描述符数量有限的问题，不再使用位图，而是使用动态数组，以链表的形式来组织。监听和返回的集合也进行了分离，但对于大量的文件描述符，poll的性能也可能变得不足够高效，因为需要全部遍历所有的文件描述符。 poll比起select，改进了文件描述数量有限制的问题，使用链表来组织文件描述符，并且对监听和返回的集合也进行了分离。但是还是需要遍历全部的文件描述符检查可读可写。 epoll： epoll是Linux特有的I/O多路复用机制，相较于select和poll，它在处理大量文件描述符时更加高效。epoll使用事件通知的方式，只有在文件描述符就绪时才会通知应用程序，而不需要轮询。并且监听集合采用红黑树，不用像select那样每次操作都传入整个集合，而是传入检测的fd就行，大小无限制，并且监听和就绪分离，只需遍历就绪集合即可，不用遍历所有的文件描述符。 epoll 使⽤事件驱动的机制，内核⾥维护了⼀个链表来记录就绪事件，当某个socket 有事件发⽣时，通过回调函数内核会将其加⼊到这个就绪事件链表中，当⽤户调⽤epollwait() 函数时，只会返回有事件发⽣的⽂件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，⼤⼤提⾼了检测的效率。 !image-20240920171041625 select/poll select 实现多路复⽤的⽅式是，将已连接的 Socket 都放到⼀个⽂件描述符集合，然后调⽤select 函数将⽂件描述符集合拷⻉到内核⾥，让内核来检查是否有⽹络事件产⽣，检查的⽅式很粗暴，就是通过遍历⽂件描述符集合的⽅式，当检查到有事件产⽣后，将此 Socket 标记为可读或可写， 接着再把整个⽂件描述符集合拷⻉回⽤户态⾥，然后⽤户态还需要再通过遍历的⽅法找到可读或可写的 Socket，然后再对其处理。所以，对于 select 这种⽅式，需要进⾏ 2 次「遍历」⽂件描述符集合，⼀次是在内核态⾥，⼀个次是在⽤户态⾥ ，⽽且还会发⽣ 2 次「拷⻉」⽂件描述符集合，先从⽤户空间传⼊内核空间，由内核修改后，再传出到⽤户空间中。 select 使⽤固定⻓度的 BitsMap，表示⽂件描述符集合，⽽且所⽀持的⽂件描述符的个数是有限制的，在 Linux 系统中，由内核中的 FDSETSIZE 限制， 默认最⼤值为 1024 ，只能监听 01023 的⽂件描述符。 poll 不再⽤ BitsMap 来存储所关注的⽂件描述符，取⽽代之⽤动态数组，以链表形式来组织，突破了 select 的⽂件描述符个数限制，当然还会受到系统⽂件描述符限制。但是 poll 和 select 并没有太⼤的本质区别， 都是使⽤「线性结构」存储进程关注的 Socket集合，因此都需要遍历⽂件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，⽽且也需要在⽤户态与内核态之间拷⻉⽂件描述符集合，这种⽅式随着并发数上来，性能的损耗会呈指数级增⻓。",
    "url": "/操作系统相关/说一下 select、poll、epoll.html",
    "lang": ""
  },
  {
    "title": "同步和异步",
    "content": "--- title: 同步和异步 --- 同步 就是指一个进程在执行某个请求的时候，若该请求需要一段时间才能返回信息，那么这个进程将会一直等待下去，直到收到返回信息才继续执行下去。用户态主动去内核态获取数据 异步是 指进程不需要一直等下去，而是继续执行下面的操作，不管其他进程的状态。当有消息返回时系统会通知进程进行处理，这样可以提高执行的效率。内核主动将数据放到了用户态指定的空间，之后通知用户态去处理数据（利用回调） 阻塞和非阻塞 强调的是程序在等待调用结果时的状态. 阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。 对于同步调用来说，很多时候当前线程还是激活的状态，只是从逻辑上当前函数没有返回而已，即同步等待时什么都不干，白白占用着资源。 同步和异步强调的是消息通信机制 (synchronous communication/ asynchronous communication)。所谓同步，就是在发出一个\"调用\"时，在没有得到结果之前，该“调用”就不返回。但是一旦调用返回，就得到返回值了。换句话说，就是由“调用者”主动等待这个“调用”的结果。而异步则是相反，\"调用\"在发出之后，这个调用就直接返回了，所以没有返回结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。而是在\"调用\"发出后，\"被调用者\"通过状态、通知来通知调用者，或通过回调函数处理这个调用",
    "url": "/操作系统相关/同步和异步.html",
    "lang": ""
  },
  {
    "title": "线程池的优缺点",
    "content": "--- title: 线程池的优缺点 --- 空间换时间 首先频繁的创建，删除线程，会带来一定的性能损耗。 所以我们为了降低资源损耗，可以复用已经创建的线程，来降低线程创建销毁带来的消耗，当接到任务的时候，避免了原来创建线程需要消耗的时间。 提高了线程的可管理性。增加系统稳定性。",
    "url": "/操作系统相关/线程池的优缺点.html",
    "lang": ""
  },
  {
    "title": "异步和回调区别",
    "content": "--- title: 异步和回调区别 --- 回调是一种编程思想编程方式，而异步和同步都作为一种消息通信机制。 回调既可以是同步也可以是异步的 同步回调：把函数b传递给函数a。执行a的时候，回调了b，a要一直等到b执行完才能继续执行； 异步回调：把函数b传递给函数a。执行a的时候，回调了b，然后a就继续往后执行，b独自执行。",
    "url": "/操作系统相关/异步和回调区别.html",
    "lang": ""
  },
  {
    "title": "epoll的水平触发和边缘触发",
    "content": "--- title: epoll的水平触发和边缘触发 --- epollwait的就绪触发有两种方式：一种是默认的水平触发方式(Level-triggered)，另一种是边缘触发模式(Edge-triggered)。以读事件为例子：水平触发模式下，只要缓冲区当中存在数据，就可以使epollwait就绪；在边缘触发的情况下，如果缓冲区中存在数据，但是数据一直没有增多，那么epollwait就不会就绪，只有缓冲区的数据增多的时候，才能使epollwait就绪。 使用水平触发的话，线程能够以更短的响应时间来处理事件，但是这可能会导致饥饿问题，如果存在某个事件传输的数据量过大，那么epollwait就会多次就绪直到处理完所有数据为止，而一些其他的任务所占用的资源就会相对变少。使用边缘触发可以避免这个问题。为了确保读操作可以将所有数据读完，可以考虑使用循环配合非阻塞的形式来处理。 在线程池架构中，主线程通常会将实际的IO交给子线程即工作线程完成，采用边缘触发可以有效地降低主线程的响应频率，提高整体的性能。除此以外，如果一次请求对应一次响应是用户追求的通信模式，那么边缘触发正好符合。 水平触发优缺点： 优点：当进行socket通信的时候，保证了数据的完整输出，进行IO操作的时候，如果还有数据，就会一直的通知你。 缺点：由于只要还有数据，内核就会不停的从内核空间转到用户空间，占用了大量内核资源，试想一下当有大量数据到来的时候，每次读取一个字节，这样就会不停的进行切换。内核资源的浪费严重。效率来讲也是很低的。 边缘触发优缺点： 优点：每次内核只会通知一次，大大减少了内核资源的浪费，提高效率。 缺点：不能保证数据的完整。不能及时的取出所有的数据。 应用场景：处理大数据。使用non-block模式的socket。",
    "url": "/操作系统相关/epoll的水平触发和边缘触发.html",
    "lang": ""
  },
  {
    "title": "SO_RCVLOWAT和SO_SNDLOWAT",
    "content": "--- title: SORCVLOWAT和SOSNDLOWAT --- 使用函数setsocketopt可以调整套接字的属性 SORCVBUF和SOSNDBUF：用来获取和调整接收/发送缓冲区的大小。注意到 setsockopt之后再getsockopt的结果会和之前传入的参数不一致。 SORCVLOWAT和SOSNDLOWAT：这个参数说明一个缓冲区的下限，如果缓冲区的字节少于下限，那么数据就不会从套接字中传递给内核协议栈或者发送给用户。这样修改了之后，发送方的数据量如果比较少，将不会触发epollwait的读就绪",
    "url": "/操作系统相关/SO_RCVLOWAT和SO_SNDLOWAT.html",
    "lang": ""
  },
  {
    "title": "非阻塞IO出现EAGAIN",
    "content": "--- title: 非阻塞IO出现EAGAIN --- 操作系统为用户提供非阻塞版本的read：当读取内核缓冲区数据时，如果没有数据，read会直接返回-1，并将errno的数值设置为EAGAIN。非阻塞操作通常会配合循环一起使用以实现一种同步非阻塞的IO模型。 利用fcntl可以改变描述符的当前状态，设置非阻塞 recv的最后一个参数标志可以在不修改已连接套接字的文件属性的情况下，把单个IO操作临时指定为非阻塞，随后执行IO操作，最后关闭非阻塞标志。",
    "url": "/操作系统相关/非阻塞IO出现EAGAIN.html",
    "lang": ""
  },
  {
    "title": "零拷贝、sendfile",
    "content": "--- title: 零拷贝、sendfile --- 传输文件的时候是采用read和send来组合完成，这种当中的数据流向是怎么样的 呢？首先打开一个普通文件，数据会从磁盘通过DMA设备传输到内存，即文件对象当中的内核缓冲区部分，然后调用read数据会从内核缓冲区拷贝到一个用户态的buf上面（buf是read函数的参数），接下来调用send，就将数据拷贝到了网络发送缓存区，最终实现了文件传输。 !image-20240918114313022 !image-20240921233433225 如何减少从内核文件缓冲区到用户态空间的拷贝呢？解决方案就是使用mmap系统调用直接建立文件和用户态空间buf的映射。这样的话数据就减少了一次拷贝。在非常多的场景下都会使用mmap来减少拷贝次数，典型的就是使用图形的应用去操作显卡设备的显存。 使用mmap系统调用只能减少数据从磁盘文件的文件对象到用户态空间的拷贝，但是依然无法避免从用户态到内核已连接套接字的拷贝（因为网络设备文件对象不支持mmap） !image-20240920163129402 sendfile系统调用可以解决这个问题，它可以使数据直接在内核中传递而不需要经过用户态空间，调用sendfile系统调用可以直接将磁盘文件的文件对象的数据直接传递给已连接套接字文件对象，从而直接发送到网卡设备之上（在内核的底层实现中，实际上是让内核磁盘文件缓冲区和网络缓冲区对应同一片物理内存） c++ ssizet sendfile(int outfd, int infd, offt offset, sizet count); !image-20240921233617352 使用sendfile的时候要特别注意，outfd一般只能填写网络套接字的描述符，表示写入的文件描述符，infd一般是一个磁盘文件，表示读取的文件描述符。从上述的需求可以得知，sendfile只能用于发送文件方的零拷贝实现，无法用于接收方，并且发送文件的大小上限是2GB。",
    "url": "/操作系统相关/零拷贝、sendfile.html",
    "lang": ""
  },
  {
    "title": "进程的挂起状态和阻塞状态",
    "content": "--- title: 进程的挂起状态和阻塞状态 --- 阻塞状态（Blocked）：该进程正在等待某⼀事件发⽣（如等待输⼊/输出操作的完成）⽽暂时停⽌运⾏，这时，即使给它CPU控制权，它也⽆法运⾏； 如果有⼤量处于阻塞状态的进程，进程可能会占⽤着物理内存空间，显然不是我们所希望的，毕竟物理内存空间是有限的，被阻塞状态的进程占⽤着物理内存就⼀种浪费物理内存的行为。 所以，在虚拟内存管理的操作系统中，通常会把阻塞状态的进程的物理内存空间换出到硬盘，等需要再次运⾏的时候，再从硬盘换⼊到物理内存。 那么，就需要⼀个新的状态，来描述进程没有占⽤实际的物理内存空间的情况，这个状态就是挂起状态。这跟阻塞状态是不⼀样，阻塞状态是等待某个事件的返回。 另外，挂起状态可以分为两种： 阻塞挂起状态：进程在外存（硬盘）并等待某个事件的出现； 就绪挂起状态：进程在外存（硬盘），但只要进⼊内存，即刻⽴刻运⾏；",
    "url": "/操作系统相关/进程的挂起状态和阻塞状态.html",
    "lang": ""
  },
  {
    "title": "带缓冲IO和不带缓冲IO",
    "content": "--- title: 带缓冲IO和不带缓冲IO --- linux对IO文件的操作分为不带缓冲的IO操作和标准IO操作（即带缓冲），刚开始，要明确以下几点： 1：不带缓冲，不是直接对磁盘文件进行读取操作,像read()和write()函数，它们都属于系统调用，只不过在用户态没有缓冲，所以叫做无缓冲IO,但对于内核来说，还是进行了缓存，只是用户层看不到罢了。如果这一点看不懂，请看第二点； 2：带不带缓冲是相对来说的，如果你要写入数据到文件上时（就是写入磁盘上），内核先将数据写入到内核中所设的缓冲储存器，假如这个缓冲储存器的长度是100个字节，你调用系统函数：ssizet write (int fd,const void buf,sizet count); 写操作时，设每次写入长度count=10个字节，那么你几要调用10次这个函数才能把这个缓冲区写满，此时数据还是在缓冲区，并没有写入到磁盘，缓冲区满时才进行实际上的IO操作，把数据写入到磁盘上，所以上面说的“不带缓冲不是就没有缓冲直写进磁盘”就是这个意思。 带缓冲IO也叫标准IO，符合ANSI C 的标准IO处理，不依赖系统内核，所以移植性强，我们使用标准IO操作很多时候是为了减少对read()和write()的系统调用次数，带缓冲IO其实就是在用户态再建立一个缓冲区，这个缓冲区的分配和优化长度等细节都是标准IO库代你处理好了，不用去操心。 无缓冲IO操作数据流向路径：数据——内核缓冲区——磁盘 标准IO操作数据流向路径：数据——流缓冲区——内核缓冲区——磁盘 buffered I/O就是通过尽可能的少使用系统调用来提高效率的.",
    "url": "/操作系统相关/带缓冲IO和不带缓冲IO.html",
    "lang": ""
  },
  {
    "title": "阻塞与⾮阻塞 IO VS 同步与异步 IO",
    "content": "--- title: 阻塞与⾮阻塞 IO VS 同步与异步 IO --- 先来看看阻塞 I/O，当⽤户程序执⾏ read ，线程会被阻塞，⼀直等到内核数据准备好，并把数据从内核缓冲区拷⻉到应⽤程序的缓冲区中，当拷⻉过程完成， read 才会返回。 注意， 阻塞等待的是「内核数据准备好」和「数据从内核态拷⻉到⽤户态」这两个过程。过程如下图： !image-20240920162026820 知道了阻塞 I/O ，来看看⾮阻塞 I/O，⾮阻塞的 read 请求在数据未准备好的情况下⽴即返回，可以继续往下执⾏，此时应⽤程序不断轮询内核，直到数据准备好，内核将数据拷⻉到应⽤程序缓冲区， read 调⽤才可以获取到结果。过程如下图： !image-20240920162052381 注意， ==这⾥最后⼀次 read 调⽤，获取数据的过程，是⼀个同步的过程，是需要等待的过程。这⾥的同步指的是内核态的数据拷⻉到⽤户程序的缓存区这个过程== 应⽤程序每次轮询内核的 I/O 是否准备好，感觉有点傻乎乎，因为轮询的过程中，应⽤程序啥也做不了，只是在循环。 I/O 多路复⽤技术就出来了，如 select、 poll，它是通过I/O 事件分发，当内核数据准备好时，再以事件通知应⽤程序进⾏操作 !image-20240920162309618 实际上，==⽆论是阻塞 I/O、⾮阻塞 I/O，还是基于⾮阻塞 I/O 的多路复⽤都是同步调⽤。因为它们在 read 调⽤时，内核将数据从内核空间拷⻉到应⽤程序空间，过程都是需要等待的，也就是说这个过程是同步的，如果内核实现的拷⻉效率不⾼， read 调⽤就会在这个同步过程中等待⽐较⻓的时间。== 当我们发起 aioread 之后，就⽴即返回，内核⾃动将数据从内核空间拷⻉到应⽤程序空间，这个拷⻉过程同样是异步的，内核⾃动完成的，和前⾯的同步操作不⼀样，应⽤程序并不需要主动发起拷⻉动作。过程如下图： !image-20240920162427446 在前⾯我们知道了， I/O 是分为两个过程的： 数据准备的过程 数据从内核空间拷⻉到⽤户进程缓冲区的过程 阻塞 I/O 会阻塞在「过程 1 」和「过程 2」，⽽⾮阻塞 I/O 和基于⾮阻塞 I/O 的多路复⽤只会阻塞在「过程 2」，所以这三个都可以认为是同步 I/O。 异步 I/O 则不同，「过程 1 」和「过程 2 」都不会阻塞。",
    "url": "/操作系统相关/阻塞与⾮阻塞 IO VS 同步与异步 IO.html",
    "lang": ""
  },
  {
    "title": "Reactor 和 Proactor",
    "content": "--- title: Reactor 和 Proactor --- reactor模式是基于I/O多路复用和面向对象的思想的。它是一种事件驱动架构，把线程和连接进行解耦，线程只用来执行事件注册的回调函数。事件驱动架构由事件生产者和事件消费者组成。采用的是同步加非阻塞，epoll监听的是可读可写事件。 Reactor模式分为两个重要组成部分，Reactor和Handler。 Reactor(反应器)：循环监听就绪IO事件，并分发给回调函数。 Handler(回调函数)：执行对应IO事件的实际业务逻辑。 reactor模式下可以分为单线程模型，多线程模型，以及主从reactor模型。 在单线程模型中：IO事件的轮询以及accept以及IO事件执行都是在一个线程中执行的。所以说都是一个任何操作都是同步的过程。效率不高。 在多线程模型中：比起单线程模型会多加入一个业务线程池。将非IO操作也就是业务逻辑处理交给业务线程池来做，这样提高reactor的IO响应效率。但是所有IO操作还是在一个reactor单线程中完成的，在高并发场景下，效率不好。 在主从reactor模型中：为了充分利用多核cpu，将reactor拆分成了主reactor和从reactor。主reactor用来accept，将新连接交给从reactor。从reactor就负责这个连接的后续IO事件，并将对应的业务逻辑操作交给线程池来处理。 !image-20240920172146268 第⼀个缺点，因为只有⼀个进程， ⽆法充分利⽤ 多核 CPU 的性能； 第⼆个缺点， Handler 对象在业务处理时，整个进程是⽆法处理其他连接的事件的， 如果业务处理耗时⽐较⻓，那么就造成响应的延迟； 所以，单 Reactor 单进程的⽅案不适⽤计算机密集型的场景，只适⽤于业务处理⾮常快速的场景。 Redis 是由 C 语⾔实现的，它采⽤的正是「单 Reactor 单进程」的⽅案，因为 Redis 业务处理主要是在内存中完成，操作的速度是很快的，性能瓶颈不在 CPU 上，所以 Redis 对于命令的处理是单进程的⽅案。 如果要克服「单 Reactor 单线程 / 进程」⽅案的缺点，那么就需要引⼊多线程 / 多进程，这样就产⽣了单 Reactor 多线程 / 多进程的⽅案。 !image-20240920172456454 Reactor 对象通过 select （IO 多路复⽤接⼝） 监听事件，收到事件后通过 dispatch 进⾏分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型； 如果是连接建⽴的事件，则交由 Acceptor 对象进⾏处理， Acceptor 对象会通过 accept⽅法 获取连接，并创建⼀个 Handler 对象来处理后续的响应事件； 如果不是连接建⽴事件， 则交由当前连接对应的 Handler 对象来进⾏响应； 上⾯的三个步骤和单 Reactor 单线程⽅案是⼀样的，接下来的步骤就开始不⼀样了： Handler 对象不再负责业务处理，只负责数据的接收和发送， Handler 对象通过 read 读取到数据后，会将数据发给⼦线程⾥的 Processor 对象进⾏业务处理； ⼦线程⾥的 Processor 对象就进⾏业务处理，处理完后，将结果发给主线程中的 Handler对象，接着由 Handler 通过 send ⽅法将响应结果发送给 client； 单 Reator 多线程的⽅案优势在于能够充分利⽤多核 CPU 的能，那既然引⼊多线程，那么⾃然就带来了多线程竞争资源的问题。 另外，「单 Reactor」的模式还有个问题， 因为⼀个 Reactor 对象承担所有事件的监听和响应，⽽且只在主线程中运⾏，在⾯对瞬间⾼并发的场景时，容易成为性能的瓶颈的地⽅ 要解决「单 Reactor」的问题，就是将「单 Reactor」实现成「多 Reactor」，这样就产⽣了多 Reactor 多进程 / 线程的⽅案。 !image-20240920173022405 Proactor 正是采⽤了异步 I/O 技术，所以被称为异步⽹络模型。 现在我们再来理解 Reactor 和 Proactor 的区别，就⽐较清晰了。 Reactor 是⾮阻塞同步⽹络模式，感知的是就绪可读写事件。在每次 感知到有事件发⽣（⽐如可读就绪事件）后，就需要应⽤进程主动调⽤ read ⽅法来完成数据的读取，也就是要应⽤进程主动将 socket 接收缓存中的数据读到应⽤进程内存中，这个过程是同步的，读取完数据后应⽤进程才能处理数据。 Proactor 是异步⽹络模式， 感知的是已完成的读写事件。在发起异步读写请求时，需要传⼊数据缓冲区的地址（⽤来存放结果数据）等信息，这样系统内核才可以⾃动帮我们把数据的读写⼯作完成，这⾥的读写⼯作全程由操作系统来做，并不需要像 Reactor 那样还需要应⽤进程主动发起 read/write 来读写数据，操作系统完成读写⼯作后，就会通知应⽤进程直接处理数据。 ⽆论是 Reactor，还是 Proactor，都是⼀种基于「事件分发」的⽹络编程模式，区别在于 Reactor 模式是基于「待完成」的 I/O 事件，⽽ Proactor 模式则是基于「已完成」的 I/O 事件。",
    "url": "/操作系统相关/Reactor 和 Proactor.html",
    "lang": ""
  },
  {
    "title": "上下文切换有什么开销",
    "content": "--- title: 上下文切换有什么开销 --- 在多线程编程中，上下文切换（Context Switch）是指操作系统内核在不同线程或进程之间切换执行时所发生的一系列操作。虽然多线程可以提高程序的并发性和资源利用率，但频繁的上下文切换会引起一定的 CPU 开销。具体的开销主要包括以下几个方面： 当操作系统决定将 CPU 从一个线程切换到另一个线程时，它需要保存当前线程的执行状态（上下文），包括寄存器内容、程序计数器和栈指针等。这些信息需要被存储到线程的控制块中，以便将来能够恢复。 保存上下文：将当前线程的状态数据保存到内存中。 恢复上下文：从内存中将下一个线程的状态数据加载到寄存器中。 这种保存和恢复操作会消耗 CPU 的时间。 在切换线程的过程中，操作系统可能需要更新相关的内存映射和页表。这涉及到对虚拟内存和物理内存的管理，可能会导致额外的内存访问开销。 上下文切换可能导致 CPU 缓存（如 L1、L2 缓存）中的数据失效。当线程切换到一个新的线程时，新的线程可能会使用不同的数据集，这意味着之前缓存的数据可能不再有效。因此，CPU 可能需要从主内存中重新加载数据，这会显著增加内存访问延迟。 操作系统内核负责决定哪个线程在何时运行。这种调度的决策过程需要一定的计算资源，包括选择合适的调度算法、维护线程的优先级、管理线程的状态等。这也会增加上下文切换的开销。 频繁的上下文切换会导致上述开销的累积，从而影响系统的整体性能。上下文切换的频率过高可能会导致应用程序性能下降，甚至形成“上下文切换风暴”，即由于频繁切换导致 CPU 资源的浪费，反而使得应用程序运行效率降低。 上下文切换引起的 CPU 开销主要涉及状态保存与恢复、内存管理、缓存失效、调度成本等多个方面。为了提高系统性能，尽量减少上下文切换的频率是优化多线程应用的重要措施之一。这可以通过合理设计线程数、优化线程的生命周期管理等方式来实现。",
    "url": "/操作系统相关/上下文切换有什么开销.html",
    "lang": ""
  },
  {
    "title": "惊群",
    "content": "--- title: 惊群 --- listen之后 fork，那么其他进程都会监听这个端口 缺点：其他进程被无效唤醒了，CPU上下文频繁切换，由阻塞到被唤醒。 如何解决？ 1、加锁 2、如何加锁？ 3、准备一个共享内存，进程共享这块区域，内部定义一个变量（0或者1），先判断如果为0那么再将这个fd加入到epoll中，再将变量置为1。（CAS原子操作最佳，也可以上互斥锁，自旋锁） 当多个线程使用同一个epoll实例监听同一组文件描述符时，一个文件描述符触发事件后，所有阻塞在epollwait的线程都可能被唤醒，但最终只有一个线程能够处理该事件。 解决这个特定的epoll惊群问题有几种有效方法： EPOLLONESHOT 标志： 当使用EPOLLONESHOT标志注册一个文件描述符时，一旦该描述符上的事件被触发并通知给某个线程后，epoll会自动将其从监听集合中移除，直到你重新添加它。 这确保了每个事件只通知一个线程，避免了多个线程竞争同一个事件。 c复制epollctl(epfd, EPOLLCTLADD, fd, &ev); ev.events = EPOLLIN | EPOLLONESHOT; ==EPOLLEXCLUSIVE 标志==（Linux 4.5+）： 这是专门为解决惊群问题设计的标志。 使用该标志注册的描述符，当事件发生时，内核会尝试只唤醒一个等待的线程。 c复制ev.events = EPOLLIN | EPOLLEXCLUSIVE; epollctl(epfd, EPOLLCTLADD, fd, &ev); 实现工作池设计： 使用单独的线程来监听epoll事件，然后将就绪的事件分发给工作线程池中的线程处理。 这种方式只有一个线程被epollwait唤醒，然后它负责分配工作。 每个线程使用独立的epoll实例： 每个线程创建和使用自己的epoll实例，并且监控不同的文件描述符集合。 这完全避免了共享epoll实例带来的惊群问题。 在现代系统中，EPOLLEXCLUSIVE是解决这个问题最直接和有效的方法，它直接在内核层面解决了惊群问题，性能开销最小。",
    "url": "/操作系统相关/惊群.html",
    "lang": ""
  },
  {
    "title": "红黑树",
    "content": "--- title: 红黑树 --- 红黑树是一个二叉搜索树，它在每个节点增加了一个存储位记录节点的颜色，可以是RED,也可以是BLACK；通过任意一条从根到叶子简单路径上颜色的约束，红黑树保证最长路径不超过最短路径的二倍，因而近似平衡（最短路径就是全黑节点，最长路径就是一个红节点一个黑节点，当从根节点到叶子节点的路径上黑色节点相同时，最长路径刚好是最短路径的两倍）。它同时满足以下特性： 节点是红色或黑色 根是黑色 叶子节点（外部节点，空节点）都是黑色，这里的叶子节点指的是最底层的空节点（外部节点），下图中的那些null节点才是叶子节点，null节点的父节点在红黑树里不将其看作叶子节点 红色节点的子节点都是黑色 红色节点的父节点都是黑色 从根节点到叶子节点的所有路径上不能有 2 个连续的红色节点 从任一节点到叶子节点的所有路径都包含相同数目的黑色节点 !image-20240928223326618 下面这颗树不是红黑树，因为加上叶子节点(null节点)后，不满足性质5 !image-20240928223427012 AVL是靠平衡因子来保持平衡的，比如平衡因子为1，那么左右子树的高度差就不能超过1，是一种强平衡。 对于红黑树而言，为何那5条性质，就能保证红黑树是平衡的？ ==因为那5条性质，可以保证红黑树等价于4阶B树== B树比较矮，它本身就是平衡的，高度越小越平衡。 红黑树就是能保证这个树高度不会特别高，红黑树的最大高度是 2 ∗ log2(n + 1) ，依然是 O(logn) 级别，因为高度不会很大进而维持一种相对平衡的状态。相比AVL树，红黑树的平衡标准比较宽松：没有一条路径会大于其他路径的2倍。这是是一种弱平衡、黑高度平衡（黑高度只算黑色节点个数，红黑树的任何一条路径的黑色节点数一样，则黑高度都是一样）。 9.1 AVL树 平衡标准比较严格：每个左右子树的高度差不超过1 最大高度是 1.44 ∗ log2 n + 2 − 1.328（100W个节点，AVL树最大树高28） 搜索、添加、删除都是 O(logn) 复杂度，其中添加仅需 O(1) 次旋转调整、删除最多需要 O(logn) 次旋转调整 9.2 红黑树 平衡标准比较宽松：没有一条路径会大于其他路径的2倍 最大高度是 2 ∗ log2(n + 1)（ 100W个节点，红黑树最大树高40） 搜索、添加、删除都是 O(logn) 复杂度，其中添加、删除都仅需 O(1) 次旋转调整 9.3 如何选择 搜索的次数远远大于插入和删除，选择AVL树； 搜索、插入、删除次数几乎差不多，选择红黑树 相对于AVL树来说，红黑树牺牲了部分平衡性以换取插入/删除操作时少量的旋转操作，整体来说性能要优于AVL树 红黑树的平均统计性能优于AVL树，实际应用中更多选择使用红黑树",
    "url": "/操作系统相关/红黑树.html",
    "lang": ""
  },
  {
    "title": "LT和ET模式",
    "content": "--- title: LT和ET模式 --- ET本身并不会造成饥饿，由于事件只通知一次，开发者一不小心就容易遗漏了待处理的数据，像是饥饿，实质是bug 使用ET模式，特定场景下会比LT更快，因为它可以便捷的处理EPOLLOUT事件，省去打开与关闭EPOLLOUT的epollctl（EPOLLCTLMOD）调用。从而有可能让你的性能得到一定的提升。 例如你需要写出1M的数据，写出到socket 256k时，返回了EAGAIN，ET模式下，当再次epoll返回EPOLLOUT事件时，继续写出待写出的数据，当没有数据需要写出时，不处理直接略过即可。而LT模式则需要先打开EPOLLOUT，当没有数据需要写出时，再关闭EPOLLOUT（否则会一直返回EPOLLOUT事件） 总体来说，ET处理EPOLLOUT方便高效些，LT不容易遗漏事件、不易产生bug 如果server的响应通常较小，不会触发EPOLLOUT，那么适合使用LT，例如redis等。而nginx作为高性能的通用服务器，网络流量可以跑满达到1G，这种情况下很容易触发EPOLLOUT，则使用ET。 关于某些场景下ET模式比LT模式效率更好，我有篇文章进行了详细的解释与测试，参看 epoll LT/ET 深入剖析 这里有两个例子，分别演示了LT与ET两种工作模式 handy/epoll-et.cc at master · yedf/handy · GitHubhandy/epoll.cc at master · yedf/handy · GitHub EPOLL事件有两种模型： Level Triggered (LT) 水平触发 .socket接收缓冲区不为空 有数据可读 读事件一直触发 .socket发送缓冲区不满 可以继续写入数据 写事件一直触发 符合思维习惯，epollwait返回的事件就是socket的状态 Edge Triggered (ET) 边沿触发 .socket的接收缓冲区状态变化时触发读事件，即空的接收缓冲区刚接收到数据时触发读事件 .socket的发送缓冲区状态变化时触发写事件，即满的缓冲区刚空出空间时触发读事件 仅在状态变化时触发事件 ET还是LT? LT的处理过程： . accept一个连接，添加到epoll中监听EPOLLIN事件 . 当EPOLLIN事件到达时，read fd中的数据并处理 . 当需要写出数据时，把数据write到fd中；如果数据较大，无法一次性写出，那么在epoll中监听EPOLLOUT事件 . 当EPOLLOUT事件到达时，继续把数据write到fd中；如果数据写出完毕，那么在epoll中关闭EPOLLOUT事件 ET的处理过程： . accept一个一个连接，添加到epoll中监听EPOLLIN|EPOLLOUT事件 . 当EPOLLIN事件到达时，read fd中的数据并处理，read需要一直读，直到返回EAGAIN为止 . 当需要写出数据时，把数据write到fd中，直到数据全部写完，或者write返回EAGAIN . 当EPOLLOUT事件到达时，继续把数据write到fd中，直到数据全部写完，或者write返回EAGAIN 从ET的处理过程中可以看到，ET的要求是需要一直读写，直到返回EAGAIN，否则就会遗漏事件。而LT的处理过程中，直到返回EAGAIN不是硬性要求，但通常的处理过程都会读写直到返回EAGAIN，==但LT比ET多了一个开关EPOLLOUT事件的步骤== LT的编程与poll/select接近，符合一直以来的习惯，不易出错 ET的编程可以做到更加简洁，某些场景下更加高效，但另一方面容易遗漏事件，容易产生bug 这里有两个简单的例子演示了LT与ET的用法(其中epoll-et的代码比epoll要少10行)： https://github.com/yedf/handy/blob/master/raw-examples/epoll.cc https://github.com/yedf/handy/blob/master/raw-examples/epoll-et.cc 针对容易触发LT开关EPOLLOUT事件的情景（让服务器返回1M大小的数据），我用ab做了性能测试 测试的结果显示ET的性能稍好，详情如下： LT 启动命令 ./epoll a ET 启动命令 ./epoll-et a ab 命令：ab -n 1000 -k 127.0.0.1/ LT 结果：Requests per second: 42.56 [#/sec] (mean) ET 结果：Requests per second: 48.55 [#/sec] (mean) 当我把服务器返回的数据大小改为48576时，开关EPOLLOUT更加频繁，性能的差异更大 ab 命令：ab -n 5000 -k 127.0.0.1/ LT 结果：Requests per second: 745.30 [#/sec] (mean) ET 结果：Requests per second: 927.56 [#/sec] (mean) 对于nginx这种高性能服务器，ET模式是很好的，而其他的通用网络库，更多是使用LT，避免使用的过程中出现bug",
    "url": "/操作系统相关/LT和ET模式.html",
    "lang": ""
  },
  {
    "title": "Unix域套接字",
    "content": "--- title: Unix域套接字 --- Unix域套接字只能用于在同一个计算机的进程间进行通信。虽然网络套接字也可以用于单机进程间的通信，但是使用Unix域套接字效率会更高，因为Unix域套接字仅仅进行数据复制，不会执行在网络协议栈中需要处理的添加、删除报文头、计算校验和、计算报文顺序等复杂操作，因而在单机的进程间通信中，更加推荐使用Unix域套接字。",
    "url": "/操作系统相关/Unix域套接字.html",
    "lang": ""
  },
  {
    "title": "协程相关",
    "content": "--- title: 协程相关 --- 无栈协程（stackless coroutine）和有栈协程（stackful coroutine）在实现机制上有显著的区别。无栈协程的上下文存储方式和切换机制与传统的函数调用和有栈协程不同。以下是对无栈协程上下文存储和切换机制的详细解释： 无栈协程的上下文通常存储在堆上或全局数据结构中，而不是调用栈上。这是因为无栈协程不维护独立的调用栈，而是依赖于状态机的实现方式。具体来说，无栈协程的上下文包括协程的局部变量、当前执行位置和其他必要的信息，这些信息需要在协程暂停和恢复时保持一致。 状态机实现： 无栈协程通常通过将协程的执行状态编码为状态机来实现。每次协程切换时，状态机会更新到下一个状态。状态机的状态和局部变量可以存储在堆上或全局数据结构中，以便在协程暂停和恢复时保持一致。 没有独立的调用栈： 无栈协程没有独立的调用栈来保存调用帧。相反，它们依赖于编译器或解释器的支持，通过状态机的方式实现协程的切换。因此，协程的上下文（包括局部变量和当前执行位置）需要存储在堆上或全局数据结构中，而不是调用栈上。 避免栈帧冲突： 由于无栈协程不维护独立的调用栈，如果协程的上下文存储在调用栈上，当协程暂停时，调用栈上的栈帧可能会被其他函数调用覆盖。因此，需要将协程的上下文存储在堆上或全局数据结构中，以避免栈帧冲突。 无栈协程的切换机制与传统的函数调用和有栈协程不同。无栈协程的切换更像是状态机的状态切换，而不是传统的函数调用的压栈和弹栈。 状态机切换： 无栈协程通过状态机的方式实现协程的切换。每次协程切换时，状态机会更新到下一个状态，并恢复相应的上下文。 上下文保存和恢复： 无栈协程的上下文（包括局部变量和当前执行位置）在协程暂停时保存，在协程恢复时恢复。这些上下文通常存储在堆上或全局数据结构中，而不是调用栈上。 以下是一个 Python 生成器的示例，展示了无栈协程的上下文存储和切换机制： def simplecoroutine(): x = 1 yield x x += 1 yield x coro = simplecoroutine() print(next(coro)) # 输出: 1 print(next(coro)) # 输出: 2 在这个示例中，生成器的状态（变量 x 和当前执行位置）由解释器管理，并存储在生成器对象中。当生成器暂停时，解释器会保存当前的上下文（包括局部变量和当前执行位置），并在生成器恢复时恢复这些上下文。 无栈协程的上下文存储在堆上或全局数据结构中，而不是调用栈上。这是因为无栈协程不维护独立的调用栈，而是依赖于状态机的实现方式。无栈协程的切换机制更像是状态机的状态切换，而不是传统的函数调用的压栈和弹栈。通过将上下文存储在堆上或全局数据结构中，可以避免栈帧冲突，并确保协程在暂停和恢复时保持一致的上下文。",
    "url": "/操作系统相关/协程相关.html",
    "lang": ""
  },
  {
    "title": "eventloop和iocontext区别",
    "content": "--- title: eventloop和iocontext区别 --- 可以将 boost::asio::iocontext 类比 为 Muduo 网络库中的 EventLoop，两者都是事件驱动模型的核心组件，负责管理 I/O 事件、定时器和异步任务。但它们在设计理念、实现细节和使用方式上有显著差异。以下是详细对比： ------ | 组件 | iocontext (Boost.Asio) | EventLoop (Muduo) | | :------------- | :------------------------------- | :------------------------------ | | 事件循环 | ✅ 驱动异步 I/O、定时器、任务队列 | ✅ 监听和分发 I/O 事件、定时器 | | 多线程支持 | ✅ 多线程可同时调用 run() | ✅ 通常单线程运行，但支持多 Loop | | 网络模型 | Proactor 模式（异步操作抽象） | Reactor 模式（事件回调） | | 跨平台性 | ✅ 跨平台（依赖系统异步接口） | ❌ 主要针对 Linux（基于 epoll） | ------ Proactor 模式：异步操作由操作系统或库内部完成，用户通过回调处理结果。 c++ socket.asyncreadsome(buffer, [](errorcode ec, sizet bytes) { // 数据就绪后回调 }); 解耦 I/O 和线程：通过 post() 或 dispatch() 提交任务到事件循环，支持多线程协作。 Reactor 模式：通过 epoll/poll 监听事件，就绪后触发回调。 channel.setReadCallback([](Timestamp t) { // 可读事件触发回调 }); 线程绑定：通常每个 EventLoop 绑定一个线程，遵循 \"one loop per thread\" 模型。 ------ boost::asio::iocontext iocontext; boost::asio::ip::tcp::acceptor acceptor(iocontext, {boost::asio::ip::tcp::v4(), 8080}); void acceptconnection() { auto socket = std::makeshared<boost::asio::ip::tcp::socket>(iocontext); acceptor.asyncaccept(socket, socket { if (!ec) { // 处理新连接 boost::asio::post(iocontext, [socket] { readrequest(socket); }); } acceptconnection(); }); } acceptconnection(); iocontext.run(); // 启动事件循环 EventLoop loop; TcpServer server(&loop, InetAddress(8080), \"EchoServer\"); server.setConnectionCallback([](const TcpConnectionPtr& conn) { if (conn->connected()) { conn->setMessageCallback([](const TcpConnectionPtr& conn, Buffer buf, Timestamp) { conn->send(buf); // 回显数据 }); } }); server.start(); loop.loop(); // 启动事件循环 ------ | 特性 | iocontext | EventLoop | | :--------------- | :------------------------------------ | :------------------------------- | | 事件触发时机 | 操作完成后回调（如数据已接收） | 事件就绪时回调（如 socket 可读） | | 线程安全 | ✅ 可多线程调用 run() | ❌ 通常单线程操作 | | 定时器精度 | 高精度（依赖系统时钟） | 依赖 timerfd（Linux 特有） | | 任务队列 | 内置任务队列（post()/dispatch()） | 需手动实现或结合其他组件 | ------ 相似性：两者都是事件循环的核心，处理 I/O、定时器和任务。 差异性： iocontext 更抽象，隐藏了底层 I/O 模型（如 Windows 使用 IOCP，Linux 使用 epoll）。 EventLoop 更贴近系统调用，直接暴露 epoll 的事件监听机制。 ------ 跨平台需求：优先选 iocontext（Boost.Asio）。 Linux 高性能服务器：Muduo 的 EventLoop 更直观，适合需要精细控制事件分发的场景。 混合使用：在 Muduo 中嵌入 iocontext 处理异步计算任务（如文件操作）。",
    "url": "/操作系统相关/eventloop和iocontext区别.html",
    "lang": ""
  },
  {
    "title": "TCPIP模型和OSI模型的区别",
    "content": "--- title: TCPIP模型和OSI模型的区别 --- OSI模型， 是国际标准化组织（ISO）制定的一个用于计算机或通信系统间互联的标准体系，将计算机网络通信划分为七个不同的层级，每个层级都负责特定的功能。每个层级都构建在其下方的层级之上，并为上方的层级提供服务。七层从下到上分别是物理层、数据链路层、网络层、传输层、会话层、表示层和应用层。虽然OSI模型在理论上更全面，但在实际网络通信中，TCP/IP模型更为实用。 TCP/IP模型分为四个层级，每个层级负责特定的网络功能。 应用层：该层与OSI模型的应用层和表示层以及会话层类似，提供直接与用户应用程序交互的接口。它为网络上的各种应用程序提供服务，如电子邮件（SMTP）、网页浏览（HTTP）、文件传输（FTP）等。 传输层：该层对应OSI模型的传输层。它负责端到端的数据传输，提供可靠的、无连接的数据传输服务。主要的传输层协议有TCP和UDP。TCP提供可靠的数据传输，确保数据的正确性和完整性；而UDP则是无连接的，适用于不要求可靠性的传输，如实时音频和视频流。 网络层：该层对应OSI模型的网络层。主要协议是IP，它负责数据包的路由和转发，选择最佳路径将数据从源主机传输到目标主机。IP协议使用IP地址来标识主机和网络，并进行逻辑地址寻址。 网络接口层：该层对应OSI模型的数据链路层和物理层。它负责物理传输媒介的传输，例如以太网、Wi-Fi等，并提供错误检测和纠正的功能。此外，网络接口层还包含硬件地址（MAC地址）的管理。",
    "url": "/网络相关/TCPIP模型和OSI模型的区别.html",
    "lang": ""
  },
  {
    "title": "从输入 URL 到页面展示到底发生了什么？",
    "content": "--- title: 从输入 URL 到页面展示到底发生了什么？ --- 输入网址，解析URL信息，准备发送HTTP请求 首先检查浏览器缓存里是否有缓存该资源，如果有直接返回；如果没有进入下一步网络请求。 接下来是DNS域名解析：网络请求前，进行DNS解析来获取请求域名的IP地址。DNS解析时会按本地浏览器缓存->本地Host文件->路由器缓存->本地DNS服务器->根DNS服务器的顺序查询域名对应IP，直到找到为止。 找到对应IP就可以TCP三次握手建立连接 客户端发送HTTP请求 服务器处理请求并返回HTTP资源 TCP四次挥手断开连接 浏览器解析响应并渲染页面",
    "url": "/网络相关/从输入 URL 到页面展示到底发生了什么？.html",
    "lang": ""
  },
  {
    "title": "HTTP请求报文和响应报文是怎样的，有哪些常见的字段？",
    "content": "--- title: HTTP请求报文和响应报文是怎样的，有哪些常见的字段？ --- 请求报文： 请求行： [方法] [URI] [HTTP版本]\\r\\n 请求头：Host:服务器域名\\r\\n ​ Content-Type:请求体的媒体类型\\r\\n ​ If-None-Match:资源的Etag值，用于缓存控制\\r\\n 空行\\r\\n 请求体：............................................. ​ HTTP报文分为请求报文和响应报文。 （1） 请求报文 请求报文主要由请求行、请求头、空行、请求体构成。 请求行包括如下字段： 方法（Method）：指定要执行的操作，如 GET、POST、PUT、DELETE 等。 资源路径（Resource Path）：请求的资源的URI（统一资源标识符）。 HTTP版本（HTTP Version）：使用的HTTP协议版本，如 HTTP/1.1 或 HTTP/2.0。 请求头的字段较多，常使用的包含以下几个： Host：请求的服务器的域名。 Accept：客户端能够处理的媒体类型。 Accept-Encoding：客户端能够解码的内容编码。 Authorization：用于认证的凭证信息，比如token数据。 Content-Length：请求体的长度。 Content-Type：请求体的媒体类型。 Cookie：存储在客户端的cookie数据。 If-None-Match：资源的ETag值，用于缓存控制。 Connection：管理连接的选项，如 keep-alive。 空行是请求头部和请求主体之间的空行，用于分隔请求头部和请求主体。而请求体通常用于 POST 和 PUT 请求，包含发送给服务器的数据。 （2) 响应报文 请求报文： 状态行： [HTTP版本] [状态码] [状态消息]\\r\\n 响应头： Expires: 响应的过期时间，之后内容被认为是过时的。\\r\\n ​ ETag: 响应体的实体标签，用于缓存和条件请求。\\r\\n Last-Modified： 资源最后被修改的日期和时间。\\r\\n 空行\\r\\n 响应体：............................................. HTTP响应报文是服务器向客户端返回的数据格式，用于传达服务器对客户端请求的处理结果以及相关的数据。一个标准的HTTP响应报文通常包含状态行、响应头、空行、响应体。 状态行包含HTTP版本、状态码和状态消息。例如：HTTP/1.1 200 OK 响应头部也是以键值对的形式提供的额外信息，类似于请求头部，用于告知客户端有关响应的详细信息。一些常见的响应头部字段包括： Content-Type：指定响应主体的媒体类型。 Content-Length：指定响应主体的长度（字节数）。 Server：指定服务器的信息。 Expires: 响应的过期时间，之后内容被认为是过时的。 ETag: 响应体的实体标签，用于缓存和条件请求。 Last-Modified： 资源最后被修改的日期和时间。 Location：在重定向时指定新的资源位置。 Set-Cookie：在响应中设置Cookie。 Access-Control-Allow-Origin: 跨源资源共享（CORS）策略，指示哪些域可以访问资源。 空行（Empty Line）在响应头和响应体之间，表示响应头的结束。而响应体是服务端实际传输的数据，可以是文本、HTML页面、图片、视频等，也可能为空。 ![](..\\..\\assets\\images\\HTTP请求报文和响应报文.png)",
    "url": "/网络相关/HTTP请求报文和响应报文是怎样的，有哪些常见的字段？.html",
    "lang": ""
  },
  {
    "title": "HTTP有哪些请求方式",
    "content": "--- title: HTTP有哪些请求方式 --- GET：请求指定的资源。（查询） POST：向指定资源提交数据进行处理请求（例如表单提交）。（增加） PUT：更新指定资源。（更改） DELETE：删除指定资源。（删除） HEAD：获取报文首部，不返回报文主体。 OPTIONS：查询服务器支持的请求方法。 PATCH：对资源进行部分更新。",
    "url": "/网络相关/HTTP有哪些请求方式.html",
    "lang": ""
  },
  {
    "title": "GET请求和POST请求的区别",
    "content": "--- title: GET请求和POST请求的区别 --- 语义不同是最大的区别，其次在技术上本质没什么区别，主要还是用合适的方法做合适的事情。 首先是语义用途不同，GET请求通常用于获取数据，POST请求用于提交数据。 然后是数据传输，GET请求将参数附加在URL之后，POST请求将数据放在请求体中。所以说GET请求安全性会更低些。 GET请求会受到浏览器制定的URL长度限制；POST请求理论上没有大小限制。 接着是幂等性，从规范上来说，GET请求是幂等的；POST请求不是幂等的。 最后是缓存，GET请求可以被缓存，POST请求默认不会被缓存。 用途：GET请求通常用于获取数据，POST请求用于提交数据。 数据传输：GET请求将参数附加在URL之后，POST请求将数据放在请求体中。 安全性：GET请求由于参数暴露在URL中，安全性较低；POST请求参数不会暴露在URL中，相对更安全。 数据大小：GET请求受到URL长度限制，数据量有限；POST请求理论上没有大小限制。 幂等性：GET请求是幂等的，即多次执行相同的GET请求，资源的状态不会改变；POST请求不是幂等的，因为每次提交都可能改变资源状态。 缓存：GET请求可以被缓存，POST请求默认不会被缓存。",
    "url": "/网络相关/GET请求和POST请求的区别.html",
    "lang": ""
  },
  {
    "title": "HTTP中常见的状态码有哪些？",
    "content": "--- title: HTTP中常见的状态码有哪些？ --- 1XX代表提示信息，协议处理的中间状态 2XX代表请求成功，其中200表示客户端请求成功，201表示创建了新资源，204表明服务器成功处理了请求，但未返回任何内容。 3XX代表请求重定向，301表示永久重定向，302表示临时重定向，304表示请求的内容没有修改过，所以服务器返回响应时直接使用缓存。 4XX代表请求错误，401表示请求需要身份验证，403表示请求的对应资源禁止被访问，404表示服务器无法找到对应资源 5XX代表服务器错误，500表示服务器内部错误，503表示服务不可用 ![](..\\..\\assets\\images\\HTTP状态码.png) 常用的包括以下几个： 200：表示客户端请求成功。 201：创建了新资源。 204 ：无内容，服务器成功处理请求，但未返回任何内容。 301：永久重定向 302： 临时重定向 304：请求的内容没有修改过，所以服务器返回此响应时，不会返回网页内容，而是使用缓存 401：请求需要身份验证 403：请求的对应资源禁止被访问 404：服务器无法找到对应资源 500：服务器内部错误 502：服务器作为网关或者代理，从上游服务器收到无效响应 503：服务不可用",
    "url": "/网络相关/HTTP中常见的状态码有哪些？.html",
    "lang": ""
  },
  {
    "title": "什么是强缓存和协商缓存",
    "content": "--- title: 什么是强缓存和协商缓存 --- 强缓存是根据Expire字段和Cache-Control字段实现的，一个是绝对时间，一个是相对时间。当强缓存失效时呢，就会发起协商缓存请求。 协商缓存有两种实现，一种是基于Last-Modified来判断，也就是根据最近修改时间,但是这种如果资源本身并没有修改，但是修改时间还是可能有变化，这样导致缓存失效。并且如果修改耗时极短，导致修改时间也没变化，但是这个时候资源本身已经改变了。另一种是根据If-None-Match字段和Etag来判断的，判断E-tag是否有变化，E-tag是服务器为资源生成的唯一标识符。根据文件内容计算出的哈希值，服务端将其和资源一起放回给客户端。 强缓存和协商缓存是HTTP缓存机制的两种类型，它们用于减少服务器的负担和提高网页加载速度。 强缓存：客户端在没有向服务器发送请求的情况下，直接从本地缓存中获取资源。 Expires强缓存：设置一个强缓存时间，此时间范围内，从内存中读取缓存并返回。但是因为Expires判断强缓存过期的机制是获取本地时间戳，与之前拿到的资源文件中的Expires字段的时间做比较来判断是否需要对服务器发起请求。这里有一个巨大的漏洞：“如果我本地时间不准咋办？”所以目前已经被废弃了。 Cache-Control强缓存：目前使用的强缓存是通过HTTP响应头中的Cache-Control字段实现，通过max-age来告诉浏览器在指定时间内可以直接使用缓存数据，无需再次请求。 协商缓存：当强缓存失效时，浏览器会发送请求到服务器，通过ETag或Last-Modified等HTTP响应头与服务器进行验证，以确定资源是否被修改。如果资源未修改，服务器返回304 Not Modified状态码，告知浏览器使用本地缓存；如果资源已修改，则返回新的资源，浏览器更新本地缓存。这种方式需要与服务器通信，但可以确保用户总是获取最新的内容。 基于Last-Modified的协商缓存 Last-Modified 是资源的最后修改时间，服务器在响应头部中返回。 当客户端读取到Last-modified的时候，会在下次的请求标头中携带一个字段:If-Modified-Since，而这个请求头中的If-Modified-Since就是服务器第一次修改时候给他的时间 服务器比较请求中的 If-Modified-Since 值与当前资源的 Last-Modified 值，如果比对的结果是没有变化，表示资源未发生变化，返回状态码 304 Not Modified。如果比对的结果说资源已经更新了，就会给浏览器正常返回资源，返回200状态。 但是这样的协商缓存有两个缺点： 因为是更改文件修改时间来判断的，所以在文件内容本身不修改的情况下，依然有可能更新文件修改时间（比如修改文件名再改回来），这样，就有可能文件内容明明没有修改，但是缓存依然失效了。 当文件在极短时间内完成修改的时候（比如几百毫秒）。因为文件修改时间记录的最小单位是秒，所以，如果文件在几百毫秒内完成修改的话，文件修改时间不会改变，这样，即使文件内容修改了，依然不会返回新的文件。 基于ETag的协商缓存：将原先协商缓存的比较时间戳的形式修改成了比较文件指纹（根据文件内容计算出的唯一哈希值）。 ETag 是服务器为资源生成的唯一标识符（文件指纹），可以是根据文件内容计算出的哈希值，服务端将其和资源一起放回给客户端。 客户端在请求头部的 If-None-Match 字段中携带上次响应的 ETag 值。 服务器比较请求中的 If-None-Match 值与当前资源的 ETag 值，如果匹配，表示资源未发生变化，返回状态码 304 Not Modified。如果两个文件指纹不吻合，则说明文件被更改，那么将新的文件指纹重新存储到响应头的ETag中并返回给客户端 ![](..\\..\\assets\\images\\强缓存和协商缓存.png) !image-20240919144747086",
    "url": "/网络相关/什么是强缓存和协商缓存.html",
    "lang": ""
  },
  {
    "title": "HTTP1.0和HTTP1.1的区别",
    "content": "--- title: HTTP1.0和HTTP1.1的区别 --- 首先http1.1是默认支持持久连接的，可以通过设置Connection字段为 keep-alive来实现，减少连接建立和关闭的开销。而http1.0默认为短连接。 http1.1是支持管道化的，可以在请求的响应到达之前发送多个请求，可以减少等待时间，但是在处理方那边还是需要按照请求顺序处理。 http1.1引入了比如etag的缓存控制策略，直接对资源进行hash,可以更加精确的确保资源是否被修改。 持久连接：HTTP/1.1 默认支持持久连接，允许在一个TCP连接上发送多个HTTP请求和响应，减少了连接建立和关闭的开销。而HTTP/1.0 默认为短连接，每次请求都需要建立一个TCP连接，并通过Connection: keep-alive头来实现持久连接。 管道化：HTTP/1.1 支持管道化(不是默认开启)，允许客户端在第一个请求的响应到达之前发送多个请求，这可以减少等待时间，提高效率。HTTP/1.0不支持管道化。 缓存控制：HTTP1.0主要使用If-Modified-Since/Expires来做为缓存判断的标准，而HTTP1.1则引入了更多的缓存控制策略例如Etag / If-None-Match等更多可供选择的缓存头来控制缓存策略。 错误处理：HTTP/1.1 增加了一些新的HTTP状态码，如100 Continue，用于增强错误处理和请求的中间响应。 Host 头：HTTP/1.1 引入了Host头，允许客户端指定请求的主机名，这使得在同一台服务器上托管多个域名成为可能。HTTP/1.0没有这个头字段。 带宽优化 ：HTTP1.0中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能， 而HTTP1.1则在请求头引入了range头域，它允许只请求资源的某个部分，即返回码是206（Partial Content）",
    "url": "/网络相关/HTTP1.0和HTTP1.1的区别.html",
    "lang": ""
  },
  {
    "title": "HTTP2.0与HTTP1.1的区别？",
    "content": "--- title: HTTP2.0与HTTP1.1的区别？ --- HTTP2.0采用的是==二进制格式传输数据==，而不是文本格式，使得==解析可以更加高效==，减少了解析时间。 HTTP2.0还采用了==头部压缩==，实现是在==两端建立一个索引表==，对相同的头只==发送索引表中的索引==。 HTTP2.0还支持==多路复用==，允许在==单个TCP连接上切分成多个流==，每个流有自己的ID，它是一个虚拟的通道，并且还可以设置优先级。 HTTP 2.0 还将所有的==传输信息分割为更小的帧==。常见的帧有==Header 帧==，用于==传输 Header 内容==，并且会开启一个新的流。再就是 ==Data 帧==，用来传输正文实体。 多个 Data 帧属于同一个流。通过这两种机制，HTTP 2.0 的客户端可以将==多个请求分到不同的流中==，然后将请求内容拆成帧，进行二进制传输。这些帧可以打散乱序发送， 然后根据==每个帧首部的流标识符重新组装==，并且可以根据优先级，决定优先处理哪个流的数据。 但是还是==受限于tcp层的队头阻塞==，这时候其他流的包的顺序号如果在tcp层是更早的，那么这个包没到达，会影响其他流的数据收发。 !img 二进制协议：HTTP/2.0 采用二进制格式传输数据，而非HTTP/1.1 的文本格式，使得解析更高效，减少了解析时间。 多路复用：HTTP/2.0 支持多路复用，允许在单个TCP连接上并行交错发送多个请求和响应，解决了HTTP/1.1 中的队头阻塞问题。 头部压缩：HTTP/2.0 引入了HPACK 压缩算法，对请求和响应的头部信息进行压缩，减少了冗余头部信息的传输，提高了传输效率。 服务器推送：HTTP/2.0 允许服务器主动推送资源给客户端，而不需要客户端明确请求，这可以减少页面加载时间。 优先级和依赖：HTTP/2.0 允许客户端为请求设置优先级，并表达请求之间的依赖关系，资源加载更加有序。",
    "url": "/网络相关/HTTP2.0与HTTP1.1的区别？.html",
    "lang": ""
  },
  {
    "title": "HTTP3.0有了解过吗？",
    "content": "--- title: HTTP3.0有了解过吗？ --- 无队头阻塞: QUIC 使用UDP协议来传输数据。一个连接上的多个stream之间==没有TCP层的序列号应答依赖==, 如果一个stream丢了一个UDP包，不会影响后面的stream，不存在 队头阻塞问题。 零 RTT 连接建立：QUIC 允许在首次连接时进行零往返时间连接建立，从而减少了连接延迟，加快了页面加载速度。 ==连接迁移==：QUIC 允许在网络切换（如从 Wi-Fi 到移动网络）时，将连接迁移到新的 IP 地址，从而减少连接的中断时间。具体的实现是==利用一个64位的随机数作为ID来标识==，而==不是利用四元组进行标识一条连接== 向前纠错机制：每个数据包除了它本身的内容之外，还包括了部分其他数据包的数据，因此少量的丢包可以通过其他包的冗余数据直接组装而无需重传。向前纠错牺牲了每个数据包可以发送数据的上限，但是减少了因为丢包导致的数据重传。 安全性：HTTP/3默认使用TLS加密，确保了数据传输的安全性。 滑动窗口的起始位置是当前收到的最大offset，而不是TCP中下一个还没有收到的ACK的包。 HTTP/3是HTTP协议的最新版本，它基于QUIC协议，具有以下特点： 无队头阻塞: QUIC 使用UDP协议来传输数据。一个连接上的多个stream之间没有依赖, 如果一个stream丢了一个UDP包，不会影响后面的stream，不存在 队头阻塞问题。 零 RTT 连接建立：QUIC 允许在首次连接时进行零往返时间连接建立，从而减少了连接延迟，加快了页面加载速度。 连接迁移：QUIC 允许在网络切换（如从 Wi-Fi 到移动网络）时，将连接迁移到新的 IP 地址，从而减少连接的中断时间。 向前纠错机制：每个数据包除了它本身的内容之外，还包括了部分其他数据包的数据，因此少量的丢包可以通过其他包的冗余数据直接组装而无需重传。向前纠错牺牲了每个数据包可以发送数据的上限，但是减少了因为丢包导致的数据重传。 安全性：HTTP/3默认使用TLS加密，确保了数据传输的安全性。 QUIC 也有个序列号，是递增的。任何一个序列号的包只发送一次，下次就要加一了。例如，发送一个包，序号是 100，发现没有返回；再次发送的时候，序号就是 101 了；如果返回的 ACK 100，就是对第一个包的响应。如果返回 ACK 101 就是对第二个包的响应，RTT 计算相对准确。但是这里有一个问题，就是怎么知道包 100 和包 101 发送的是同样的内容呢？ QUIC 定义了一个 offset 概念。QUIC 既然是面向连接的，也就像 TCP 一样，是一个数据流，发送的数据在这个数据流里面有个偏移量 offset，可以通过 offset 查看数据发送到了哪里，这样只要这个 offset 的包没有来，就要重发；如果来了，按照 offset 拼接，还是能够拼成一个流。",
    "url": "/网络相关/HTTP3.0有了解过吗？.html",
    "lang": ""
  },
  {
    "title": "HTTPS和HTTP有哪些区别",
    "content": "--- title: HTTPS和HTTP有哪些区别 --- 两者的主要区别在于安全性和数据加密： 加密层：HTTPS 在HTTP 的基础上增加了SSL/TLS 协议作为加密层，确保数据传输的安全性。而HTTP 数据传输是明文的，容易受到攻击。 HTTP 连接建立相对简单， TCP 三次握手之后便可进行 HTTP 的报文传输。而 HTTPS 在 TCP 三次握手之后，还需进行 SSL/TLS 的握手过程，才可进入加密报文传输。 端口：HTTPS 通常使用端口443 ，而HTTP 使用端口80。 HTTPS 协议需要向 CA 申请数字证书，来保证服务器的身份是可信的。 !image-20240919144921820",
    "url": "/网络相关/HTTPS和HTTP有哪些区别.html",
    "lang": ""
  },
  {
    "title": "HTTPS的工作原理(HTTPS建立连接的过程)",
    "content": "--- title: HTTPS的工作原理(HTTPS建立连接的过程) --- HTTPS 采⽤的是对称加密和非对称加密结合的「混合加密」⽅式： HTTPS在建立了TCP连接之后还需要进行TLS握手阶段，我介绍一下基于RSA算法密钥交换的流程。 1.客户端首先发送client hello,将随机数R1,TLS版本号，密码套件（RSA算法）发过去。 2.服务端收到后，发送server hello,将随机数R2,以及它的数字证书发送过去，并且确认TLS版本号以及密码套件 3.客户端通过CA来校验证书，取出公钥，并通过公钥加密随机数pre-master-key发送给服务端。再结合这三个随机数算出会话密钥，这个会话密钥就是为之后通信做对称加密的密钥。 4.服务端利用自己的私钥解密取出pre-master-key，同样结合这三个随机数算出会话密钥。然后发送握手数据的摘要。之后进行对称加密。 HTTPS 主要基于SSL/TLS 协议，确保了数据传输的安全性和完整性, 其建立连接并传输数据的过程如下： 密钥交换：客户端发起HTTPS请求后，服务器会发送其公钥证书给客户端。 证书验证：客户端会验证服务器的证书是否由受信任的证书颁发机构（CA ）签发，并检查证书的有效性。 加密通信：一旦证书验证通过，客户端会利用三个随机数生成一个随机的对称加密密钥，并使用服务器的公钥加密这个密钥，然后发送给服务器。 建立安全连接：服务器使用自己的私钥解密得到对称加密密钥，此时客户端和服务器都有了相同的密钥，可以进行加密和解密操作。 数据传输：使用对称加密密钥对所有传输的数据进行加密，确保数据在传输过程中的安全性。 完整性校验：SSL/TLS协议还包括消息完整性校验机制，如消息认证码，确保数据在传输过程中未被篡改。 结束连接：数据传输完成后，通信双方会进行会话密钥的销毁，以确保不会留下安全隐患。 数字证书签发和验证流程： !image-20241021164246846/image-20241021164246846.png)",
    "url": "/网络相关/HTTPS的工作原理(HTTPS建立连接的过程).html",
    "lang": ""
  },
  {
    "title": "TCP和UDP的区别",
    "content": "--- title: TCP和UDP的区别 --- TCP是面向连接的协议，需要在数据传输前建立连接；UDP是无连接的，不需要建立连接。 TCP提供可靠的数据传输，保证数据包的顺序和完整性；UDP不保证数据包的顺序或完整性。 TCP具有拥塞控制机制，可以根据网络状况调整数据传输速率；UDP没有拥塞控制，发送速率通常固定。 TCP通过滑动窗口机制进行流量控制，避免接收方处理不过来；UDP没有流量控制。 TCP能够检测并重传丢失或损坏的数据包；UDP不提供错误恢复机制。 TCP有复杂的报文头部，包含序列号、确认号等信息；UDP的报文头部相对简单。 由于TCP的连接建立、数据校验和重传机制，其性能开销通常比UDP大；UDP由于简单，性能开销小。 适用场景：TCP适用于需要可靠传输的应用，如网页浏览、文件传输等；UDP适用于对实时性要求高的应用，如语音通话、视频会议等。 !image-20241120171015625 !image-20241120170951675",
    "url": "/网络相关/TCP和UDP的区别.html",
    "lang": ""
  },
  {
    "title": "TCP连接如何确保可靠性",
    "content": "--- title: TCP连接如何确保可靠性 --- TCP通过差错控制（序列号、确认应答、数据校验）、超时重传、流量控制、拥塞控制等机制，确保了数据传输的可靠性和效率。 序列号：每个TCP段都有一个序列号，确保数据包的顺序正确。 确认应答：接收方发送ACK确认收到的数据，如果发送方在一定时间内没有收到确认，会重新发送数据。 数据校验：TCP使用校验和来检测数据在传输过程中是否出现错误，如果检测到错误，接收方会丢弃该数据包，并等待重传。 超时重传：发送方设置一个定时器，如果在定时器超时之前没有收到确认，发送方会重传数据。 流量控制：TCP通过滑动窗口机制进行流量控制，确保接收方能够处理发送方的数据量。 拥塞控制：TCP通过算法如慢启动、拥塞避免、快重传和快恢复等，来控制数据的发送速率，防止网络拥塞。",
    "url": "/网络相关/TCP连接如何确保可靠性.html",
    "lang": ""
  },
  {
    "title": "TCP的拥塞控制是如何实现的",
    "content": "--- title: TCP的拥塞控制是如何实现的 --- TCP拥塞控制可以在网络出现拥塞时动态调整数据传输的速率，防止网络过载 主要有四个机制： 首先是==慢启动==，初始阶段的时候,TCP发送方会以较小的发送窗口开始传输数据。每次成功收到确认之后，会通过指数增长的方式来增加发送窗口的大小。 一旦达到一定的阈值之后，TCP发送方就会进入==拥塞避免==的阶段，在这个阶段发送方是以线性增加的方式来增加发送窗口的大小。 如果发送方连续收到相同的确认，它会认为发生了数据包的丢失，这个时候会==快速重传==没有确认的数据包，不必等待超时。 在发生快重传后，TCP会进入==快恢复==阶段，将慢启动阈值设置为当前窗口的一半，并重新调整拥塞窗口的大小。 !image-20241016144347470 TCP拥塞控制可以在网络出现拥塞时动态地调整数据传输的速率，以防止网络过载。TCP拥塞控制的主要机制包括以下几个方面： 慢启动（Slow Start）： 初始阶段，TCP发送方会以较小的发送窗口开始传输数据。随着每次成功收到确认的数据，发送方逐渐增加发送窗口的大小，实现指数级的增长，这称为慢启动。这有助于在网络刚开始传输时谨慎地逐步增加速率，以避免引发拥塞。 拥塞避免（Congestion Avoidance）： 一旦达到一定的阈值（通常是慢启动阈值），TCP发送方就会进入拥塞避免阶段。在拥塞避免阶段，发送方以线性增加的方式增加发送窗口的大小，而不再是指数级的增长。这有助于控制发送速率，以避免引起网络拥塞。 快速重传（Fast Retransmit）： 如果发送方连续收到相同的确认，它会认为发生了数据包的丢失，并会快速重传未确认的数据包，而不必等待超时。这有助于更快地恢复由于拥塞引起的数据包丢失。 快速恢复（Fast Recovery）： 在发生快速重传后，TCP进入快速恢复阶段。在这个阶段，发送方不会回到慢启动阶段，而是将慢启动阈值设置为当前窗口的一半，并将拥塞窗口大小设置为慢启动阈值加上已确认但未被快速重传的数据块的数量。这有助于更快地从拥塞中恢复。 在初始阶段，TCP发送方会以较小的发送窗口开始传输数据。随着每次成功收到确认的数据，发送方会通过指数增加的方式逐渐增加发送窗口的大小，这叫做慢启动。这有助于在网络刚开始传输时谨慎地逐步增加速率，以避免引发拥塞。一旦达到了一定阈值，TCP发送方就会进入到拥塞避免阶段。在这个阶段，发送方以线性增加的方式增加发送窗口的大小。这也有助于控制发送速率，以避免引起网络拥塞。如果发送方连续收到相同的确认，它会认为发生了数据包的丢失，并会快速重传未确认的数据包，而不必等待超时。这有助于更快地恢复由于拥塞引起的数据包丢失。在发生快重传后，TCP会进入快恢复阶段。在这个阶段，发送方不会回到慢启动阶段，而是将慢启动阈值设置为当前窗口的一半，并将拥塞窗口大小设置为慢启动阈值加上已确认但未被快重传的数据块的数量。这有助于更快地从拥塞中恢复。",
    "url": "/网络相关/TCP的拥塞控制是如何实现的.html",
    "lang": ""
  },
  {
    "title": "TCP的流量控制是如何实现的？",
    "content": "--- title: TCP的流量控制是如何实现的？ --- 流量控制就是让发送方发送速率不要过快，让接收方来得及接收。 tcp是利用滑动窗口机制来实现流量控制的。可以动态调整双方的数据传输速率。 在每个TCP报文段都包含一个窗口字段，这个字段动态调整表示发送方可以发送多少字节的数据而不用等待确认。 当接收方的缓冲区快满了，它会减少窗口大小通知发送方。 并且接收方会定期发送ack报文来告知发送方已成功接收的数据。 一、拥塞控制和流量控制的区别 拥塞控制：拥塞控制是作用于网络的，它是防止过多的数据注入到网络中，避免出现网络负载过大的情况；常用的方法就是：（ 1 ）慢开始、拥塞避免（ 2 ）快重传、快恢复。 流量控制：流量控制是作用于接收者的，它是控制发送者的发送速度从而使接收者来得及接收，防止分组丢失的。 流量控制就是让发送方发送速率不要过快，让接收方来得及接收。利用滑动窗口机制就可以实施流量控制，主要方法就是动态调整发送方和接收方之间数据传输速率。 滑动窗口大小： 在TCP通信中，每个TCP报文段都包含一个窗口字段，该字段指示发送方可以发送多少字节的数据而不等待确认。这个窗口大小是动态调整的。 接收方窗口大小： 接收方通过TCP报文中的窗口字段告诉发送方自己当前的可接收窗口大小。这表示接收方缓冲区中还有多少可用空间。 流量控制的目标： 流量控制的目标是确保发送方不要发送超过接收方缓冲区容量的数据。如果接收方的缓冲区快满了，它会减小窗口大小，通知发送方暂停发送，以防止溢出。 动态调整： 发送方会根据接收方的窗口大小动态调整发送数据的速率。如果接收方的窗口大小增加，发送方可以加速发送数据。如果窗口大小减小，发送方将减缓发送数据的速率。 确认机制： 接收方会定期发送确认（ACK）报文，告知发送方已成功接收数据。这也与流量控制密切相关，因为接收方可以通过ACK报文中的窗口字段来通知发送方它的当前窗口大小。",
    "url": "/网络相关/TCP的流量控制是如何实现的？.html",
    "lang": ""
  },
  {
    "title": "UDP如何实现可靠传输",
    "content": "--- title: UDP如何实现可靠传输 --- UDP==在传输层无法保证数据的可靠传输，只能通过应用层来实现了==。实现的方式可以参照tcp可靠性传输的方式，只是实现转移到了应用层。关键在于两点，从应用层角度考虑： （1）提供==超时重传，能避免数据报丢失==。 （2）提供==确认序列号，可以对数据报进行确认和排序==。 本端：首先在UDP数据报定义一个首部，首部包含确认序列号和时间戳，时间戳是用来计算RTT(数据报传输的往返时间)，计算出合适的RTO(重传的超时时间)。然后收到对端的确认之后才发送下一个的数据报。当时间超时，本端重传数据报，同时RTO扩大为原来的两倍，重新开始计时。 对端：接受到一个数据报之后取下该数据报首部的时间戳和确认序列号，并添加本端的确认数据报首部之后发送给对端。根据此序列号对已收到的数据报进行排序并丢弃重复的数据报。",
    "url": "/网络相关/UDP如何实现可靠传输.html",
    "lang": ""
  },
  {
    "title": "TCP连接三次握手的过程，为什么是三次，可以是两次或者更多吗？",
    "content": "--- title: TCP连接三次握手的过程，为什么是三次，可以是两次或者更多吗？ --- 首先假设客户端进行连接，发送一个SYN报文到服务端，请求建立连接，客户端进入SYNSENT状态。 服务端收到SYN报文后呢，会从LISTEN状态转变为SYNRCVD状态。并会发送一个ACK-SYN报文作为回应，此时这个连接是一个半连接状态，服务端会将它存到半连接队列中。 客户端收到这个报文后，会返回一个ACK报文作为回应，然后客户端进入了ESTABLISHED状态，而服务端收到ACK后也会进入ESTABLISHED状态，并且之后就可以利用accept方法将全连接队列中的连接拿出并生成对应的连接fd了。 为什么要三次握手呢？ 因为只有三次握手才可以确认双方的接收能力和发送能力，第一次握手保证了客户端到服务端的通道是开放的，第二次握手是保证了服务端到客户端的通道是开放的，第三次握手保证了客户端能够接收到服务端的确认，这样才可以确保双方的通道是可用的。 如果只使用两次握手的话，服务端没法确认客户端的接收能力是否正常，客户端可能已经连接关闭了，但是服务端接收到了之前的连接报文，然后建立了连接，这就导致了资源的浪费。 (1) 三次握手的过程 第一次握手：客户端向服务器发送一个SYN （同步序列编号）报文，请求建立连接，客户端进入SYNSENT 状态。 第二次握手：服务器收到SYN 报文后，如果同意建立连接，则会发送一个SYN-ACK （同步确认）报文作为响应，同时进入SYNRCVD 状态。 第三次握手：客户端收到服务器的SYN-ACK 报文后，会发送一个ACK （确认）报文作为最终响应，之后客户端和服务器都进入ESTABLISHED 状态，连接建立成功。 (2)为什么需要三次握手 通过三次握手，客户端和服务器都能够确认对方的接收和发送能力。第一次握手确认了客户端到服务器的通道是开放的；第二次握手确认了服务器到客户端的通道是开放的；第三次握手则确认了客户端能够接收到服务器的确认，从而确保了双方的通道都是可用的。 而如果仅使用两次握手，服务器可能无法确定客户端的接收能力是否正常，比如客户端可能已经关闭了连接，但之前发送的连接请求报文在网络上延迟到达了服务器，服务器就会主动去建立一个连接，但是客户端接收不到，导致资源的浪费。而四次握手可以优化为三次。 !image-20240920102706907",
    "url": "/网络相关/TCP连接三次握手的过程，为什么是三次，可以是两次或者更多吗？.html",
    "lang": ""
  },
  {
    "title": "TCP连接四次挥手的过程，为什么是四次？",
    "content": "--- title: TCP连接四次挥手的过程，为什么是四次？ --- 假设客户端主动调用关闭连接，发送FIN报文给服务端，并且客户端进入FINWAIT1状态，然后服务端收到这个报文后，read会返回EOF，并且进入CLOSEWAIT,并发送ACK报文给客户端，服务端在这个状态还是可以发送数据的。客户端收到这个ACK后，会进入FINWAIT2状态。 当服务端主动调用关闭连接函数的时候，发送一个FIN报文给客户端，服务端此时进入LASTACK状态。 客户端收到这个FIN报文后，会返回一个ACK报文，并进入TIMEWAIT状态，这个状态等待2个MSL时间才会关闭。服务端收到这个ACK后就会进入 CLOSE状态。 为什么需要四次挥手？ 因为TCP是全双工通信，两次挥手只能释放一端到另一端的TCP连接。 并且，只有四次挥手，才能确保双方都能接收到最后一个数据报的ACK。如果使用三次握手的话，服务器在发送最后一个数据段后立即关闭连接，而客户端可能因为网络原因接收不到这个数据，那么也就无法正常关闭。 !image-20240918100057136 （1）四次挥手的过程 第一次挥手：客户端发送一个FIN报文给服务端，表示自己要断开数据传送，报文中会指定一个序列号 (seq=x)。然后，客户端进入FIN-WAIT-1 状态。 第二次挥手：服务端收到FIN报文后，回复ACK报文给客户端，且把客户端的序列号值+1，作为ACK报文的序列号(seq=x+1)。然后，服务端进入CLOSE-WAIT(seq=x+1)状态，客户端进入FIN-WAIT-2状态。 第三次挥手：服务端也要断开连接时，发送FIN报文给客户端，且指定一个序列号(seq=y+1)，随后服务端进入LAST-ACK状态。 第四次挥手：客户端收到FIN报文后，发出ACK报文进行应答，并把服务端的序列号值+1作为ACK报文序列号(seq=y+2)。此时客户端进入TIME-WAIT状态。服务端在收到客户端的ACK 报文后进入CLOSE 状态。如果客户端等待2MSL没有收到回复，才关闭连接。 客户端主动调用关闭连接的函数，于是就会发送 FIN 报文，这个 FIN 报文代表客户端不会再发送数据了，进入 FINWAIT1 状态； 服务端收到了 FIN 报文，然后马上回复一个 ACK 确认报文，此时服务端进入 CLOSEWAIT 状态。在收到 FIN 报文的时候，TCP 协议栈会为 FIN 包插入一个文件结束符 EOF 到接收缓冲区中，服务端应用程序可以通过 read 调用来感知这个 FIN 包，这个 EOF 会被放在已排队等候的其他已接收的数据之后，所以必须要得继续 read 接收缓冲区已接收的数据； 接着，当服务端在 read 数据的时候，最后自然就会读到 EOF，接着 read() 就会返回 0，这时服务端应用程序如果有数据要发送的话，就发完数据后才调用关闭连接的函数，如果服务端应用程序没有数据要发送的话，可以直接调用关闭连接的函数，这时服务端就会发一个 FIN 包，这个 FIN 报文代表服务端不会再发送数据了，之后处于 LASTACK 状态； 客户端接收到服务端的 FIN 包，并发送 ACK 确认包给服务端，此时客户端将进入 TIMEWAIT 状态； 服务端收到 ACK 确认包后，就进入了最后的 CLOSE 状态； 客户端经过 2MSL 时间之后，也进入 CLOSE 状态； （2）为什么需要四次挥手 TCP是全双工通信，可以双向传输数据。任何一方都可以在数据传送结束后发出连接释放的通知，待对方确认后进入半关闭状态。 当另一方也没有数据再发送的时候，则发出连接释放通知，对方确认后才会完全关闭 TCP 连接。因此两次挥手可以释放一端到另一端的TCP连接，完全释放连接一共需要四次挥手。 只有通过四次挥手，才可以确保双方都能接收到对方的最后一个数据段的确认，主动关闭方在发送完最后一个ACK后进入TIME-WAIT 状态，这是为了确保被动关闭方接收到最终的ACK ，如果被动关闭方没有接收到，它可以重发FIN 报文，主动关闭方可以再次发送ACK 。 而如果使用三次挥手，被动关闭方可能在发送最后一个数据段后立即关闭连接，而主动关闭方可能因为网络原因接受不到这个数据段，那么也就无法正常关闭，而此时被动关闭方已经关闭连接了，也无法重传这个数据段。 服务器收到客户端的 FIN 报文时，内核会马上回一个 ACK 应答报文，但是服务端应用程序可能还有数据要发送，所以并不能马上发送 FIN 报文，而是将发送 FIN 报文的控制权交给服务端应用程序： 如果服务端应用程序有数据要发送的话，就发完数据后，才调用关闭连接的函数； 如果服务端应用程序没有数据要发送的话，可以直接调用关闭连接的函数， 从上面过程可知，是否要发送第三次挥手的控制权不在内核，而是在被动关闭方（上图的服务端）的应用程序，因为应用程序可能还有数据要发送，由应用程序决定什么时候调用关闭连接的函数，当调用了关闭连接的函数，内核就会发送 FIN 报文了，所以服务端的 ACK 和 FIN 一般都会分开发送。 > FIN 报文一定得调用关闭连接的函数，才会发送吗？ 不一定。 如果进程退出了，不管是不是正常退出，还是异常退出（如进程崩溃），内核都会发送 FIN 报文，与对方完成四次挥手。 计算机网络 - TCP 协议原理总结 | 春水煎茶 (writings.sh)",
    "url": "/网络相关/TCP连接四次挥手的过程，为什么是四次？.html",
    "lang": ""
  },
  {
    "title": "HTTP的Keep-Alive是什么？TCP 的 Keepalive 和 HTTP 的 Keep-Alive 是一个东西吗？",
    "content": "--- title: HTTP的Keep-Alive是什么？TCP 的 Keepalive 和 HTTP 的 Keep-Alive 是一个东西吗？ --- HTTP 的 Keep-Alive，是由应用层实现的，称为 HTTP 长连接 每次请求都要经历这样的过程：建立 TCP连接 -> HTTP请求资源 -> 响应资源 -> 释放连接，这就是HTTP短连接，但是这样每次建立连接都只能请求一次资源，所以HTTP 的 Keep-Alive实现了使用同一个 TCP 连接来发送和接收多个 HTTP 请求/应答，避免了连接建立和释放的开销，就就是 HTTP 长连接。通过设置HTTP头Connection: keep-alive来实现。 TCP 的 Keepalive，是由TCP 层（内核态）实现的，称为 TCP 保活机制，是一种用于在 TCP 连接上检测空闲连接状态的机制 当TCP连接建立后，如果一段时间内没有任何数据传输，TCP Keepalive会发送探测包来检查连接是否仍然有效。 TCP的Keepalive是发送心跳包来确保对端是否能够正常响应",
    "url": "/网络相关/HTTP的Keep-Alive是什么？TCP 的 Keepalive 和 HTTP 的 Keep-Alive 是一个东西吗？.html",
    "lang": ""
  },
  {
    "title": "DNS查询过程",
    "content": "--- title: DNS查询过程 --- DNS 用来将主机名和域名转换为IP地址, 其查询过程一般通过以下步骤： 本地DNS缓存检查->本地DNS服务器->根DNS服务器(告知去哪个顶级域名DNS服务器查询)->顶级域名服务器(告知去哪个权威DNS服务器查询)->权威DNS服务器存储特定域名和IP地址->返回给本地DNS解析器，并缓存 本地DNS缓存检查：首先查询本地DNS缓存，如果缓存中有对应的IP地址，则直接返回结果。 如果本地缓存中没有，则会向本地的DNS服务器（通常由你的互联网服务提供商（ISP）提供， 比如中国移动）发送一个DNS查询请求。 如果本地DNS解析器有该域名的ip地址，就会直接返回，如果没有缓存该域名的解析记录，它会向根DNS服务器发出查询请求。根DNS服务器并不负责解析域名，但它能告诉本地DNS解析器应该向哪个顶级域（.com/.net/.org）的DNS服务器继续查询。 本地DNS解析器接着向指定的顶级域名DNS服务器发出查询请求。顶级域DNS服务器也不负责具体的域名解析，但它能告诉本地DNS解析器应该前往哪个权威DNS服务器查询下一步的信息。 本地DNS解析器最后向权威DNS服务器发送查询请求。 权威DNS服务器是负责存储特定域名和IP地址映射的服务器。当权威DNS服务器收到查询请求时，它会查找\"example.com\"域名对应的IP地址，并将结果返回给本地DNS解析器。 本地DNS解析器将收到的IP地址返回给浏览器，并且还会将域名解析结果缓存在本地，以便下次访问时更快地响应。 浏览器发起连接： 本地DNS解析器已经将IP地址返回给您的计算机，您的浏览器可以使用该IP地址与目标服务器建立连接，开始获取网页内容。 !img",
    "url": "/网络相关/DNS查询过程.html",
    "lang": ""
  },
  {
    "title": "CDN是什么，有什么作用？",
    "content": "--- title: CDN是什么，有什么作用？ --- CDN是一种分布式网络服务，通过将内容存储在分布式的服务器上，使用户可以从距离较近的服务器获取所需的内容，从而加速互联网上的内容传输。 就近访问：CDN 在全球范围内部署了多个服务器节点，用户的请求会被路由到距离最近的 CDN 节点，提供快速的内容访问。 内容缓存：CDN 节点会缓存静态资源，如图片、样式表、脚本等。当用户请求访问这些资源时，CDN 会首先检查是否已经缓存了该资源。如果有缓存，CDN 节点会直接返回缓存的资源，如果没有缓存所需资源，它会从源服务器（原始服务器）回源获取资源，并将资源缓存到节点中，以便以后的请求。通过缓存内容，减少了对原始服务器的请求，减轻了源站的负载。 可用性：即使某些节点出现问题，用户请求可以被重定向到其他健康的节点。",
    "url": "/网络相关/CDN是什么，有什么作用？.html",
    "lang": ""
  },
  {
    "title": "Cookie和Session是什么？有什么区别？",
    "content": "--- title: Cookie和Session是什么？有什么区别？ --- cookie和session都是用来管理用户的状态和身份的，区别是cookie通过在客户端记录信息，而session在服务端记录信息。 通常服务器将cookie发送给用户，然后用户将cookie存储在本地，客户端发送的请求是携带cookie的。 如果使用session的时候，服务器要为每个用户分配一个唯一的session ID,然后将这个ID发给用户，用户利用这个ID与服务端的session做一个对应。 并且cookie的存储容量通常比较小，session的存储容量比较大，通常没有固定的限制。 cookie的话容易被篡改，所以安全性更低 (1) Cookie和Session是什么？ Cookie 和 Session 都用于管理用户的状态和身份, Cookie通过在客户端记录信息确定用户身份，Session通过在服务器端记录信息确定用户身份。 Cookie 通常，服务器会将一个或多个 Cookie 发送到用户浏览器，然后浏览器将这些 Cookie 存储在本地。 服务器在接收到来自客户端浏览器的请求之后，就能够通过分析存放于请求头的Cookie得到客户端特有的信息，从而动态生成与该客户端相对应的内容。 Session 客户端浏览器访问服务器的时候，服务器把客户端信息以某种形式记录在服务器上。这就是Session。Session 主要用于维护用户登录状态、存储用户的临时数据和上下文信息等。服务器为每个用户分配一个唯一的Session ID，通常存储在Cookie中。 （2） Cookie和Session的区别？ 存储位置：Cookie 数据存储在用户的浏览器中，而 Session 数据存储在服务器上。session相当于用一个key与cookie做了关联，存在了服务中，之后将这个key发给浏览器，浏览器就以这个key来作为cookie了 数据容量：Cookie 存储容量较小，一般为几 KB。Session 存储容量较大，通常没有固定限制，取决于服务器的配置和资源。 安全性：由于 Cookie 存储在用户浏览器中，因此可以被用户读取和篡改。相比之下，Session 数据存储在服务器上，更难被用户访问和修改。 生命周期：Cookie可以设置过期时间，Session 依赖于会话的持续时间或用户活动。 传输方式：Cookie 在每次 HTTP 请求中都会被自动发送到服务器，而 Session ID 通常通过 Cookie 或 URL 参数传递。",
    "url": "/网络相关/Cookie和Session是什么？有什么区别？.html",
    "lang": ""
  },
  {
    "title": "RESTFUL",
    "content": "--- title: RESTFUL --- RESTFUL不仅仅是指API，而是一种==架构风格==，中文名叫做==表述性状态转移==。 ==核心在于无状态==，其实就是==服务端只维护资源的状态，而客户端来维护会话的状态==，会话的状态就是指当前这个会话进行到哪一步了。==这样的话客户端进行调用的时候，不是去想过程是如何操作的，而是去以资源状态的结果为核心。== 比如要访问下一页数据，客户端已经记录了当前在哪一个页面了，所以他只需要告诉服务端我需要访问哪一个页面即可，而不是告诉服务端我要访问下一页。 总结就是RestFul不仅仅是一种API，而是一种架构模式，主要面向的是资源，提供的是无状态服务 ，==有利于横向扩展做高并发的==。 -------------------------- 然而 RESTful 可不仅仅是指 API，而是一种架构风格，全称 Representational State Transfer，表述性状态转移，来自一篇重要的论文《架构风格与基于网络的软件架构设计》（Architectural Styles and the Design of Network-based Software Architectures）。 ==所谓的无状态，其实是服务端维护资源的状态，客户端维护会话的状态== 按照这种思路，==对于 API 的设计，就慢慢变成了以资源为核心，而非以过程为核心。==也就是说，客户端只要告诉服务端你想让资源状态最终变成什么样就可以了，而不用告诉我过程，不用告诉我动作。 还是文件目录的例子。客户端应该访问哪个绝对路径，而非一个动作，我就要进入某个路径。再如，库存的调用，应该查看当前的库存数目，然后减去购买的数量，得到结果的库存数。这个时候应该设置为目标库存数（但是当前库存数要匹配），而非告知减去多少库存。 这种 API 的设计需要实现幂等，因为网络不稳定，就会经常出错，因而需要重试，但是一旦重试，就会存在幂等的问题，也就是同一个调用，多次调用的结果应该一样，不能一次支付调用，因为调用三次变成了支付三次。不能进入 cd a，做了三次，就变成了 cd a/a/a。也不能扣减库存，调用了三次，就扣减三次库存。 但是 SOAP 的 XML 正文中，是可以放任何动作的。例如 XML 里面可以写 < ADD >，< MINUS > 等。这就方便使用 SOAP 的人，将大量的动作放在 API 里面。 RESTful 没这么复杂，也没给客户提供这么多的可能性，正文里的 JSON 基本描述的就是资源的状态，没办法描述动作，而且能够出发的动作只有 CRUD，也即 POST、GET、PUT、DELETE，也就是对于状态的改变。 SOAP 过于复杂，而且设计是面向动作的，因而往往因为架构问题导致并发量上不去。 RESTful 不仅仅是一个 API，==而且是一种架构模式，主要面向资源，提供无状态服务==，有利于横向扩展应对高并发。",
    "url": "/网络相关/RESTFUL.html",
    "lang": ""
  },
  {
    "title": "HTTP为什么使用TCP？",
    "content": "--- title: HTTP为什么使用TCP？ --- HTTP 使用 TCP（传输控制协议） 作为其底层传输协议，主要原因在于 TCP 的特性能够满足 HTTP 对可靠性、有序性和连接管理的核心需求。以下是具体原因： ------ 核心需求：HTTP 主要用于传输网页、文件、API 数据等，这些场景要求数据完整无误地到达接收方。 TCP 的机制： 确认应答（ACK）：接收方确认收到数据包，发送方才会继续发送下一个。 重传机制：如果数据包丢失或损坏，TCP 会自动重传。 流量控制：通过滑动窗口机制，避免发送方压垮接收方的缓冲区。 对比 UDP：UDP 不保证可靠性，数据包可能丢失或乱序，不适合 HTTP 的典型场景。 ------ 建立连接（三次握手）：HTTP 的请求-响应模型需要双方在通信前建立稳定的连接。 复制 客户端 -> SYN -> 服务端 客户端 <- SYN-ACK <- 服务端 客户端 -> ACK -> 服务端 断开连接（四次挥手）：确保双方完成数据传输后有序释放资源。 优势：TCP 的连接管理简化了 HTTP 的会话逻辑，避免应用层处理复杂的连接状态。 ------ 按序传输：TCP 保证数据包的接收顺序与发送顺序一致。 HTTP 的依赖：网页内容（HTML、CSS、JavaScript、图片等）需要按顺序加载，否则可能导致页面渲染错误。 ------ 大文件传输：HTTP 常用于传输大文件（如视频、软件包），TCP 的流式传输（将数据视为连续字节流）能高效处理此类场景。 对比 UDP：UDP 的数据包有固定大小限制，需应用层自行分片和重组。 ------ 网络基础设施：TCP 是互联网的基石协议，所有网络设备（路由器、防火墙等）均对其提供良好支持。 HTTP 的普及：HTTP/1.0 到 HTTP/2 均基于 TCP，确保了向后兼容性和生态系统的成熟。 ------ 尽管 HTTP 传统上依赖 TCP，但 HTTP/3 改用 UDP 作为底层协议，并通过 QUIC 协议 解决了 TCP 的某些缺陷： 解决队头阻塞（Head-of-Line Blocking）：QUIC 在 UDP 上实现了多路复用，单个数据包丢失不会阻塞整个连接。 更快的连接建立：QUIC 将加密和连接握手合并为单次往返（1-RTT），减少延迟。 适应移动网络：QUIC 支持连接迁移（如从 Wi-Fi 切换到 5G），更适合现代移动设备。 ------ | 特性 | TCP（HTTP/1.1、HTTP/2） | UDP + QUIC（HTTP/3） | | :----------- | :-------------------------- | :---------------------------- | | 可靠性 | ✅ 内置重传、确认机制 | ✅ QUIC 在应用层实现可靠性 | | 有序性 | ✅ 保证数据顺序 | ✅ QUIC 保证流内有序 | | 连接建立速度 | ❌ 三次握手（1.5-RTT） | ✅ 0-RTT 或 1-RTT | | 队头阻塞 | ❌ 单连接多流可能阻塞 | ✅ 单个流阻塞不影响其他流 | | 适用场景 | 传统 Web、兼容性要求高 | 高延迟网络（如 5G）、实时通信 | ------ 需要兼容旧设备或网络环境。 应用对可靠性要求极高，且无需极低延迟。 高延迟或不稳定网络（如移动网络）。 需要快速连接建立（如实时通信、在线游戏）。",
    "url": "/网络相关/HTTP为什么使用TCP？.html",
    "lang": ""
  },
  {
    "title": "TIME_WAIT状态存在的意义",
    "content": "--- title: TIMEWAIT状态存在的意义 --- 设计 TIMEWAIT 状态，主要有两个原因： 防止历史连接中的数据，被后面相同四元组的连接错误的接收；在TIMEWAIT状态中，旧数据会自然消失。 保证「被动关闭连接」的一方，能被正确的关闭；如果不设置TIMEWAIT状态，被动关闭方如果没有收到ACK，此时会重发FIN，如果这个时候主动关闭方已经是CLOSE的状态了，那么会返回一个RST报文过来，这个时候被动关闭方会出现异常终止。而TIMEWAIT是2MSL的时间，对端如果没有收到ACK会触发TCP重传机制，重新发送一个FIN，这样一来一去刚好两个MSL的时间。 在 TCP (传输控制协议) 中，RST（Reset）报文用于强制终止连接或拒绝一个不合法的连接请求。RST 报文的主要作用可以归纳为以下几点： 当一方希望立即终止现有的 TCP 连接时，可以发送 RST 报文。这种情况通常发生在以下情况下： 应用程序意外终止，导致 socket 被关闭。 需要强制清理连接，例如在检测到网络故障时。 当服务器收到一个连接请求（SYN 报文），但该连接不应被接受时，可以发送 RST 报文。这种情况可能发生在： 服务器没有监听该端口。 客户端连接了错误的IP地址或端口。 RST 报文可以用来处理错误情况。例如： 当一个数据包到达的连接已经关闭，接收方可以发送 RST 报文，以通知发送方该连接不存在。 在数据传输中出错时，接收方可以通过发送 RST 报文来指示问题，终止不可靠的连接。 在某些情况下，网络中的连接可能由于各种原因变得无效。使用 RST 报文可以迅速清理这些无效连接，帮助释放资源。 TCP 标志位：RST 报文的 TCP 标志位设置为 RST（即 R），并且通常没有其他标志位被设置。 序列号：RST 报文中可能包含序列号或确认号，以指明将要重置的连接的状态。",
    "url": "/网络相关/TIME_WAIT状态存在的意义.html",
    "lang": ""
  },
  {
    "title": "TCP半包和粘包",
    "content": "--- title: TCP半包和粘包 --- TCP是一种流式的通信协议，消息是以字节流的方式在信道中传输，这就意味着一个重要的事情，消息和消息之间是没有边界的。在不加额外约定的情况下，通信双方并不知道发送和接收到底有没有接收完一个消息，有可能多个消息会在一次传输中被发送和接收（江湖俗称\"粘包\"），也有可能一个消息需要多个传输才能被完整的发送和接收(江湖俗称\"半包\")。",
    "url": "/网络相关/TCP半包和粘包.html",
    "lang": ""
  },
  {
    "title": "什么是中间人攻击",
    "content": "--- title: 什么是中间人攻击 --- 中间人攻击(Man-in-the-MiddleAttack，简称“MITM攻击”)是指攻击者与通讯的两端分别创建独立的联系，并交换其所收到的数据，使通讯的两端认为他们正在通过一个私密的连接与对方直接对话，但事实上整个会话都被攻击者完全控制。",
    "url": "/网络相关/什么是中间人攻击.html",
    "lang": ""
  },
  {
    "title": "TLS握手过程",
    "content": "--- title: TLS握手过程 --- 传统的TLS握手过程基本都是用RSA算法来实现密钥加密的 1.客户端首先发送Client Hello, 将随机数R1,TLS版本号，密码套件发过去 2.服务端收到后发送Server Hello, 将随机数R2，以及数字证书发送 ,并确认TLS版本号，确认密码套件。Server Hello down 3.客户端从CA处校验证书、取出公钥，并通过公钥加密R3发送给服务端。再结合R1,R2,R3算出会话密钥，这个会话密钥就是为之后通信做加密的。然后开始使用会话密钥，并发送握手数据的摘要 4.服务端利用自己的私钥解密取出R3。结合R1,R2,R3算出会话密钥。开始使用会话密钥，并发送握手数据的摘要。 之后通过密钥进行加密报文",
    "url": "/网络相关/TLS握手过程.html",
    "lang": ""
  },
  {
    "title": "什么是SYN,SYN泛洪攻击呢？",
    "content": "--- title: 什么是SYN,SYN泛洪攻击呢？ --- SYN就是TCP报文中用来表示建立一个新连接的字段 SYN泛洪攻击是通过重复发送初始连接请求（SYN）数据包，攻击者能够压倒目标服务器机器上的所有可用端口，导致目标设备根本不响应合法流量 服务器响应每个连接请求，并留下开放端口准备好接收响应。 当服务器等待从未到达的最终ACK数据包时，攻击者继续发送更多的SYN数据包。每个新的SYN数据包的到达导致服务器暂时维持新的开放端口连接一段时间，一旦所有可用端口被使用，服务器就无法正常工作。 分布式攻击（DDoS）：如果使用僵尸网络创建攻击，则将攻击溯源到源的可能性很低。对于增加的混淆级别，攻击者可能会使每个分布式设备也欺骗发送数据包的IP地址。如果攻击者正在使用诸如未来僵尸网络的僵尸网络，他们通常不会关心被感染设备的IP掩蔽",
    "url": "/网络相关/什么是SYN,SYN泛洪攻击呢？.html",
    "lang": ""
  },
  {
    "title": "如果已经建立连接，但是客户端出现故障怎么办",
    "content": "--- title: 如果已经建立连接，但是客户端出现故障怎么办 --- TCP协议有一个保活机制。 这个机制的原理是这样的： 定义⼀个时间段，在这个时间段内，如果没有任何连接相关的活动，TCP 保活机制会开始作⽤，每隔⼀个时间间隔，发送⼀个探测报⽂，该探测报⽂包含的数据⾮常少，如果连续⼏个探测报⽂都没有得到响应，则认为当前的TCP 连接已经死亡，系统内核将错误信息通知给上层应⽤程序。",
    "url": "/网络相关/如果已经建立连接，但是客户端出现故障怎么办.html",
    "lang": ""
  },
  {
    "title": "讲讲拆包与组包",
    "content": "--- title: 讲讲拆包与组包 --- TCP通信分包指的是将应用层数据拆分成多个小的TCP报文段进行传输的过程。TCP协议并不保证应用层数据在传输过程中的边界，因此接收端需要根据TCP报文段的头部信息来正确地重组应用层数据。分包的主要原因包括： MTU限制： 网络中的链路通常有最大传输单元（MTU）限制，超过MTU的数据需要进行分片，导致TCP分包。 拥塞控制： TCP会根据网络状况进行流量控制和拥塞控制，可能导致数据分包和合并。 窗口大小： TCP通信中的窗口大小动态变化，可能会导致数据的分包和合并。 TCP通信组包是指接收端根据接收到的TCP报文段将数据进行重组的过程。接收端需要根据TCP头部信息中的序列号和数据长度来正确地组装应用层数据。组包的主要原因包括： TCP头部信息： TCP报文段的头部包含了序列号和数据长度等信息，接收端可以根据这些信息来确定数据的边界，进行组包操作。 接收窗口： TCP通信中的接收窗口大小会动态调整，接收端根据窗口大小来接收数据，可能会导致数据的分包和合并。 2、粘包、拆包发生原因 发生TCP粘包或拆包有很多原因，现列出常见的几点，可能不全面，欢迎补充， 1、要发送的数据大于TCP发送缓冲区剩余空间大小，将会发生拆包。 2、待发送数据大于MSS（最大报文长度），TCP在传输前将进行拆包。 3、要发送的数据小于TCP发送缓冲区的大小，TCP将多次写入缓冲区的数据一次发送出去，将会发生粘包。 4、接收数据端的应用层没有及时读取接收缓冲区中的数据，将发生粘包。",
    "url": "/网络相关/讲讲拆包与组包.html",
    "lang": ""
  },
  {
    "title": "ping的原理",
    "content": "--- title: ping的原理 --- ping 命令执行的时候，源主机首先会构建一个 ICMP 回送请求消息数据包。 ICMP 数据包内包含多个字段，最重要的是两个： 第一个是类型，对于回送请求消息而言该字段为 8； 另外一个是序号，主要用于区分连续 ping 的时候发出的多个数据包。 每发出一个请求数据包，序号会自动加 1。为了能够计算往返时间 RTT，它会在报文的数据部分插入发送时间。 !image-20241013114709765 将这个数据包打个IP数据头，然后在数据链路层加入MAC头信息，然后在最底层通过网络传出去",
    "url": "/网络相关/ping的原理.html",
    "lang": ""
  },
  {
    "title": "TCP滑动窗口",
    "content": "--- title: TCP滑动窗口 --- 滑动窗口分两种：发送窗口和接收窗口。 由于TCP是全双工的，所以通信的每一端都会同时维护两种窗口 tcp会对每一个数据包进行唯一标号，两个通信端各自维护自己的序号，然后还有确认号用来表示接收方期待对方发送的下一个包的序号。并且确认号可以实现累计确认机制，表示确认所有小于当前确认号的数据包。并且还可以将确认号随着数据包一起发过去，这叫做捎带确认。 此外利用选择确认机制，可以一定程度上优化无意义的重传 如何控制累计确认的时机呢？ TCP协议中的nagle算法给出的办法是延迟确认。其大概的原理是，未确认的包达到一定量、或者达到一个时间阈值，才回复一次确认。 发送方只能发送窗口内的数据包 ，接收方只能接收窗口内的数据包 。 窗口大小是受限的，也是动态的。流量控制就是由接收方控制的、调节发送方发送速度的机制。也就是在回复时设置TCP协议头中的窗口大小。 窗口大小的初始化，是在 连接建立 过程中两端协商确定的， 在后续的传输阶段，它会因主动或被动的原因而动态变化。",
    "url": "/网络相关/TCP滑动窗口.html",
    "lang": ""
  },
  {
    "title": "命令行使用MYSQL",
    "content": "--- title: 基本SQL语句 --- 安装： shell sudo apt update sudo apt-get install mysql-server 在最新的 Ubuntu 2204 版本上，安装 MySQL 时默认不会提示你设置 root 用户密码。这是因为 MySQL 在安装时默认使用 Unix socket 身份验证方式，这意味着只有系统的 root 用户或者通过 sudo 提权的用户才可以直接访问 MySQL 的 root 用户，而不需要密码。 通过 sudo 提权访问 MySQL： 既然 MySQL 默认使用 Unix socket 身份验证方式，你可以通过 sudo 提权来访问 MySQL： sh sudo mysql -u root 这将直接以 root 用户身份登陆 MySQL。 设置 MySQL root 用户密码： 要为 MySQL 的 root 用户设置一个密码，可以按照以下步骤进行： 使用 sudo 登陆 MySQL： sh sudo mysql -u root 在 MySQL Shell 中，运行以下命令来设置 root 用户密码（将 yournewpassword 替换为你想要设置的密码）： sql ALTER USER 'root'@'localhost' IDENTIFIED WITH mysqlnativepassword BY 'yournewpassword'; FLUSH PRIVILEGES; 退出 MySQL： sql EXIT; 现在你可以使用 root 用户和设置的密码进行登录了： sh mysql -u root -p 创建新用户并赋予权限： 如果你不想以 root 用户身份使用 MySQL，可以创建一个新的 MySQL 用户并赋予其适当的权限。 sh sudo mysql -u root 然后在 MySQL Shell 中运行以下命令： sql CREATE USER 'newuser'@'localhost' IDENTIFIED BY 'userpassword'; GRANT ALL PRIVILEGES ON . TO 'newuser'@'localhost' WITH GRANT OPTION; FLUSH PRIVILEGES; 现在你可以使用新创建的用户登录 MySQL： sh mysql -u newuser -p 默认情况下，MySQL 在 Ubuntu 2204 上安装时使用 Unix socket 身份验证，这意味着你需要通过 sudo 提权来访问 MySQL。你可以通过 sudo mysql -u root 直接登录 MySQL，然后根据需要设置 root 用户密码或者创建新的 MySQL 用户。 !SQL-Joins mysql -u root -p 登录 SQL语句以;为结尾 sql show databases; 展示所有的库 create database 库名; 创建库 use 库名; 使用库，使用任何操作table指令前先use select database(); 展示当前的库 show tables; 展示当前库里的表 create table 表名(列名 类型 额外属性, ..., ); 创建表 describe 表名; 查看表的具体属性信息 show create table 表名; 查看表的创建语句 mysql insert into 表名 values(...,...,...,...); //插入新行，需要列出所有列 insert into 表名 (...,...,...,...) values(...,...,...,...); //插入新行，只需部分列 sql select from 表名; 展示表的内容 select from 表名 where 条件表达式; (等于: = ,不等于: <>) select 列名,列名... from 表名; select distinct 列名... from 表名; 可以按列去重 order by 列名 (desc)， 列名 (asc)；按序查表 like where 列名 like '%y' sql 聚集函数 count() max() min() avg() 分组聚集 select owner,count() from pet group by owner; sql 多表配合 内连接 inner join select from pet inner join event on pet.name =event.name; sql update 更新数据 update 表名 set 列名 =新值 where 条件子句 sql delete删除数据 delete from 表名 where 条件子句 sql 调整表的结构 alter table 表名 add 列名 类型; alter table 表名 drop 列名; alter table 表名 change 旧列名 新列名 类型; sql 给表改名： rename table 旧表名 to 新表名; 删除表 drop table 表名; 满足实体完整性 主键（只能有一个），唯一键：不允许有两行在这一列取相同的值 在create table的时候在列名后面 加上 primary key 主键一般选取一个和业务无关的数据 业务上有实体完整性的要求，一般用唯一键 unique key autoincrement 自增 mysql库 Mysql库安装 sudo apt install libmysqlclient-dev 会存在libmysqlclient.so和libmysqlclient.a两个库以及一些Mysql的头文件 MYSQL -> 与服务端的连接 MYSQLRES -> 服务端返回的结果 每一行的数组 MYSQLROW -> 每一行 MYSQLFIELD -> 列 c MYSQL mysqlinit(MYSQL mysql);//为MySQL连接分配资源，参数一般填NULL，数据结构MYSQL是操作资源的句柄 void mysqlclose(MYSQL mysql); //关闭MYSQL连接 MYSQL mysqlrealconnect(MYSQL mysql, const char host, const char user,const char passwd,const chardb, unsigned int port, const char unixsocket, unsigned long clientflag); //mysql填mysqlinit的返回值 //host 填 \"localhost\" //root 填 \"root\" //passwd 填 mysql用户密码 //db填 database的名字 //port 填0， unixsocket填 NULL，clientnflag 填0 const char mysqlerror(MYSQL mysql); //返回报错原因 需要加上链接选项 -lmysqlclient c++ int mysqlquery(MYSQL mysql,const char stmtstr); //返回-1 为错误 MYSQLRES res = mysqlstoreresult(MYSQL mysql);//在调用mysqlquery和mysqlrealquery之后再调用 //MYSQLRES是行的数组 MYSQLROW row = mysqlfetchrow(MYSQLRES result) mysqlnumrows(MYSQLRES result);//得到行数 mysqlnumfields(MYSQLRES result);//得到列数 注意事项： 读指令之后必须调用mysqlstoreresult 每个子线程启动之后先调用一次init和realconnect (放入临界区代码中)",
    "url": "/mysql相关/基本SQL语句.html",
    "lang": ""
  },
  {
    "title": "事务",
    "content": "--- title: 事务 --- 构成单一逻辑工作单元的操作集合，一组命令的集合。 1、原子性A：事务中的操作，要么全部都发生，要么全部都不发生。 2、一致性C：事务的执行的前后，数据的完整性保持一致。且事务执行前后数据的总和不变，或者说是保持同一个约束。 3、隔离性I：一个事务内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个事务之间不能互相干扰。与隔离级别有关系。 4、持久性D：指一个事务一旦提交，它对数据库中数据的改变就应该是永久性的 mysql start transaction/begin; commit rollback (to sp1) savepoint (sp1) release savepoint mysql show create table member; select count() from user; 概念：脏写是指当多个事务并发写同一数据时，先执行的事务所写的数据会被后写的数据覆盖 !image-20230630114849828 如果一个事务A向数据库写数据，但该事务还没提交或终止，另一个事务B就看到了事务A写入数据库的数据，这个现象我们称为脏读 !image-20230630143209624 一个事务有对同一个数据项的多次读取，但是在某前后两次读取之间，另一个事务更新该数据项，并且提交了。在后一次读取时，感知到了提交的更新。这个现象我们称为不可重复读。 !image-20230630143843945 一个事务需要进行前后两次统计，在这两次统计期间，另一个事务插入了新的符合统计条件的记录，并且提交了。导致前后两次统计的数据条数不一致 !image-20230630144300870 四个隔离等级。==读未提交 < 读已提交< 可重复读 < 可串行化。== !image-20230630145317362 不能产生脏写，演示步骤如下 !image-20230630150021155 可以产生脏读，演示方法如下： 左边会话没有提交的情况下，右边会话都可以读到更新的数据age = 30; !image-20230630150201050 可以产生不可重复读，演示如下： 左边会话将事务提交，然后右边会话发现第一次读的时候age = 50, 现在age = 30,前后两次读的时候，数据是不一样的。 !image-20230630150336706 可以产生幻读，演示顺序： !image-20230630150808009 !image-20230630150858162 不可以产生脏写与脏读，但是可以产生不可重复读与幻读。 可以避免脏写、脏读、不可重复读，但是可以产生幻读。（借助了主键唯一性的特点） !image-20230630152849553 !image-20230630153029562 脏写、脏读、不可重复读、幻读都可以避免。 !image-20230630153416305 mysql select @@session.transactionisolation; select @@txisolation; select @@session.transactionisolation; set session transaction isolation level read uncommitted; set session transaction isolation level read committed; set session transaction isolation level repeatable read; set session transaction isolation level serializable; mysql利用MVCC无锁机制。 数据库表中除了原数据的这些列以外，还维护了3个我们看不到的隐藏列 DBTRXID 当前这条数据被哪个事务维护了 DBROLLPTR 存储这条新数据的老数据的指针，一旦需要更新的操作需要回滚，就会利用这个指针来进行数据回滚，这些历史数据存在了undolog里面，以链表的方式进行存储 在没有维护主键ID的时候，会维护一个ROWID的隐藏列。利用这个来建主键索引 创建一个readview快照信息，再通过这个readview去找undolog里的每一条信息。按规则一个个找。 MVCC就是乐观锁的一种体现 读未提交无需锁无需MVCC, 因为修改数据改源数据，会出现脏读 读已提交 每次查询都会创建Readview读取数据 可重复读 同样的查询只会第一次创建Readview快照 串行化直接使用的表锁，或者是读写锁 对于「读提交」和「可重复读」隔离级别的事务来说，它们是通过 Read View 来实现的，它们的区别在于创建 Read View 的时机不同，大家可以把 Read View 理解成一个数据快照，就像相机拍照那样，定格某一时刻的风景。「读提交」隔离级别是在「每个语句执行前」都会重新生成一个 Read View，而「可重复读」隔离级别是「启动事务时」生成一个 Read View，然后整个事务期间都在用这个 Read View 1、MySQL 支持4种隔离级别，默认为 RR (repeatable read)，Oracle 只支持 read committed 和 serializable 两种隔离级别，默认为 read committed. 2、随着隔离等级的提高，并发程度是越来越低的，并发产生的问题是越来越少的。隔离级别越低，并发程度越高，产生的问题越多。",
    "url": "/mysql相关/事务.html",
    "lang": ""
  },
  {
    "title": "索引",
    "content": "--- title: 索引 --- 帮助MySQL高效获取数据的数据结构 !image-20230630160308180 数据库中的数据存储在磁盘里面。 顺序存储：对空间的要求比较高，需要连续的大片空间。查找的时间复杂度O(N),删除插入数据需要移动数据 二分查找（折半查找）：也需要连续的空间，对空间的要求比较高。查找的时间复杂度O(logN) 哈希：查找的速度是比较快。但是有哈希冲突。key值哈希之后是没有顺序的，不能支持范围查找。 二叉树：数据量比较大的时候，树的高度是比较高的。树的高度过高，那么每次比较的时候，都会进行磁盘IO，而磁盘IO的速率是比较慢的，那么树的高度会影响速率。查找的时间复杂度O（logN） ==B树：每个结点中存放数据key与value值，同时还会存放指向孩子结点的指针，而每个大结点是有大小限制的，那么就会限制一个结点中索引的个数，会导致树的高度更高。（相对于B+树而言的）。== ==B+树：每个非叶子结点只存放数据key信息，不存放value信息，这样可以尽量的将一个结点中索引的条数增加，那么就可以尽量的减少树的高度，也就减少了磁盘IO的次数。== 为什么不使用跳表呢？ 总结：对空间的连续性、对范围查找、对磁盘IO的次数、查找的时间复杂度。 最终，MySQL数据库中的索引使用的是B+树这样的数据结构。 !image-20230630163036005 !image-20230630163017013 主键索引：以主键创建的索引 非主键索引（辅助索引）：以非主键创建的索引。唯一索引、普通索引、组合索引、全文索引。 主键：==非空、唯一。== mysql show index from member; create table test1 (id int, age int, math float, primary key(id)); alter table test2 add primary key(id); 特点：==唯一，但是可以为空== mysql create table test3 (id int, age int, math float, unique mathidx(math)); alter table test4 add unique index mathidx(math); create unique index ageidx on test4(age); mysql create table test3 (id int, age int, math float, mathidx(math)); alter table test4 add index mathidx(math); create index ageidx on test4(age); 将几个列合在一起创建索引 mysql create table test3 (id int, age int, math float, mathageidx(math,age)); alter table test4 add index mathageidx(math,age); create index agemathidx on test4(age,math); mysql alter table test4 drop index mathidx; drop index ageidx on test4; 在组合索引中，哪一列放在前面还是后面，是有区别的，r如果以数学成绩与人名建组合索引，比如:（math,name）,那么会先按照math进行排序，然后在数学成绩相同的情况再按照name进行排序，那么单独使用name进行查找的时候，就用不到索引。但是如果建组合索引的时候，使用（name，math）那么会先按照name进行排序，然后在名字相同的情况再按照math进行排序，那么单独使用math进行查找的时候，就用不到索引。在最左边的列相同的情况，右边的列才局部有序，如果整体来看的话，右边的列是没有顺序的。 !image-20230630173545550 优点：==提高查询速率，减少IO成本。== 缺点：1、索引也会占用空间。2、更新索引也会耗费时间，会影响表的更新速率 !image-20230703101618738 !image-20230703102128436 如果使用主键进行查询，那么select后面的所有列都可以在主键索引树上查到。但是，如果查询的时候，查询条件使用的是辅助索引，那么select后面的列，如果使用的是（星号），那么在辅助索引树的叶子结点下面，有些列是无法查到的，因为叶子结点下面的data信息中存放的是该条数据的主键。如果select后面的列不是主键也不是查询的列（例如此处的age），那么就需要在辅助索引树下面找到主键，然后再通过主键在主键索引树上将待查询的列找到，就这种现象称为回表。如果待查询的列在辅助索引树上可以命中的，就称为索引覆盖（覆盖索引） mysql //id = primary key name = index select age from tableName where id = 18; select age from tableName where name = 'Alice'; select id,name from tableName where name = 'Alice'; select id,name from tableName where age = 77; !image-20230703110052128 !image-20230703110159024 额外：索引下推（MySQL5.6推出的查询优化策略) 索引下推是虽然某些列无法使用到联合索引，但是它包含在联合索引中，所以可以直接在存储引擎中过滤出来满足条件的记录，如果找到了，就执行回表操作获取整个记录。如果没有索引下推的话 ，找到一个满足第一个条件但是不满足第二个条件的也进行回表将记录传给Server层，然后Server层再进行判断，这样可能会出现多次回表，因为满足第一个条件的记录可能比较多，每个需要回表一次然后判断一次。 当你的查询语句的执行计划里，出现了 Extra 为 Using index condition，那么说明使用了索引下推的优化。",
    "url": "/mysql相关/索引.html",
    "lang": ""
  },
  {
    "title": "引擎",
    "content": "--- title: 引擎 --- !image-20230630174705251 大体来说，MySQL可以分为 Server 层和存储引擎层。 Server 层包括连接器、查询缓存、解析器、优化器和执行器等，涵盖了 MySQL 大多数核心服务功能。 存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory等多个存储引擎 《高性能MySQL》 作用：存储和提取数据。插件式的。 !image-20230701095445674 !image-20230701094921683 !image-20230701094855065 有外键依赖的表，进行删除与插入的时候，需要注意。 !image-20230701094834320 MySQL 5.5之前默认的存储引擎。 特点： a. 查询速度很快 b. 支持表锁 c. 支持全文索引(正排索引、倒排索引) d. 不支持事务 使用MyISAM 存储表，会生成三个文件. .frm # 存储表结构，是任何存储引擎都有的 .myd # 存放数据 .myi # 存放索引 索引和数据是分开的，叫做非聚集索引 MySQL 5.5以及以后版本默认的存储引擎。没有特殊应用，推荐使用InnoDB引擎。 特点： a. 支持事务（具备ACID四大特征） b. 支持行锁和表锁（默认支持行锁） c. 支持MVCC(多版本并发控制)，版本号。 d. 支持崩溃恢复(redo log) e. 支持外键一致性约束（外键依赖） 使用 InnoDB 存储表，会生成两个文件. .frm # 存储表结构，是任何存储引擎都有的 .ibd # 存放数据和索引 !image-20230701101640374 !image-20230701102036994 索引和数据是分开存放的，这样的索引叫非聚集索引。 索引和数据存放在一起，这样的索引叫聚集索引。 特点： a. 所有数据都存放在内存中，因此数据库重启后会丢失 b. 支持表锁 c. 支持Hash和BTree索引 d. 不支持Blob（大的二进制信息）和Text字段（大的文本信息） mysql create temporary table test1 (id int, age int, math float, primary key(id)); create table test2 (id int, age int, math float)ENGINE=memory; drop命令可以删除数据库、删除表。是将数据库的名字、表的名字都删除了。比如表，会将表的结构、表中的数据都删除。 delete命令可以删除表中的数据，但是删除不了表中结构，更不能删除表的名字。 !image-20230701111148440",
    "url": "/mysql相关/引擎.html",
    "lang": ""
  },
  {
    "title": "锁",
    "content": "--- title: 锁 --- 对数据操作的粒度划分 ==表级锁==：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低。 ==行级锁==：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 从对数据操作的类型划分： ==读锁（共享锁）==：同一份数据，多个读操作可以同时进行而互不影响。 ==写锁（排它锁）==：当前操作没有完成之前，它会阻断其他读锁和写锁。 mysql lock table 表名 read; (加读锁) lock table 表名 write; (加写锁) 2.1、左边会话将某张表加了读锁之后，右边会话可以进行读该表，但是不能进行写 !image-20230701112635457 2.2、左边会话对某张表加了读锁之后，本会话对本表可以进行读但是不能写 !image-20230701112903572 2.3、左边会话对某张表加了读锁之后，右边会话也可以对同一张表加读锁。 !image-20230701113411645 可以查看锁的占用情况 show open tables !image-20230701113227985 2.4、左边会话对testmyisam加了读锁之后，左边会话可以读testmyisam这张表，但是不能读testmyisam之外的表，除非先对其他表加读锁，才能执行读操作（一心不能二用） 2.5、左边会话加写锁的特点 !image-20230701114225579 总结：读共享，写排它。 mysql SELECT ... LOCK IN SHARE MODE; #加读锁 SELECT ... FOR UPDATE;#加写锁 !image-20230701115341477 锁的是一个间隙（范围），就不能在间隙中进行写操作。 !image-20230701120037474 在线上在执行 update、delete、select ... for update 等具有加锁性质的语句，一定要检查语句是否走了索引，如果是全表扫描的话，会对每一个索引加 next-key 锁，相当于把整个表锁住了",
    "url": "/mysql相关/锁.html",
    "lang": ""
  },
  {
    "title": "范式",
    "content": "--- title: 范式 --- 范式：设计数据库时需要遵循的规则 第一范式：确保每列保持原子性，不能再分。 第二范式：确保表中的每列都和主键完全相关，就是说完全通过主键确定其他列的信息。 第三范式：确保每列都和主键列直接相关,而不是间接相关，不存在传递依赖。不能出现主键A决定B，然后B决定C的情况 优点： 1）可以尽量减少数据冗余 2）范式化的表通常比反范式化的表更小 3）范式化的数据库更新起来更加快； 不足： 1）范式化的表，在查询的时候经常需要很多join关联,增加查询的代价。 2）更难进行索引优化 允许表中有少量的冗余信息。 优点： 1）可以减少表的关联 2）可以更好的进行索引优化 缺点： 1）存在数据冗余及数据维护异常 2）对数据的修改需要更多的成本 1、范式设计与反范式设计互为优缺点（范式设计的优点是反范式的缺点，范式设计的缺点是反范式的优点） 2、如果数据库中的表更新比较频繁，就使用范式设计，如果更多的在数据的查询上面，就使用反范式设计",
    "url": "/mysql相关/范式.html",
    "lang": ""
  },
  {
    "title": "慢查询和执行计划",
    "content": "--- title: 慢查询和执行计划 --- 记录了查询比较慢（执行时间长）的SQL的日志。 mysql mysql> show variables like \"%longquery%\"; #慢查询日志的时间上限值 +-----------------+----------+ | Variablename | Value | +-----------------+----------+ | longquerytime | 1.000000 | +-----------------+----------+ 1 row in set (0.00 sec) mysql> show variables like \"%slow%\"; #慢查询日志的开关情况以及存放文件的路径 +---------------------------+---------------------------------+ | Variablename | Value | +---------------------------+---------------------------------+ | logslowadminstatements | OFF | | logslowslavestatements | OFF | | slowlaunchtime | 2 | | slowquerylog | ON | | slowquerylogfile | /var/lib/mysql/wangdao-slow.log | +---------------------------+---------------------------------+ 5 rows in set (0.01 sec) mysql> set global longquerytime=0.1;#设置慢查询日志的时间上限 set global slowquerylog=on;#打开慢查询日志的开关 命令：==explain + SQL== !image-20230703114638150 !image-20230703114735883 可以根据id的大小，确定那个表先被访问。 当id相同的时候，按照从上到下的顺序执行；当id不同的时候，按照id从大到小的顺序执行；当id既有相同也有不同的时候，会先按照==id大的先执行==，然后当id相同的时候，按照从上到下执行。 !image-20230703143852027 !image-20230703150919958 mysql student CREATE TABLE student ( id int(11) NOT NULL, name varchar(20) DEFAULT NULL, age int(11) DEFAULT NULL, c1 varchar(20) DEFAULT NULL, c2 varchar(20) DEFAULT NULL, c3 varchar(20) DEFAULT NULL, dtime datetime DEFAULT NULL, KEY id (id) USING BTREE, KEY c1c2c3idx (c1,c2,c3) ) ENGINE=InnoDB DEFAULT CHARSET=latin1 总结： system 最快：不进行磁盘 IO，在内存中查询。 const：PK 或者 unique 上的等值查询 （where id = 1） eqref：PK 或者 unique 上的 join 查询，等值匹配，对于前表的每一行，后表只有一行命中 ref：非唯一索引，等值匹配，可能有多行命中 range：索引上的范围扫描，例如：between、in、> index：索引上的全集扫描，例如：InnoDB 的 count ALL 最慢：全表扫描 possiblekeys：可能用到的索引。 key：实际用到的索引。 kenlen：索引字段的最大可能长度 keylen= n 字符集(latin1 = 1, gbk = 2, utf8 = 3, utf8mb4 = 4) + varchar（额外占用两个字节） + NULL（额外占用一个字节） datetime类型在5.6中字段长度是5个字节，datetime类型在5.5中字段长度是8个字节 !image-20230703164752846",
    "url": "/mysql相关/慢查询和执行计划.html",
    "lang": ""
  },
  {
    "title": "索引优化",
    "content": "--- title: 索引优化 --- sql 1.不在索引列上做任何操作（计算，函数等等），会导致索引失效 2.慎用不等于号，会使索引失效 3.存储引擎不能使用索引中范围条件右边的列 只访问索引的查询：索引列和查询列一致，尽量用覆盖索引，减少select NULL/NOT NULL的可能影响：c1允许为空，作为where的查询条件不会使索引失效；c2不允许为空，c2 is null is not null都不会用到索引 字符串类型加引号，不加引号会索引失效 UNION的效率比or更好 8.like查询要当心，就是是否满足最左前缀 ​ 频繁作为查询条件的字段应该创建索引 ​ 与其他表关联的字段应该创建索引 ​ 经常用于排序和分段的字段 ​ 数据量太少 ​ 经常更新的表 ​ 数据字段中包含太多的重复值 mysql insert into student values (6, '19',34, '19', '19', '19', '2015-02-12 10:10:00'); show create table student; explain select from student where id = 3; explain select from student where id + 1 = 4; show create table student; explain select from student where c1 = 'wuhan'; explain select from student where c1 <> 'wuhan'; explain select c1 from student where c1 <> 'wuhan'; explain select c1,c2 from student where c1 <> 'wuhan'; explain select from student where c1 > 'wuhan' or c1 < 'wuhan'; explain select c1 from student where c1 > 'wuhan' or c1 < 'wuhan'; explain select c1 from student where c1 > 'wuhan' UNION select c1 from student where c1 < 'wuhan'; explain select from student where c1 = 'wuhan' and c2 > 'c' and c3 = 'wangdao'; explain select from student where c1 = 'wuhan' and c2 like 'c%' and c3 = 'wangdao'; explain select c1,c2 from student where c1 = 'wuhan'; explain select from student where c1 = 'wuhan'; explain select c1,c2 from student where c2 ='wuhan'; explain select from student where c1 = '19'; explain select from student where c1 = 19; explain select from student where c1 = '19'; explain select from student where c1 = 'wuhan'; explain select from student where c1 = '19' or c1 = 'wuhan'; explain select from student where c1 = '19' union select from student where c1 = 'wuhan'; ==前缀索引优化== 使用前缀索引是为了减小索引字段大小，可以增加一个索引页中存储的索引值，有效提高索引的查询速度。在一些大字符串的字段作为索引时，使用前缀索引可以帮助我们减小索引项的大小。 不过前缀索引有一定的局限性，例如：order by就无法使用前缀索引；无法把前缀索引用作覆盖索引 ==覆盖索引优化== 利用覆盖索引可以避免回表的操作，建立一个联合索引与主键和所需要的列。 ==主键索引最好是自增的== InnoDB 创建主键索引默认为聚簇索引，数据被存放在了 B+Tree 的叶子节点上。也就是说，同一个叶子节点内的各个数据是按主键顺序存放的，因此，每当有一条新的数据插入时，数据库会根据主键将其插入到对应的叶子节点中。 如果我们使用自增主键，那么每次插入的新数据就会按顺序添加到当前索引节点的位置，不需要移动已有的数据，当页面写满，就会自动开辟一个新页面。因为每次插入一条新记录，都是追加操作，不需要重新移动数据，因此这种插入数据的方法效率非常高。 如果我们使用非自增主键，由于每次插入主键的索引值都是随机的，因此每次插入新的数据时，就可能会插入到现有数据页中间的某个位置，这将不得不移动其它数据来满足新数据的插入，甚至需要从一个页面复制数据到另外一个页面，我们通常将这种情况称为页分裂。页分裂还有可能会造成大量的内存碎片，导致索引结构不紧凑，从而影响查询效率 另外，主键字段的长度不要太大，==因为主键字段长度越小，意味着二级索引的叶子节点越小==（二级索引的叶子节点存放的数据是主键值），这样二级索引占用的空间也就越小 ==索引最好是NOT NULL== 第一原因：==索引列存在 NULL 就会导致优化器在做索引选择的时候更加复杂，更加难以优化==，因为可为 NULL 的列会使索引、索引统计和值比较都更复杂，比如进行索引统计时，count 会省略值为NULL 的行。 第二个原因：NULL 值是一个没意义的值，但是它会占用物理空间，所以会带来的存储空间的问题，因为 InnoDB 存储记录的时候，如果表中存在允许为 NULL 的字段，那么行格式中至少会用1字节空间来存储NULL值列表。 ==防止索引失效== 不在索引列上做任何计算或者函数操作，会导致索引失效 慎用不等于号，会使索引失效 存储引擎不能使用索引中范围条件>或者<右边的列 只访问索引的查询：索引列和查询列一致，尽量用覆盖索引，减少select NULL/NOT NULL的可能影响：c1允许为空，作为where的查询条件不会使索引失效；c2不允许为空，c2 is null is not null都不会用到索引 字符串类型加引号，不加引号会索引失效 UNION的效率比or更好 like查询要当心，就是是否满足最左前缀 在where子句中，如果在or前的条件列是索引列，而在or后的条件列不是索引列，那么索引会失效。 注意避免使用Using filesort，当查询语句中包含group by 操作，而且无法利用索引完成排序操作的时候，就需要利用相应的排序算法了。甚至会文件排序",
    "url": "/mysql相关/索引优化.html",
    "lang": ""
  },
  {
    "title": "MySQL主从复制原理",
    "content": "--- title: 主从复制 --- 1、在业务复杂的系统中，有这么一个情景，有一句sql语句需要锁表，导致暂时不能使用读的服务，那么就很影响运行中的业务，使用==主从复制，让主库负责写，从库负责读，这样，即使主库出现了锁表的情景，通过读从库也可以保证业务的正常运作。== 2、做==数据的热备份== 3、==架构的扩展==。业务量越来越大，I/O访问频率过高，单机无法满足，此时做==多库的存储，降低磁盘I/O访问的频率，提高单个机器的I/O性能。== MySQL 主从复制是指==数据可以从一个MySQL数据库服务器主节点复制到一个或多个从节点==。MySQL 默认采用异步复制方式，这样从 节点不用一直访问主服务器来更新自己的数据，数据的更新可以在远程连接上进行，从节点可以复制主数据库中的所有数据库或者特 定的数据库，或者特定的表。 !image-20240923085552118 !image-20240915100526272 原理： （1）master服务器将数据的改变记录二进制binlog日志，当master上的数据发生改变时，则将其改变写入二进制日志中； （2）slave服务器会在一定时间间隔内对master二进制日志进行探测其是否发生改变，如果发生改变，则开始一个I/OThread请求 master二进制事件 （3）同时主节点为每个I/O线程启动一个dump线程，用于向其发送二进制事件，并保存至从节点本地的中继日志relay log中，从节点将启动SQL线程从中继日志中读取二进制日志，在本地回放到binlog，使得其数据和主节点的保持一致，最后I/OThread和SQLThread将进入睡眠状态，等待下一次被唤醒。 也就是说： 从库会生成两个线程,一个I/O线程,一个SQL线程;I/O线程会去请求主库的binlog,并将得到的binlog写到本地的relay-log(中继日志)文件中;主库会生成一个log dump线程,用来给从库I/O线程传binlog; SQL线程,会读取relay log文件中的日志,并解析成sql语句逐一执行; 注意： 1--master将操作语句记录到binlog日志中，然后授予slave远程连接的权限（master一定要开启binlog二进制日志功能；通常为了数 据安全考虑，slave也开启binlog功能）。 2--slave开启两个线程：IO线程和SQL线程。其中：IO线程负责读取master的binlog内容到中继日志relay log里；SQL线程负责从relaylog日志里读出binlog内容，并更新到slave的数据库里，这样就能保证slave数据和master数据保持一致了。 3--Mysql复制至少需要两个Mysql的服务，当然Mysql服务可以分布 在不同的服务器上，也可以在一台服务器上启动多个服务。 4--Mysql复制最好确保master和slave服务器上的Mysql版本相同 （如果不能满足版本一致，那么要保证master主节点的版本低于 slave从节点的版本） 5--master和slave两节点间时间需同步 具体步骤： 1、从库通过手工执行change master to 语句连接主库，提供了连 接的用户一切条件（user 、password、port、ip），并且让从库 知道，二进制日志的起点位置（file名 position 号）； start slave 2、从库的IO线程和主库的dump线程建立连接。 3、从库根据change master to 语句提供的file名和position号， IO线程向主库发起binlog的请求。 4、主库dump线程根据从库的请求，将本地binlog以events的方式 发给从库IO线程。 5、从库IO线程接收binlog events，并存放到本地relay-log中，传 送过来的信息，会记录到master.info中 6、从库SQL线程应用relay-log，并且把应用过的记录到relaylog.info中，默认情况下，已经应用过的relay 会自动被清理purge mysql的主从复制都是单线程的操作，主库对所有DDL和DML产生的日志写进binlog，由于binlog是顺序写，所以效率很高，slave的sql thread线程将主库的DDL和DML操作事件在slave中重放。DML和DDL的IO操作是随机的，不是顺序，所以成本要高很多， 另一方面，由于sql thread也是单线程的，当主库的并发较高时，产生的DML数量超过slave的SQL thread所能处理的速度，或者当slave中有大型query语句产生了锁等待，那么延时就产生了。 解决方案： 1.业务的持久化层的实现采用分库架构，mysql服务可平行扩展，分散压力。 2.单个库读写分离，一主多从，主写从读，分散压力。这样从库压力比主库高，保护主库。 3.==服务的基础架构在业务和mysql之间加入memcache或者redis的cache层。降低mysql的读压力。== 4.==不同业务的mysql物理上放在不同机器，分散压力。== 5.使用比主库更好的硬件设备作为slave，mysql压力小，延迟自然会变小。 6.使用更加强劲的硬件设备 （一）一主一从 !image-20240915100635913 （二）主主复制 !image-20240915100649510 (三）一主多从 !image-20240915100705825 （四）多主一从 !image-20240915100718220 （五）联级复制 !image-20240915100731527",
    "url": "/mysql相关/主从复制.html",
    "lang": ""
  },
  {
    "title": "主从复制流程",
    "content": "--- title: 主从复制流程 --- !image-20241031143330887 !img 备库 B 跟主库 A 之间维持了一个长连接。主库 A 内部有一个线程，专门用于服务备库 B 的这个长连接。一个事务日志同步的完整过程是这样的： 在备库 B 上通过 change master 命令，设置主库 A 的 IP、端口、用户名、密码，以及要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量。 在备库 B 上执行 start slave 命令，这时候备库会启动两个线程，就是图中的 iothread 和 sqlthread。 其中 iothread 负责与主库建立连接。 主库 A 校验完用户名、密码后，开始按照备库 B 传过来的位置，从本地读取 binlog，通过dump线程发给 B。 备库 B 拿到 binlog 后，写到本地文件，称为中转日志（relay log）。 sqlthread 读取中转日志，解析出日志里的命令，并执行，并回放到binlog中。",
    "url": "/mysql相关/主从复制流程.html",
    "lang": ""
  },
  {
    "title": "join语句",
    "content": "--- title: join语句 --- 第一个问题：能不能使用 join 语句？ 如果可以使用 Index Nested-Loop Join 算法，也就是说可以用上被驱动表上的索引，其实是没问题的； 如果使用 Block Nested-Loop Join 算法，扫描行数就会过多。尤其是在大表上的 join 操作，这样可能要扫描被驱动表很多次，会占用大量的系统资源。所以这种 join 尽量不要用。 所以你在判断要不要使用 join 语句时，就是看 explain 结果里面，Extra 字段里面有没有出现“Block Nested Loop”字样。 第二个问题是：如果要使用 join，应该选择大表做驱动表还是选择小表做驱动表？ 如果是 Index Nested-Loop Join 算法，应该选择小表做驱动表； 如果是 Block Nested-Loop Join 算法：在 joinbuffersize 足够大的时候，是一样的；在 joinbuffersize 不够大的时候（这种情况更常见），应该选择小表做驱动表。 所以，这个问题的结论就是，总是应该==使用小表做驱动表==。 在决定哪个表做驱动表的时候，应该是两个表按照各自的条件过滤，过滤完成之后，计算参与 join 的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表。 能否用上被驱动表的索引，对 join 语句的性能影响很大。",
    "url": "/mysql相关/join语句.html",
    "lang": ""
  },
  {
    "title": "快照读和当前读",
    "content": "--- title: 快照读和当前读 --- 简单来说，快照读就是在事务开始的时候创建了一个数据的快照，在整个事务过程中都读这个快照； 而当前读，则是每次都去读最新数据。 MySQL 在可重复读这个隔离级别下，查询的执行效果和快照读非常接近。 在RR级别下，快照读是通过MVCC(多版本控制)和undo log来实现的，当前读是通过加临键锁(next key lock)来实现的。",
    "url": "/mysql相关/快照读和当前读.html",
    "lang": ""
  },
  {
    "title": "主从同步延迟",
    "content": "--- title: 主从同步延迟 --- 因为主库的binlog是顺序写，效率很高，但是从库的sql线程将binlog进行重放的过程，sql语句的IO操作是随机的，所以成本要高很多 另一方面，因为sql线程是单线程的，主库并发比较高的时候，产生的DML数量是会超过sql线程处理的速度的。 也有可能是从库中有个查询语句发生了上锁的阻塞，也会产生延时。 解决方案： 提高从库的硬件设备 不同业务的mysql放到不同的机器上，分散压力。 在从库上加个缓存层，降低mysql的读压力",
    "url": "/mysql相关/主从同步延迟.html",
    "lang": ""
  },
  {
    "title": "事务 A",
    "content": "--- title: 可重复读隔离级别是如何避免幻读的 --- 针对==快照读==（普通 select 语句），是==通过 MVCC 方式解决了幻读==，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。 针对==当前读==（select ... for update 等语句(加锁的语句)），是==通过 next-key lock（记录锁+间隙锁）方式解决了幻读==，因为当执行 select ... for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。 快照读：单纯的select操作。读取的是快照（ReadView）中的数据，可能是历史数据 当前读：select ... for update/in share mode、update、insert、delete。读取的总是当前的最新数据 对于快照读，RR中一个事务的所有快照读读取的都是同一份快照，所以无论其他的事务怎么修改，无论是更新还是插入删除，都不会影响当前事务的快照读结果，也就不会出现不可重复读、幻读的情形。 对于当前读，你读取的行，以及行的间隙都会被加锁，直到事务提交时才会释放，其他的事务无法进行修改，所以也不会出现不可重复读、幻读的情形。 所以如果你总是进行快照读，或者总是进行当前读，是不会出现幻读的情况的。（如果同时出现快照读和当前读，则可能幻读，下文的两个场景都是这种情况） 可重复读隔离级是由 MVCC（多版本并发控制）实现的，实现的方式是启动事务后，在执行第一个查询语句后，会创建一个 Read View，后续的查询语句利用这个 Read View，通过这个 Read View 就可以在 undo log 版本链找到事务开始时的数据，所以事务过程中每次查询的数据都是一样的，即使中途有其他事务插入了新纪录，是查询不出来这条数据的，所以就很好了避免幻读问题。 MySQL 里除了普通查询是快照读，其他都是当前读，比如 update、insert、delete，这些语句执行前都会查询最新版本的数据，然后再做进一步的操作。 这很好理解，假设你要 update 一个记录，另一个事务已经 delete 这条记录并且提交事务了，这样不是会产生冲突吗，所以 update 的时候肯定要知道最新的数据。 另外，select ... for update 这种查询语句是当前读，每次执行的时候都是读取最新的数据。 接下来，我们假设select ... for update当前读是不会加锁的（实际上是会加锁的），在做一遍实验。 !img 所以，Innodb 引擎为了解决「可重复读」隔离级别使用「当前读」而造成的幻读问题，就引出了间隙锁。 可重复读隔离级别下虽然很大程度上避免了幻读，但是还是没有能完全解决幻读。 我举例一个可重复读隔离级别发生幻读现象的场景。 还是以这张表作为例子： !img 事务 A 执行查询 id = 5 的记录，此时表中是没有该记录的，所以查询不出来。 sql mysql> begin; Query OK, 0 rows affected (0.00 sec) mysql> select from tstu where id = 5; Empty set (0.01 sec) 然后事务 B 插入一条 id = 5 的记录，并且提交了事务。 sql mysql> begin; Query OK, 0 rows affected (0.00 sec) mysql> insert into tstu values(5, '小美', 18); Query OK, 1 row affected (0.00 sec) mysql> commit; Query OK, 0 rows affected (0.00 sec) 此时，事务 A 更新 id = 5 这条记录，对没错，事务 A 看不到 id = 5 这条记录，但是他去更新了这条记录，这场景确实很违和，然后再次查询 id = 5 的记录，事务 A 就能看到事务 B 插入的纪录了，幻读就是发生在这种违和的场景。 mysql mysql> update tstu set name = '小林coding' where id = 5; Query OK, 1 row affected (0.01 sec) Rows matched: 1 Changed: 1 Warnings: 0 mysql> select from tstu where id = 5; +----+--------------+------+ | id | name | age | +----+--------------+------+ | 5 | 小林coding | 18 | +----+--------------+------+ 1 row in set (0.00 sec) !img 除了上面这一种场景会发生幻读现象之外，还有下面这个场景也会发生幻读现象。 T1 时刻：事务 A 先执行「快照读语句」：select from ttest where id > 100 得到了 3 条记录。 T2 时刻：事务 B 往插入一个 id= 200 的记录并提交； T3 时刻：事务 A 再执行「当前读语句」 select from ttest where id > 100 for update 就会得到 4 条记录，此时也发生了幻读现象。 要避免这类特殊场景下发生幻读的现象的话，就是尽量在开启事务之后，马上执行 select ... for update 这类当前读的语句，因为它会对记录加 next-key lock，从而避免其他事务插入一条新记录。",
    "url": "/mysql相关/可重复读隔离级别是如何避免幻读的.html",
    "lang": ""
  },
  {
    "title": "索引的节点是按区还是页分的？",
    "content": "--- title: 索引的节点是按区还是页分的？ --- 我们知道 InnoDB 存储引擎是用 B+ 树来组织数据的。 B+ 树中叶子节点层是通过双向链表连接起来的，如果是以页为单位来分配存储空间，那么链表中相邻的两个页之间的物理位置并不是连续的，可能离得非常远，那么磁盘査询时就会有大量的随机I/0，随机 I/0是非常慢的。 解决这个问题也很简单，就是让链表中相邻的页的物理位置也相邻，这样就可以使用顺序 I/0 了，那么在范围查询(扫描叶子节点)的时候性能就会很高。 那具体怎么解决呢? 在表中数据量大的时候，为某个索引分配空间的时候就不再按照页为单位分配了，而是按照区(extent)为单位分配。每个区的大小为 1MB，对于 16KB 的页来说，连续的 64 个页会被划为一个区，这样就使得链表中相邻的页的物理位置也相邻，就能使用顺序IO了",
    "url": "/mysql相关/索引的节点是按区还是页分的？.html",
    "lang": ""
  },
  {
    "title": "意向锁的问题",
    "content": "--- title: 意向锁的问题 --- 在使用InnoDB引擎的表里对某些记录加上【共享锁】前，需要先在表级别加上一个【意向共享锁】 在使用InnoDB引擎的表里对某些记录加上【独占锁】前，需要先在表级别加上一个【意向独占锁】 普通的select是不会加行级锁的，普通的select语句是利用MVCC实现一致性读，是无锁的。 但是，当执行插入，更新，删除操作，需要先对表加上意向独占锁，然后对该记录加独占锁。 意向共享锁和意向独占锁这种表级锁不会和行级别的共享锁和独占锁发生冲突。 意向锁的目的是为了快速判断表里是否有记录被加锁。 一个事务在插入一条记录的时候，需要判断插入位置是否已被其他事务加了间隙锁（next-key lock也包括间隙锁） 如果有的话，插入操作就会发生阻塞。直到拥有间隙锁的那个事务提交为止，在此期间会生成一个插入意向锁，表明有事务想在某个区间插入新记录，但是现在处于等待状态。 插入意向锁不是意向锁，是一种特殊的间隙锁（锁住一个点），属于行级别锁",
    "url": "/mysql相关/意向锁的问题.html",
    "lang": ""
  },
  {
    "title": "MySQL如何加行级锁的？",
    "content": "--- title: MySQL如何加行级锁的？ --- 行级锁加锁规则比较复杂，不同的场景，加锁的形式是不同的。 加锁的对象是索引，加锁的基本单位是 next-key lock，它是由记录锁和间隙锁组合而成的，next-keylock 是前开后闭区间，而间隙锁是前开后开区间。 但是，next-key lock 在一些场景下会退化成记录锁或间隙锁。 那到底是什么场景呢?总结一句在能使用记录锁或者间隙锁就能避免幻读现象的场景下，next-keylock 就会退化成记录锁或间隙锁。 当我们用唯一索引进行等值查询的时候，查询的记录存不存在，加锁的规则也会不同: 当查询的记录是「存在」的，在索引树上定位到这一条记录后，将该记录的索引中的 next-key lock 会退化成「记录锁」。 当查询的记录是「不存在」的，在索引树找到第一条大于该查询记录的记录后，将该记录的索引中的next-key lock 会退化成「间隙锁」 范围查询和等值查询的加锁规则是不同的。 当唯一索引进行范围查找时，会对每一个扫描到的索引加 next-key 锁，然后如果遇到下面这些情况，当唯一索引进行范围查询时，退化成记录锁或者间隙锁: 情况一:针对「大于等于」的范围查询，因为存在等值査询的条件，那么如果等值查询的记录是存在于表中，那么该记录的索引中的 next-key 锁会退化成记录锁。 情况二:针对「小于或者小于等于」的范围查询，要看条件值的记录是否存在于表中: 当条件值的记录不在表中，那么不管是「小于」还是「小于等于」条件的范围查询，扫描到终止范围查询的记录时，该记录的索引的 next-key 锁会退化成间隙锁，其他扫描到的记录，都是在这些记录的索引上加 next-key 锁。 当条件值的记录在表中， 如果是「小于」条件的范围查询，扫描到终止范围查询的记录时，该记录的索引的 next-key 锁会退化成间隙锁，其他扫描到的记录，都是在这些记录的索引上加 next-key锁: 如果「小于等于」条件的范围查询，扫描到终止范围查询的记录时，该记录的索引 next-key 锁不会退化成间隙锁。其他扫描到的记录，都是在这些记录的索引上加 next-key 锁。 当我们用非唯一索引进行等值查询的时候，因为存在两个索引，一个是主键索引，一个是非唯一索引（辅助索引），所以在加锁时，同时会对这两个索引都加锁，但是对主键索引加锁的时候，只有满足查询条件的记录才会对它们的主键索引加锁。 针对非唯一索引等值查询时，查询的记录存不存在，加锁的规则也会不同: 当查询的记录「存在」时，由于不是唯一索引，所以肯定存在索引值相同的记录，于是非唯一索引等值查询的过程是一个扫描的过程，直到扫描到第一个不符合条件的二级索引记录就停止扫描，然后在扫描的过程中，对扫描到的二级索引记录加的是 next-key 锁，而对于第一个不符合条件的二级索引记录该二级索引的 next-key 锁会退化成间隙锁。同时，在符合查询条件的记录的主键索引上加记录锁。 当查询的记录「不存在」时，扫描到第一条不符合条件的二级索引记录，该二级索引的 next-key 锁会退化成间隙锁。因为不存在满足查询条件的记录，所以不会对主键索引加锁。 非唯一索引范围查询，索引的next-key lock不会有退化为间隙锁和记录锁的情况。 如果锁定读查询语句，没有使用索引列作为查询条件，或者查询语句没有走索引查询，导致扫描是全表扫描。那么，每一条记录的索引上都会加next-key锁，这样就相当于锁住的全表，这时如果其他事务对表进行增删改操作，都会被阻塞。 重点在于 执行update,delete,select for update等具有加锁性质的语句，一定要检查语句是否走了索引，如果是全表扫描的话，会对每一个索引加next-key锁，相当于把整个表锁住。",
    "url": "/mysql相关/MySQL如何加行级锁的？.html",
    "lang": ""
  },
  {
    "title": "mysql深分页问题",
    "content": "--- title: mysql深分页问题 --- mysql select id,name,balance from account where updatetime> '2020-09-19' limit 100000,10; 我们先来看下这个SQL的执行流程： 通过普通二级索引树idxupdatetime，过滤updatetime条件，找到满足条件的记录ID。 通过ID，回到主键索引树，找到满足记录的行，然后取出展示的列（回表） 扫描满足条件的100010行，然后扔掉前100000行，返回。 因为以上的SQL，回表了100010次，实际上，我们只需要10条数据，也就是我们只需要10次回表其实就够了。因此，我们可以通过减少回表次数来优化。 如果我们把查询条件，转移回到主键索引树，那就可以减少回表次数啦。转移到主键索引树查询的话，查询条件得改为主键id了，之前SQL的updatetime这些条件咋办呢？抽到子查询那里嘛 子查询那里怎么抽的呢？因为二级索引叶子节点是有主键ID的，所以我们直接根据updatetime来查主键ID即可，同时我们把 limit 100000的条件，也转移到子查询，完整SQL如下： mysql select id,name,balance FROM account where id >= (select a.id from account a where a.updatetime >= '2020-09-19' limit 100000, 1) LIMIT 10; 延迟关联的优化思路，跟子查询的优化思路其实是一样的：都是把条件转移到主键索引树，然后减少回表。不同点是，延迟关联使用了inner join代替子查询。 优化后的SQL如下： mysql SELECT acct1.id,acct1.name,acct1.balance FROM account acct1 INNER JOIN (SELECT a.id FROM account a WHERE a.updatetime >= '2020-09-19' ORDER BY a.updatetime LIMIT 100000, 10) AS acct2 on acct1.id= acct2.id; limit 深分页问题的本质原因就是：偏移量（offset）越大，mysql就会扫描越多的行，然后再抛弃掉。这样就导致查询性能的下降。 其实我们可以采用标签记录法，就是标记一下上次查询到哪一条了，下次再来查的时候，从该条开始往下扫描。就好像看书一样，上次看到哪里了，你就折叠一下或者夹个书签，下次来看的时候，直接就翻到啦。 假设上一次记录到100000，则SQL可以修改为： mysql select id,name,balance FROM account where id > 100000 order by id limit 10; 这样的话，后面无论翻多少页，性能都会不错的，因为命中了id索引。但是这种方式有局限性：需要一种类似连续自增的字段。 很多时候，可以将limit查询转换为已知位置的查询，这样MySQL通过范围扫描between...and，就能获得到对应的结果。 如果知道边界值为100000，100010后，就可以这样优化： mysql select id,name,balance FROM account where id between 100000 and 100010 order by id;",
    "url": "/mysql相关/mysql深分页问题.html",
    "lang": ""
  },
  {
    "title": "buffer pool",
    "content": "--- title: buffer pool --- buffer pool就是innodb引擎的一个缓冲池，来提高数据库的读写性能。 可以通过调整innodbbufferpoolsize参数来设置buffer pool的大小 buffer pool中存储 数据页，索引页，undo页，等。。。 <img src=\"..\\..\\assets/images/3013-buffer pool/bufferpool内容.drawio.png\" alt=\"img\" style=\"zoom:50%;\" /> Innodb为每一个缓存页都创建了一个控制块，控制块信息包括【缓存页的表空间、页号、缓存页地址、链表节点】 <img src=\"..\\..\\assets/images/3013-buffer pool/缓存页.drawio.png\" alt=\"img\" style=\"zoom:50%;\" /> 为了能够快速找到空闲的缓存页，可以使用链表结构，将空闲缓存页的控制块作为链表的节点，这个链表称为Free链表（空闲链表） <img src=\"..\\..\\assets/images/3013-buffer pool/freelist.drawio.png\" alt=\"img\" style=\"zoom:67%;\" /> 还有一个flush链表，与free链表类似，链表的节点也是控制块，在于它的元素都是脏页 <img src=\"..\\..\\assets/images/3013-buffer pool/bufferpollpage.png\" alt=\"img\" style=\"zoom:67%;\" /> Free Page(空闲页)，表示此页未被使用，位于 Free 链表; Clean Page(干净页)，表示此页已被使用，但是页面未发生修改，位于LRU 链表. Dirty Page(脏页)，表示此页「已被使用」且「已经被修改」，其数据和磁盘上的数据已经不一致当脏页上的数据写入磁盘后，内存数据和磁盘数据一致，那么该页就变成了干净页。脏页同时存在于LRU 链表和 Flush 链表。 简单的 LRU 算法并没有被 MySQL 使用，因为简单的 LRU 算法无法避免下面这两个问题 预读失效; Buffer Pool 污染; 如何解决呢？ 让预读的页停留在buffer pool里的时间要尽可能的短，让真正被访问的页才移动到LRU链表的头部，从而保证真正被读取的热数据留在buffer pool里的时间尽可能长。 mysql是这样做的，改进了LRU算法，将LRU划分了2个区域：old区域和young区域。这个比例一般是yound:old = 63:37 !img 划分这两区域后，预读的页只需要加入到old区域的头部，当页被真正访问的时候，才将页插入yound区域的头部。 如果预读的页一直没有被访问到，就会从Old区域移除，这样就不会影响young区域中的热点数据。 是什么：当某一个sql语句扫描了大量的数据（索引失效的时候造成全盘扫描也会），在buffer pool空间比较有限的情况下，可能会将buffer pool里的所有页都替换出去，导致大量热数据被淘汰，等这些热数据再次被访问的时候，由于缓存未命中，就会产生大量的磁盘IO，性能 会急剧下降。 mysql是这样做的，进入到young区域的条件增加了一个停留在old区域的时间判断。 如果后续的访问时间和第一次访问的时间在某个间隔内，那么该缓存也不会移动到young区域的头部； 如果后续的访问时间和第一次访问的时间不在某个间隔内，那么该缓存会移动到young区域的头部； 也就是说，只有同时满足被访问以及在old区域停留时间超过1秒(参数innodboldblockstime)这两个条件，才会进入到young区域头部。 此外，防止频繁移动，只有young区域后3/4的节点被访问了才会移动到头部。",
    "url": "/mysql相关/buffer pool.html",
    "lang": ""
  },
  {
    "title": "脏页什么时候被刷盘？",
    "content": "--- title: 脏页什么时候被刷盘？ --- 当redo log日志满了的情况，会主动触发脏页刷新到磁盘 当buffer pool空间不足时，需要将一部分数据页淘汰掉，如果淘汰的是脏页，需要先将脏页同步到磁盘； mysql认为空闲的时候，后台线程会定期将适量的脏页刷到磁盘 mysql正常关闭的时候，会把所有的脏页刷到磁盘",
    "url": "/mysql相关/脏页什么时候被刷盘？.html",
    "lang": ""
  },
  {
    "title": "可重复读隔离下的锁机制",
    "content": "--- title: 可重复读隔离下的锁机制 --- 原则 1：加锁的基本单位是 next-key lock。next-key lock 是前开后闭区间。 原则 2：查找过程中访问到的对象才会加锁。 优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。 优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。 一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。",
    "url": "/mysql相关/可重复读隔离下的锁机制.html",
    "lang": ""
  },
  {
    "title": "为什么redo log有两阶段提交？",
    "content": "--- title: 为什么redo log有两阶段提交？ --- 首先是为了保证redo log和bin log之间的逻辑一致。 事务提交后，redo log 和bin log都要持久化到磁盘，但是这两个是独立的逻辑，可能出现半成功的状态，这样就造成两份日志之间的逻辑不一致。 我们可以用反证的方式，如果采用以下操作： 先写redo log再写bin log：假设在redo log写完，bin log还没有写完的时候，mysql挂掉了。然后系统重启通过redo log进行数据恢复，但是这个时候bin log中是没有这个数据的。所以如果要用bin log来恢复临时库的话，这个库就会少了这次更新。 先写bin log再写redo log：假设在bin log写完，mysql挂掉了。然后系统重启通过bin log进行数据恢复，但是这个时候redo log中是没有这个数据的。所以使用bin log来恢复的话就会多出一次事务。 所以不用两阶段提交的话，数据库的状态和它用日志恢复的库的状态就会不一致。 分为三个时间点 1 redolog的prepare阶段 2 写binlog 3 redolog的commit 当在2之前崩溃时 重启恢复：后发现没有commit，回滚。备份恢复：没有binlog 。 此时也是一致的 当在3之前崩溃 重启恢复：虽没有commit，但满足prepare和binlog完整，所以重启后会自动commit。备份：有binlog. 是一致的",
    "url": "/mysql相关/为什么redo log有两阶段提交？.html",
    "lang": ""
  },
  {
    "title": "一条SQL查询语句是如何执行的",
    "content": "--- title: 一条SQL查询语句是如何执行的 --- 连接器:连接器负责跟客户端建立连接、获取权限、维持和管理连接。 查询缓存: MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以key-value 对的形式，被直接缓存在内存中，但是查询缓存这个功能是弊大于利的，当表只要发生更新，那么缓存就完全失效了，所以只适合用在静态表上，在mysql8.0之后，这个功能就被完全移除掉了。 分析器:你输入的是由多个字符串和空格组成的一条SQL 语句，此时需要进行词法分析和语法分析。MySQL 需要识别出里面的字符串分别是什么，代表什么。 优化器:先有预处理阶段，检查表或者字段是否存在。优化器是在表里面有多个索引的时候，决定使用哪个索引; 或者在一个语句有多表关联(join )的时候，决定各个表的连接顺序。 执行器: MySQL通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行阶段与存储引擎开始交互，开始执行语句。 但是对于更新流程的话，还会涉及到日志模块，redolog和binlog，undolog。 !img 首先呢，MYSQL的架构分为两层：Server层和存储引擎层。 Server层是负责建立连接，分析和执行SQL语句的。 1.==连接器：== 在执行语句前首先要和服务层建立连接，第一步是连接器的工作，负责跟客户端建立连接，获取权限，维持和管理连接。 2.==查询缓存== 连接器完成工作之后，MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以key-value 对的形式，被直接缓存在内存中。但是实际上对于更新频繁的表来说，查询缓存的命中率很低。所以Mysql8.0版本开始，就将查询缓存删掉了。 3.==解析器（包含在分析器中）== 之后呢是进行解析SQL，这个工作是交给解析器来做的。==解析器进行词法分析和语法分析。== SQL没有语法问题之后然后执行SQL，每条SELECT 查询语句流程主要可以分为下面这三个阶段： 预处理阶段；优化阶段；执行阶段； 4.==预处理器== 预处理阶段是检查语句中的表或者字段是否存在，并将号扩展为表的所有列。 5.==优化器== 经过预处理阶段后，还需要为 SQL 查询语句先制定一个执行计划，这个工作交由「优化器」来完成的。==优化器主要负责将 SQL 查询语句的执行方案确定下来==，比如在表里面有多个索引的时候，优化器会基于查询成本的考虑，来决定选择使用哪个索引。 6.==执行器== 经历完优化器后，就确定了执行方案，接下来 MySQL 就真正开始执行语句了，这个工作是由「执行器」完成的。在执行的过程中，执行器就会和存储引擎交互了，交互是以记录为单位的。",
    "url": "/mysql相关/一条SQL查询语句是如何执行的.html",
    "lang": ""
  },
  {
    "title": "一条SQL更新语句是如何执行的？",
    "content": "--- title: 一条SQL更新语句是如何执行的？ --- 连接器:连接器负责跟客户端建立连接、获取权限、维持和管理连接。 分析器:你输入的是由多个字符串和空格组成的一条SQL 语句，此时需要进行词法分析和语法分析。MySQL 需要识别出里面的字符串分别是什么，代表什么。 优化器:优化器是在表里面有多个索引的时候，决定使用哪个索引; 或者在一个语句有多表关联(join )的时候，决定各个表的连接顺序。 执行器: MySQL通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行阶段与存储引擎开始交互，开始执行语句。 判断数据页是否在buffer pool中，如果不在就从磁盘读入buffer pool。然后对数据页进行修改，标记为脏页，将这条更新记录到redo log中，此时redo log在prepare状态。redolog是记录了页的修改，是物理日志 执行器再生成这个操作的bin log，并把bin log写入磁盘中 提交事务后，才会将redo log改为commit状态。更新完成。 !img",
    "url": "/mysql相关/一条SQL更新语句是如何执行的？.html",
    "lang": ""
  },
  {
    "title": "事务的四大特性有哪些？",
    "content": "--- title: 事务的四大特性有哪些？ --- 事务的四大特性通常被称为 ACID 特性 原子性：确保事务的所有操作要么全部执行成功，要么全部失败回滚，不存在部分成功的情况。 ==原子性是通过 undo log（回滚日志）==来保证的； 一致性：事务在执行前后，数据库从一个一致性状态转变到另一个一致性状态，必须满足数据约束条件，不会出现矛盾的结果。 一致性则是通过持久性+原子性+隔离性来保证； 隔离性：多个事务并发执行时，每个事务都应该被隔离开来，一个事务的执行不应该影响其他事务的执行。 ==隔离性是通过 MVCC（多版本并发控制） 或锁机制==来保证的； > 并且mysql Innodb引擎中的可重复读很大程度上解决了幻读问题（mvcc和间隙锁），所以我认为mysql的可重复读和串行化才是满足了这里隔离性的要求的。 持久性：一旦事务被提交，它对数据库的改变就是永久性的，即使在系统故障或崩溃后也能够保持。 ==持久性是通过 redo log （重做日志）==来保证的；",
    "url": "/mysql相关/事务的四大特性有哪些？.html",
    "lang": ""
  },
  {
    "title": "数据库的事务隔离级别有哪些？",
    "content": "--- title: 数据库的事务隔离级别有哪些？ --- 读未提交（Read Uncommitted）： 允许一个事务读取另一个事务尚未提交的数据修改。 最低的隔离级别，存在脏读、不可重复读和幻读的问题。 读已提交（Read Committed）： 一个事务只能读取已经提交的数据。其他事务的修改在该事务提交之后才可见。 解决了脏读问题，但仍可能出现不可重复读和幻读。 可重复读（Repeatable Read）： 事务执行期间，多次读取同一数据会得到相同的结果，即在事务开始和结束之间，其他事务对数据的修改不可见。 解决了不可重复读问题，但在理论上仍可能出现幻读，所以引入了MVCC（快照读）和间隙锁（当前读）很大程度来避免了幻读问题。 序列化（Serializable）： 最高的隔离级别，确保事务之间的并发执行效果与串行执行的效果相同，即不会出现脏读、不可重复读和幻读。",
    "url": "/mysql相关/数据库的事务隔离级别有哪些？.html",
    "lang": ""
  },
  {
    "title": "MySQL的执行引擎有哪些？",
    "content": "--- title: MySQL的执行引擎有哪些？ --- MySQL的执行引擎主要负责查询的执行和数据的存储, 其执行引擎主要有MyISAM、InnoDB、Memery 等。 InnoDB引擎提供了对事务ACID的支持，还提供了行级锁和外键的约束，是目前MySQL的默认存储引擎，适用于需要事务和高并发的应用。 MyISAM引擎是早期的默认存储引擎，支持全文索引，但是不支持事务，也不支持行级锁和外键约束，适用于快速读取且数据量不大的场景。 Memery就是将数据放在内存中，访问速度快，但数据在数据库服务器重启后会丢失。",
    "url": "/mysql相关/MySQL的执行引擎有哪些？.html",
    "lang": ""
  },
  {
    "title": "MySQL为什么使用B+树来作索引",
    "content": "--- title: MySQL为什么使用B+树来作索引 --- B+树是一个B树的变种，提供了高效的数据检索、插入、删除和范围查询性能。 单点查询：B 树进行单个索引查询时，最快可以在 O(1) 的时间代价内就查到。从平均时间代价来看，会比 B+ 树稍快一些。但是 B 树的查询波动会比较大，因为每个节点既存索引又存记录，所以有时候访问到了非叶子节点就可以找到索引，而有时需要访问到叶子节点才能找到索引。B+树的非叶子节点不存放实际的记录数据，仅存放索引，所以数据量相同的情况下，相比存储即存索引又存记录的 B 树，B+树的非叶子节点可以存放更多的索引，因此 B+ 树可以比 B 树更「矮胖」，查询底层节点的磁盘 I/O次数会更少。 插入和删除效率：B+ 树有大量的冗余节点，删除一个节点的时候，可以直接从叶子节点中删除，甚至可以不动非叶子节点，删除非常快。B+ 树的插入也是一样，有冗余节点，插入可能存在节点的分裂（如果节点饱和），但是最多只涉及树的一条路径。B 树没有冗余节点，删除节点的时候非常复杂，可能涉及复杂的树的变形。 范围查询：B+ 树所有叶子节点间有一个链表进行连接，而 B 树没有将所有叶子节点用链表串联起来的结构，因此只能通过树的遍历来完成范围查询，这会涉及多个节点的磁盘 I/O 操作，范围查询效率不如 B+ 树。存在大量范围检索的场景，适合使用 B+树，比如数据库。而对于大量的单个索引查询的场景，可以考虑 B 树，比如nosql的MongoDB。 B+树更适合磁盘IO B+Tree一个节点是一个page，是一种多叉树结构，每个结点都是一个16k的数据页，能存放较多索引信息。一次IO一个page，大大节省了磁盘IO的操作。 B+Tree一个page 能存放较多索引信息 ，所以树的层数比较低， 三层左右就可以存储2kw左右的数据也就是说查询一次数据，如果这些数据页都在磁盘里，那么最多需要查询三次磁盘IO。 原生跳表不适合磁盘IO 跳表是链表结构，一条数据一个结点，那么一个node节点一次磁盘io， 一个page 页规模的IO存储的性能 估计要下降1000倍以上。 原生跳表 一个node存放一个 索引信息 ，所以树的层数比较高 如果最底层要存放2kw数据，且每次查询都要能达到二分查找的效果，2kw大概在2的24次方 左右， 所以，2kw数据的跳表大概高度在24层左右。 如果要进行查找，大概要进行 24次磁盘IO。 这里讲的是原生跳表， 如果经过各种改进，那个不在此文讨论范围。 所以，虽然在理论上，跳表的时间复杂度和B+树相同 ，但是： B+树更适合 磁盘IO， 更合适MYSQL。 从反面来说， 跳表更适合内存IO， 更适合redis。 那么，为啥 redis 用跳表而不用B+树？",
    "url": "/mysql相关/MySQL为什么使用B+树来作索引.html",
    "lang": ""
  },
  {
    "title": "说一下索引失效的场景？",
    "content": "--- title: 说一下索引失效的场景？ --- 索引失效意味着查询操作不能有效利用索引进行数据检索，从而导致性能下降，下面一些场景会发生索引失效。 使用OR条件：当使用OR连接多个条件，并且每个条件用到不同的索引列时，只要有条件列不为索引列，索引可能不会被使用。 使用非等值查询：当使用!=或<>操作符时，索引可能不会被使用，特别是当非等值条件在WHERE子句的开始部分时。 对列进行类型转换： 如果在查询中对列进行类型转换，例如将字符列转换为数字或日期，索引可能会失效。 使用LIKE左模糊语句：以通配符%开头的LIKE查询会导致索引失效。 函数或表达式：在列上使用函数或表达式作为查询条件，通常会导致索引失效。 表连接中的列类型不匹配： 如果在连接操作中涉及的两个表的列类型不匹配，索引可能会失效。例如，一个表的列是整数，另一个表的列是字符，连接时可能会导致索引失效。",
    "url": "/mysql相关/说一下索引失效的场景？.html",
    "lang": ""
  },
  {
    "title": "什么是慢查询？原因是什么？可以怎么优化？",
    "content": "--- title: 什么是慢查询？原因是什么？可以怎么优化？ --- 数据库查询的执行时间超过指定的longquerytime超时时间时，就被称为慢查询，一般记录在慢查询日志中。 原因： 查询语句比较复杂（SQL语句没写好）：查询涉及多个表，包含复杂的连接和子查询，可能导致执行时间较长。 查询数据量大：当查询的数据量庞大时，即使查询本身并不复杂，也可能导致较长的执行时间。 缺少索引(索引没有设计好)：如果查询的表没有合适的索引，需要遍历整张表才能找到结果，查询速度较慢。 MYSQL选错索引：优化器不一定每次都选到成本最低的索引，应急方案是加上force index 数据库设计不合理：数据库表设计庞大，查询时可能需要较多时间。 并发冲突：当多个查询同时访问相同的资源时，可能发生并发冲突，导致查询变慢。 硬件资源不足：如果MySQL服务器上同时运行了太多的查询，会导致服务器负载过高，从而导致查询变慢 优化： 运行语句或者查看慢查询日志，找到慢查询的sql（show variables like \"%slow%\";） 查询区分度最高的字段 使用explain来展示执行计划：显示mysql如何使用索引来处理select语句以及连接表，可以帮助选择更好的索引、写出更优化的查询语句，通过type字段来查看访问类型，如果是eqref,count,system是比较快的，ALL,index,range,ref是比较次的。 order by limit形式的sql语句，让排序的表优先查 考虑建立索引原则",
    "url": "/mysql相关/什么是慢查询？原因是什么？可以怎么优化？.html",
    "lang": ""
  },
  {
    "title": "undo log、redo log、binlog 有什么用？",
    "content": "--- title: undo log、redo log、binlog 有什么用？ --- !redologbinlog undo log是Innodb存储引擎层生成的日志，实现了事务中的原子性，主要用于事务回滚和MVCC。 redo log是Innodb存储引擎层生成的物理日志，记录了某个数据页做了什么修改，每当执行一个事务就会产生一条或者多条物理日志，并且redo log是采用顺序写，效率更好，写满了后又回到开头进行写，如果在buffer pool中修改数据直接写回磁盘的话，是一个随机写，效率就不高了，并且还引入了redo log buffer（实现组提交），具体就是先写入这个buffer中，然后再通过刷盘(0:每隔一秒写入，1:提交时就刷盘，2:写入系统缓存pagecache（这个pagecache和mysql不是同一个进程），由操作系统调度fsync)，redo log主要是恢复事务数据的（崩溃恢复），保证持久性的。 binlog (归档日志）是整个mysql的Server 层生成的逻辑日志，记录所有日志，主要用于数据备份和主从复制。binlog也可以设置参数实现刷盘(0:每次提交事务只write,每隔一秒刷盘，1:提交时写入刷盘，N:写入系统缓存pagecache，积累N次事务再刷盘)。 你要对 undo log、redo log 的必要性有深刻理解。因为面试官可能会问出一些反直觉的问题。 没有 undo log 会怎样？没有 undo log 就没有后悔药，没有办法回滚。 没有 redo log 会怎样？没有 redo log 的话，写到 buffer pool，宕机了就会丢失数据。 为什么非得引入 redo log，干嘛不直接修改数据？直接修改数据就是随机写，性能极差。",
    "url": "/mysql相关/undo log、redo log、binlog 有什么用？.html",
    "lang": ""
  },
  {
    "title": "MySQL和Redis的区别是什么",
    "content": "--- title: MySQL和Redis的区别是什么 --- Redis是非关系型数据库，基于键值对，支持多种数据结构；而MySQL是一种关系型数据库，使用表来组织数据。 Redis将数据存在内存中，通过持久化机制将数据写入磁盘，MySQL通常将数据存储在磁盘上。 Redis不使用SQL，而是使用自己的命令集，MySQL使用SQL来进行数据查询和操作。 Redis以高性能和低延迟为目标，适用于读多写少的应用场景，MySQL 适用于需要支持复杂查询、事务处理、拥有大规模数据集的场景。 Redis 更适合处理高速、高并发的数据访问，以及需要复杂数据结构和功能的场景，在实际应用中，很多系统会同时使用 MySQL 和 Redis，将redis作为缓存使用。",
    "url": "/mysql相关/MySQL和Redis的区别是什么.html",
    "lang": ""
  },
  {
    "title": "全表扫描和全主键扫描",
    "content": "--- title: 全表扫描和全主键扫描 --- 从执行流程来看，全表扫描和全主键扫描都需要从主键索引中读取数据。 InnoDB每读取一条记录，都需要把该记录所属的数据页从磁盘上的表空间文件读取到 Bufer Pool中(如果数据页已在Buffer Pool中，不需要重复读取)。 全表扫描需要读取表中所有记录，全主键扫描需要读取主键索引的所有记录，由于InnoDB表的数据和主键索引合二为·了，两者都会把主键索引中所有叶子结点数据页全部读取到 Buffer Pool中。 从这点可见，InnoDB全主键扫描的执行效率并不会比全表扫描高。 全表扫描和和全主键扫描读取时，都需要先定位到主键索引第1个叶子结点数据页中的第1条用户记录，然后沿着第1条用户记录依次读取，直到读完主键索引中的所有记录(或者说表中的所有记录)。 所以，我们可以认为全表扫描和全主键扫描本质上是一样的。既然如此，执行计划中为什么要区分全表扫描和全主键扫描呢? 这是因为 MySQL支持多种存储引擎，对于使用堆表的存储引擎(例如MyISAM)，因为表中数据和索引是分开存储的,全表扫描和全主键扫描确实不同。 server 层确定执行计划时，对于所有存储引擎一视同仁，InnoDB自然也就区分全表扫描和全主键扫描了。",
    "url": "/mysql相关/全表扫描和全主键扫描.html",
    "lang": ""
  },
  {
    "title": "索引下推",
    "content": "--- title: 索引下推 --- 索引下推 (lndex Condition pushdown，简称ICP)是MySQL 5.6 版本引入的-种索引优化技术，它可以在索引遍历过程中，==某些列无法使用到联合索引，但是它包含在联合索引中==，就对索引中包含的所有字段先做判断，过滤掉不符合条件的记录之后再回表，可以有效的减少回表次数。 当你的查询语句的执行计划里，出现了 Extra 为 Using index condition，那么说明使用了索引下推的优化。 索引下推的下推其实就是指将==部分上层（服务层）负责的事情，交给了下层（引擎层）去处理==。 我们来具体看一下，在没有使用ICP的情况下，MySQL的查询： 存储引擎读取索引记录； 根据索引中的主键值，定位并读取完整的行记录； 存储引擎把记录交给Server层去检测该记录是否满足WHERE条件。 使用ICP的情况下 存储引擎读取索引记录（不是完整的行记录）； 判断条件部分能否用索引中的列来做检查，条件不满足，则处理下一行索引记录； 条件满足，使用索引中的主键去定位并读取完整的行记录（就是所谓的回表）； 存储引擎把记录交给服务层，服务层检测该记录是否满足条件的其余部分。 mysql 联合索引：【name + age + position】 SELECT FROM employees WHERE name like 'wei66%' AND age =22 AND position ='manager 对于联合索引(name,age,position)，正常情况按照最左前缀原则，以上SQL语句只会走name字段索引。因为根据name字段过滤完，得到的索引行里的age和position是无序的，无法很好的利用索引。 在MySQL5.6之前的版本，这个查询只能在联合索引里匹配到名字是wei66开头的索引，然后拿这些索引对应的主键逐个回 表，到主键索引上找出相应的记录，再比对age和position这两个字段的值是否符合。 但是在MySQL5.6及之后的版本使用了索引下推优化后，上面那个查询在联合索引里匹配到名字是wei66开头的索引之后，同时还会在索引里过滤age和position这两个字段，如下图，明显是使用到了索引的这3个字段。最后拿着过滤完剩下的索引对应的主键id再回表查整行数据。",
    "url": "/mysql相关/索引下推.html",
    "lang": ""
  },
  {
    "title": "什么时候索引失效反而效果更好",
    "content": "--- title: 什么时候索引失效反而效果更好 --- 用索引去查如果是查的辅助索引，可能会有回表操作，优化器在做执行计划的时候会计算成本判断走不走索引或者走哪个索引。 小表查询： 读取大部分或所有行： 低区分度的字段： 复杂查询：对于复杂的多表连接查询，优化器有时候可以选择执行计划不使用某个索引",
    "url": "/mysql相关/什么时候索引失效反而效果更好.html",
    "lang": ""
  },
  {
    "title": "为什么不推荐外键",
    "content": "--- title: 为什么不推荐外键 --- 性能影响：插入和删除操作变慢，外键约束要求数据库在插入和删除期间做额外的完整性检查，这会导致性能下降。大批量的插入或者更新操作也会因为外键约束变得更加耗时。 灵活性受限：数据库迁移重构困难，使用外键可能会使数据库结构变更更加复杂，因为外键增加了表之间的依赖性。 复杂管理：事务复杂性，外键可能会导致更加复杂的事务管理。 分布式系统的局限：在微服务架构或者分布式数据库中，可能不便于使用外键，因为不同的服务或数据库实例可以在不同的节点上运行，会有一致性的问题。 应用层管理：所以一般时在应用层通过业务逻辑来维护数据一致性，而不是依赖外键约束。 何时使用外键呢？ 明确的数据完整性需求：当确保数据完整性至关重要时，外键约束可以提供可靠的保障。 数据模型简单：在简单的数据模型中，外键对于维护基本的数据关系是有效的。 开发效率优先：对于小型项目或者开发效率优先的项目，外键可以简化数据完整性管理。",
    "url": "/mysql相关/为什么不推荐外键.html",
    "lang": ""
  },
  {
    "title": "联合索引的技巧",
    "content": "--- title: 联合索引的技巧 --- 覆盖索引：如果查询条件使用的是普通索引（或是联合索引的最左原则字段），查询结果是联合索引的字段或是主键，不用回表操作，直接返回结果，减少IO磁盘读写读取正行数据 最左前缀：联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符 设置查找频繁数据靠左：根据创建联合索引的顺序，以最左原则进行where检索，比如（age，name）以age=1 或 age= 1 and name=‘张三’可以使用索引，单以name=‘张三’ 不会使用索引，考虑到存储空间的问题，还请==根据业务需求，将查找频繁的数据进行靠左创建索引== 建立联合索引的时候，区分度大的字段排在前面 索引下推：like 'hello%’and age >10 检索，MySQL5.6版本之前，会对匹配的数据进行回表查询。5.6版本后，会先过滤掉age<10的数据，再进行回表查询，减少回表率，提升检索速度 联合索引注意遇到范围查询（> 和<）就会停止匹配 给经常需要组合使用where ... order by ....的字段建立联合索引。",
    "url": "/mysql相关/联合索引的技巧.html",
    "lang": ""
  },
  {
    "title": "什么是两阶段锁",
    "content": "--- title: 什么是两阶段锁 --- 在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到==事务结束时才释放==。这个就是两阶段锁协议。 这个设定，对我们使用事务有什么帮助呢？那就是，如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。",
    "url": "/mysql相关/什么是两阶段锁.html",
    "lang": ""
  },
  {
    "title": "change buffer是什么",
    "content": "--- title: change buffer是什么 --- 当我们需要更新一个数据的时候，我们是直接操作对应的数据页的，如果这个数据页在buffer pool中可以直接更新。 而如果这个数据页没有在buffer pool中时，InnoDB会将这些更新操作缓存在change buffer中，这样就不需要从磁盘中读这个数据页了。 在下次查询需要访问这个数据页的时候，将对应的数据页读入buffer pool中，再执行change buffer中与这个页有关的操作，这个操作就叫做change buffer的merge。 然后除了访问这个数据页会触发merge外，有后台线程会定期merge。 而唯一索引是不使用change buffer的，因为对于唯一索引来说，所有更新操作都得判断是否违反唯一性约束。所以这就必须先将对应数据页读入到buffer pool中才能判断。这样就没必要使用change buffer了。 并且change buffer也是可以持久化的。每次merge还会将change buffer的变更也写入到redo log中。 可以简单地对比这redo log和change buffer机制在提升更新性能上的收益的话，redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。 ------------------------------------------ 当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。 在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。 通过这种方式就能保证这个数据逻辑的正确性。需要说明的是，虽然名字叫作 change buffer，实际上它是可以持久化的数据。也就是说，change buffer 在内存中有拷贝，也会被写入到磁盘上。 将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。 除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。 显然，如果能够将更新操作先记录在 change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用 buffer pool 的，所以这种方式还能够避免占用内存，提高内存利用率。 那么，什么条件下可以使用 change buffer 呢？ 对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入 (4,400) 这个记录，就要先判断现在表中是否已经存在 k=4 的记录，而这必须要将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用 change buffer 了。 因此，==唯一索引的更新就不能使用 change buffer，实际上也只有普通索引可以使用。== 如果要简单地对比这两个机制在提升更新性能上的收益的话，redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。 merge 的执行流程是这样的： 从磁盘读入数据页到内存（老版本的数据页）； 从 change buffer 里找出这个数据页的 change buffer 记录 (可能有多个），依次应用，得到新版数据页； 写 redo log。这个 redo log 包含了数据的变更和 change buffer 的变更。到这里 merge 过程就结束了。 这时候，数据页和内存中 change buffer 对应的磁盘位置都还没有修改，属于脏页，之后各自刷回自己的物理数据，就是另外一个过程了。 现在，你已经理解了 change buffer 的机制，那么我们再一起来看看如果要在这张表中插入一个新记录 (4,400) 的话，InnoDB 的处理流程是怎样的。 第一种情况是，这个记录要更新的目标页在内存中。这时，InnoDB 的处理流程如下： 对于唯一索引来说，找到 3 和 5 之间的位置，判断到没有冲突，插入这个值，语句执行结束； 对于普通索引来说，找到 3 和 5 之间的位置，插入这个值，语句执行结束。 这样看来，普通索引和唯一索引对更新语句性能影响的差别，只是一个判断，只会耗费微小的 CPU 时间。但，这不是我们关注的重点。 第二种情况是，这个记录要更新的目标页不在内存中。这时，InnoDB 的处理流程如下： 对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束； 对于普通索引来说，则是将更新记录在 change buffer，语句执行就结束了。 将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。change buffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。",
    "url": "/mysql相关/change buffer是什么.html",
    "lang": ""
  },
  {
    "title": "order by语句工作",
    "content": "--- title: order by语句工作 --- 在where情况下，当order by 的字段不带索引： extra字段如果带有using filesort就需要排序，然后mysql会分配一块内存sortbuffer用来排序。（先准备sortbuffer用来排序） 先通过辅助索引找到满足where条件的主键id。（找主键id） 再回表去主键索引树中==取出所有要求返回的字段值(可能是a,b,c)==，存入sortbuffer中（回表取出要求的字段） 从辅助索引找下一个记录的主键id，重复上述操作直到不满足条件为止。 对sortbuffer中的数据按照==order by 的字段（可能就是a）==进行排序（排序直接返回即可） 如果排序所需的内存比sortbuffersize还要大，就要使用外部排序，生成临时文件辅助排序。 全字段排序如果查询的字段非常多，单行很大，那么需要分成很多临时文件，排序性能会很差。 所以==mysql中会判断如果单行长度超过某个值，就会采用新的算法。== 新的算法放入 sortbuffer 的字段，==只有要排序的列和主键 id==。 具体的步骤： 初始化sortbuffer，放入两个字段，需要排序的列和主键id 从辅助索引找到第一个满足条件的主键id，然后回表到主键索引树中取出待排序的列(a)和主键id字段，放入sortbuffer中 继续从辅助索引找下个满足条件的主键id，重复步骤直到不满足条件为止。 对sortbuffer中的数据按照order by 的字段进行排序； 然后遍历排序结果，并按照主键id回到主键索引中取出查找的字段(b和c)（==回表操作==）。 如果 MySQL 实在是担心排序内存太小，会影响排序效率，才会采用 rowid 排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据。 如果 MySQL 认为内存足够大，会优先选择全字段排序，把需要的字段都放到 sortbuffer 中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据。 这也就体现了 MySQL 的一个设计思想：如果内存够，就要多利用内存，尽量减少磁盘访问。 如果order by的字段是带索引的。那么可以不需要临时表，也不需要排序了。 并且我们还可以使用索引覆盖进一步的提高效率，减少回表操作。 order by rand() 使用了内存临时表，内存临时表排序的时候使用了 rowid 排序方法。",
    "url": "/mysql相关/order by语句工作.html",
    "lang": ""
  },
  {
    "title": "当前读和快照读",
    "content": "--- title: 当前读和快照读 --- 注意区分快照读和当前读。 当前读指的是select for update或者select lock in share mode，insert,update, 指的是在更新之前必须先查寻当前的值，因此叫当前读。 快照读指的是在语句执行之前或者在事务开始的时候会创建一个视图（readview），后面的读都是基于这个视图的，不会再去查询最新的值。 ---------- 在 MySQL 中，InnoDB 存储引擎是支持事务的，并且实现了多版本并发控制（MVCC），这使得它能够支持两种主要的读操作：快照读（Snapshot Read）和当前读（Current Read）。 快照读是指读取操作在不加锁的情况下获取数据的一致视图。这通常用于实现可重复读（Repeatable Read）和读已提交（Read Committed）隔离级别下的读操作。 特征： 数据来自于事务开始时或查询开始时的一致快照。 不会阻塞其他事务的写操作，不会被其他事务的写锁所阻塞。 实现了 MVCC，通过读取数据的快照来实现一致性。 典型操作： SELECT FROM table 这样的查询在默认情况下会使用快照读，以保证性能和一致性。 隔离级别： 在可重复读和读已提交隔离级别下，普通的 SELECT 查询会使用快照读。 在可重复读（Repeatable Read）下，事务内的所有快照读看到的是事务开始时的快照。 在读已提交（Read Committed）下，事务内的每个快照读看到的是执行时的最新已提交版本。 当前读则是指读取操作在获取数据时需要加锁，以便获取当前的最新数据版本。这通常用于需要修改数据或确保读取和写入之间没有并发冲突的操作。 特征： 读取的是最新的数据版本，并且会对读取的数据加锁。 阻塞其他事务的写操作，并且可能被其他事务的写锁阻塞。 常用于需要一致性读写的场景。 典型操作： SELECT ... LOCK IN SHARE MODE：读取数据并加共享锁。 SELECT ... FOR UPDATE：读取数据并加排他锁。 INSERT, UPDATE, DELETE：这些操作在执行过程中会进行当前读以根据最新数据版本进行修改。 隔离级别： 在所有事务隔离级别下，如果查询需要修改数据（即 DML 操作），则通常使用当前读。 在可串行化（Serializable）隔离级别下，所有的读操作实际上都是当前读，因为它们都需要确保事务之间没有并发冲突。 快照读适用于大多数只读查询，以提高并发性和性能，而不需要考虑事务间的锁冲突。 当前读则在需要确保查询和后续修改的一致性时使用，或者在需要锁定资源以避免冲突的用例中，例如实现乐观锁和悲观锁机制。 选择使用哪种读方式，取决于具体的业务需求、事务隔离级别以及性能和一致性的权衡。",
    "url": "/mysql相关/当前读和快照读.html",
    "lang": ""
  },
  {
    "title": "幻读问题",
    "content": "--- title: 幻读问题 --- 在可重复读隔离级别上，为了解决幻读问题，InnoDB 只好引入新的锁，也就是间隙锁 (Gap Lock)。 间隙锁和行锁合称 next-key lock，每个 next-key lock（临键锁） 是前开后闭区间。也就是说，我们的表 t 初始化以后，如果用 select from t for update 要把整个表所有记录锁起来，就形成了 7 个 next-key lock，分别是 (-∞,0]、(0,5]、(5,10]、(10,15]、(15,20]、(20, 25]、(25, +supremum]。 间隙锁和 next-key lock 的引入，帮我们解决了幻读的问题，但同时也带来了一些“困扰”。 两个session分别存在间隙锁，然后分别往间隙锁里插入数据 间隙锁是在可重复读隔离级别下才会生效的。 所以，你如果把隔离级别设置为读提交的话，就没有间隙锁了。 但同时，你要解决可能出现的数据和日志不一致问题，需要把 binlog 格式设置为 row。 如果读提交隔离级别够用，也就是说，业务不需要可重复读的保证，这样考虑到读提交下操作数据的锁范围更小（没有间隙锁），这个选择是合理的。",
    "url": "/mysql相关/幻读问题.html",
    "lang": ""
  },
  {
    "title": "分库分表",
    "content": "--- title: 分库分表 --- MySQL 分库分表的好处就像把一个大仓库拆分成若干个小仓库一样，比如你有一家超市，里面有很多商品，但是这些商品分类很杂，放在一个仓库里可能很混乱，而且也不容易查找。如果把这个仓库拆分成好几个仓库，比如有食品仓库、服装仓库、电子产品仓库，这样就很容易找到想要的商品，同样的道理，MySQL 分库分表也是这样，把大的数据库拆分成若干个小的数据库，把大表按照合适的分表策略拆分成多个具有独立存储空间的小表，拆分后的小表可以存储在同一个数据库，也可以分散在不同数据库中。这样就能更好的管理数据（例如改善数据库的备份和恢复性能）增加可用性和安全性,提高数据访问（包括查询和读写）效率。 分库分表是一种常用的数据库扩展方案，它将一个大的数据库或大表按照不同的规则进行切分，将数据分散到多个数据库实例或表中，以提高数据库的性能和可扩展性。 分库分表通常包括垂直分表、垂直分库、水平分库、水平分表四种方案。 是将一个大数据表按数据列（字段）的维度进行拆分（分成多个表，每个表存储其中一部分字段）。垂直分表适用于数据表列数过多、列之间关联较小的场景。它带来的提升是： 1、减少了 IO 冲突和锁表的几率。例如：查看商品详情的用户与浏览商品信息的用户互不影响。 2、充分发挥热数据的操作效率，例如：商品信息的操作的高效率不会被商品描述信息（BLOG：二进制类型数据，存储图片、视频等或 TEXT：长文本类型数据）访问的低效率所拖累。 是将一个大型数据库中的数据按照业务模块进行拆分，将不同的业务模块分配到不同的数据库上。它的核心理念是专库专用。它带来的提升是： 1、解决业务层面的耦合，业务清晰。 2、能对不同业务的数据进行分级管理、维护、监控、扩展等。 3、高并发场景下，垂直分库一定程度的提升 IO、减少数据库连接数、降低单机硬件资源的瓶颈。 垂直分库适用于业务模块复杂、数据表之间关联较小的场景，每个库可以部署在不同的服务器上，从而达到多个服务器共同分摊压力的效果，但是依然没有解决单表数据量过大的问题。当一个应用难以再细粒度的垂直切分，或切分后数据量行数巨大，存在单库读写、存储性能瓶颈，这时候就需要进行水平分库了。 是将一个大型数据库依据一定规则，按数据行的维度进行切分，将数据行分配到不同的数据库上，每个库可以部署在不同的服务器上。水平分库是对数据行的拆分，不影响表结构。它带来的提升是： 1、解决了单库数据量大和高并发的性能瓶颈。 2、提高了系统的稳定性及可用性。稳定性提升体现在 IO 冲突减少，锁定减少，可用性指如果某个库出问题，数据库仍部分可用。 经过水平切分的优化，往往能解决单库存储量及性能瓶颈。但由于同一个表被分配在不同的数据库，需要额外进行数据操作的路由工作，因此大大提升了系统复杂度。 是将一个大表按照数据行的维度进行拆分，将数据行分配到不同的数据表中。水平分表适用于数据表行数过多、数据访问量分散的场景。 它带来的提升是： 1、优化单一表数据量过大而产生的性能问题 2、避免 IO 争抢并减少锁表的几率 库内的水平分表，解决了单一表数据量过大的问题，分出来的小表中只包含一部分数据，从而使得单个表的数据量变小，提高检索性能。",
    "url": "/mysql相关/分库分表.html",
    "lang": ""
  },
  {
    "title": "数据库范式",
    "content": "--- title: 数据库范式 --- 即表的列的具有原子性,不可再分解，即列的信息，不能分解, 第一范式（1NF）解决了列重复的问题。 第二范式（2NF）是在第一范式（1NF）的基础上增加了两个新约束，每个表必须有一个主键，并且非主键列必须完全依赖于整个主键，而不能只依赖于主键的一部分。第二范式（2NF）解决了非主键列对主键的部分函数依赖问题。 满足第三范式（3NF）必须先满足第二范式（2NF）。简而言之，第三范式（3NF）要求一个数据库表中不包含已在其它表中已包含的非主键字段。就是说，表的信息，如果能够被推导出来，就不应该单独的设计一个字段来存放(能尽量外键 join 就用外键 join)。第三范式(3NF)解决了传递函数依赖问题。 满足最低要求的范式是第一范式（1NF）。在第一范式的基础上进一步满足更多规范要求的称为第二范式（2NF），其余范式以次类推。一般说来，数据库只需满足第三范式(3NF）就行了",
    "url": "/mysql相关/数据库范式.html",
    "lang": ""
  },
  {
    "title": "binlog的写入机制",
    "content": "--- title: binlog的写入机制 --- 其实，binlog 的写入逻辑比较简单： 事务执行过程中，先把日志写到 binlog cache，事务提交的时候，再把 binlog cache 写到 binlog 文件中。 一个事务的 binlog 是不能被拆开的，因此不论这个事务多大，也要确保一次性写入。 这就涉及到了 binlog cache 的保存问题。系统给 binlog cache 分配了一片内存，每个线程一个，参数 binlogcachesize 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。 每个线程有自己 binlog cache，但是共用同一份 binlog 文件。 !img 图中的 write，指的就是指把日志写入到文件系统的 page cache，并没有把数据持久化到磁盘，所以速度比较快。 图中的 fsync，才是将数据持久化到磁盘的操作。一般情况下，我们认为 fsync 才占磁盘的 IOPS。 write 和 fsync 的时机，是由参数 syncbinlog 控制的： syncbinlog=0 的时候，表示每次提交事务都只 write，不 fsync（此时内容写在了page cache中），由操作系统决定何时将数据持久化到磁盘； syncbinlog=1 的时候，表示每次提交事务都会执行 fsync； syncbinlog=N(N>1) 的时候，表示每次提交事务都 write（数据内容在page cache中），但累积 N 个事务后才 fsync。 !img 把“写 binlog”当成一个动作。 但实际上，写 binlog 是分成两步的： 先把 binlog 从 binlog cache 中写到磁盘上的 binlog 文件； 调用 fsync 持久化。 这么一来，binlog 也可以组提交了。在执行图 5 中第 4 步把 binlog fsync 到磁盘时，如果有多个事务的 binlog 已经写完了，也是一起持久化的，这样也可以减少 IOPS 的消耗。",
    "url": "/mysql相关/binlog的写入机制.html",
    "lang": ""
  },
  {
    "title": "redolog的写入机制",
    "content": "--- title: redolog的写入机制 --- redo log有三种状态 !img 这三种状态分别是： 存在 redo log buffer 中，物理上是在 MySQL 进程内存中，就是图中的红色部分； 写到磁盘 (write)，但是没有持久化（fsync)，物理上是在文件系统的 page cache 里面，也就是图中的黄色部分； 持久化到磁盘，对应的是 hard disk，也就是图中的绿色部分。 日志写到 redo log buffer 是很快的，wirte 到 page cache 也差不多，但是持久化到磁盘的速度就慢多了。 为了控制 redo log 的写入策略，InnoDB 提供了 innodbflushlogattrxcommit 参数，它有三种可能取值： 设置为 0 的时候，表示每次事务提交时都只是把 redo log 留在 redo log buffer 中 ; 设置为 1 的时候，表示每次事务提交时都将 redo log 直接持久化到磁盘； 设置为 2 的时候，表示每次事务提交时都只是把 redo log 写到 page cache。 InnoDB 有一个后台线程，每隔 1 秒，就会把 redo log buffer 中的日志，调用 write 写到文件系统的 page cache，然后调用 fsync 持久化到磁盘。 注意，事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些 redo log 也会被后台线程一起持久化到磁盘。也就是说，一个没有提交的事务的 redo log，也是可能已经持久化到磁盘的。 实际上，除了后台线程每秒一次的轮询操作外，还有两种场景会让一个没有提交的事务的 redo log 写入到磁盘中。 一种是，redo log buffer 占用的空间即将达到 innodblogbuffersize 一半的时候，后台线程会主动写盘。注意，由于这个事务并没有提交，所以这个写盘动作只是 write，而没有调用 fsync，也就是只留在了文件系统的 page cache。 另一种是，并行的事务提交的时候，顺带将这个事务的 redo log buffer 持久化到磁盘。假设一个事务 A 执行到一半，已经写了一些 redo log 到 buffer 中，这时候有另外一个线程的事务 B 提交，如果 innodbflushlogattrxcommit 设置的是 1，那么按照这个参数的逻辑，事务 B 要把 redo log buffer 里的日志全部持久化到磁盘。这时候，就会带上事务 A 在 redo log buffer 里的日志一起持久化到磁盘。 !img 我们介绍两阶段提交的时候说过，时序上 redo log 先 prepare， 再写 binlog，最后再把 redo log commit。 如果把 innodbflushlogattrxcommit 设置成 1，那么 redo log 在 prepare 阶段就要持久化一次，因为有一个崩溃恢复逻辑是要依赖于 prepare 的 redo log，再加上 binlog 来恢复的。 每秒一次后台轮询刷盘，再加上崩溃恢复这个逻辑，InnoDB 就认为 redo log 在 commit 的时候就不需要 fsync 了，只会 write 到文件系统的 page cache 中就够了。 现在你就能理解了，WAL 机制主要得益于两个方面： redo log 和 binlog 都是顺序写，磁盘的顺序写比随机写速度要快； 组提交机制，可以大幅度降低磁盘的 IOPS 消耗。",
    "url": "/mysql相关/redolog的写入机制.html",
    "lang": ""
  },
  {
    "title": "binlog的格式",
    "content": "--- title: binlog的格式 --- binlog是逻辑日志 当 binlogformat=statement 时，binlog 里面记录的就是 SQL 语句的原文（相当于记录了逻辑操作）。你可以用 binlog 的格式改为 binlogformat=‘row’ 可以看到，与 statement 格式的 binlog 相比，前后的 BEGIN 和 COMMIT 是一样的。但是，row 格式的 binlog 里没有了 SQL 语句的原文，而是替换成了两个 event：Tablemap 和 Deleterows。 Tablemap event，用于说明接下来要操作的表是 test 库的表 t; Deleterows event，用于定义删除的行为。 当 binlogformat 使用 row 格式的时候，binlog 里面记录了真实删除行的主键 id，这样 binlog 传到备库去的时候，就肯定会删除 id=4 的行，不会有主备删除不同行的问题，记录了行数据最终被修改成什么样了，每行数据的变化结果都会被记录，比如执行批量update，更新多少行数据就会产生多少条记录。 基于上面的信息，我们来讨论一个问题：为什么会有 mixed 这种 binlog 格式的存在场景？ 推论过程是这样的： 因为有些 statement 格式的 binlog 可能会导致主备不一致，所以要使用 row 格式。 但 row 格式的缺点是，很占空间。比如你用一个 delete 语句删掉 10 万行数据，用 statement 的话就是一个 SQL 语句被记录到 binlog 中，占用几十个字节的空间。但如果用 row 格式的 binlog，就要把这 10 万条记录都写到 binlog 中。这样做，不仅会占用更大的空间，同时写 binlog 也要耗费 IO 资源，影响执行速度。 所以，MySQL 就取了个折中方案，也就是有了 mixed 格式的 binlog。mixed 格式的意思是，MySQL 自己会判断这条 SQL 语句是否可能引起主备不一致，如果有可能，就用 row 格式，否则就用 statement 格式。",
    "url": "/mysql相关/binlog的格式.html",
    "lang": ""
  },
  {
    "title": "说一下MVCC",
    "content": "--- title: 说一下MVCC --- mysql利用MVCC无锁机制。 ==避免读写阻塞。== 单纯使用锁的时候，并发性能会比较差。即便是在读写锁这种机制下，读和写依旧是互斥的。而数据库是一个性能非常关键的中间件，如果某个线程修改某条数据就让其他线程都不能读这条数据，这种性能损耗是无法接受的。 所以 InnoDB 引擎引入了 MVCC，就是为了减少读写阻塞。 MVCC是什么 MVCC 是 MySQL InnoDB 引擎用于控制数据并发访问的协议。MVCC 主要是借助于版本链来实现的。在 InnoDB 引擎里面，每一行都有两个额外的列，一个是 trxid，代表的是修改这一行数据的事务 ID。另外一个是 rollptr，代表的是回滚指针。InnoDB 引擎通过回滚指针，将数据的不同版本串联在一起，也就是版本链。这些串联起来的历史版本，被放到了 undolog 里面。当某一个事务发起查询的时候，MVCC 会根据事务的隔离级别来生成不同的 Read View，从而控制事务查询最终得到的结果。 这种通过「版本链」来控制并发事务访问同一个记录时的行为就叫 MVCC（多版本并发控制）。结合readview来进行CAS比较,所以说这是一种乐观锁的表现 数据库表中除了原数据的这些列以外，还维护了3个我们看不到的隐藏列 DBTRXID 当前这条数据被哪个事务维护了 DBROLLPTR 存储这条新数据的老数据的指针，一旦需要更新的操作需要回滚，就会利用这个指针来进行数据回滚，这些历史数据存在了undolog里面，以链表的方式进行存储。回滚指针。InnoDB 通过 rollptr 把每一行的历史版本串联在一起。 在没有维护主键ID的时候，会维护一个ROWID的隐藏列。利用这个来建主键索引 创建一个readview快照信息，再通过这个readview去找undolog里的每一条信息。按规则一个个找。 MVCC就是乐观锁的一种体现 读未提交无需锁无需MVCC,因为修改数据改源数据，会出现脏读 读已提交 每次查询都会创建Readview读取数据 可重复读 同样的查询只会第一次创建Readview快照（每个事务对应一个readview） 串行化直接使用的表锁，或者是读写锁 对于「读提交」和「可重复读」隔离级别的事务来说，它们是通过 Read View 来实现的，它们的区别在于创建 Read View 的时机不同，大家可以把 Read View 理解成一个数据快照，就像相机拍照那样，定格某一时刻的风景。 「读提交」隔离级别是在「每个语句执行前」都会重新生成一个 Read View， 而「可重复读」隔离级别是「启动事务时」生成一个 Read View，然后整个事务期间都在用这个 Read View !image-20241015115522940 Readview有四个重要的字段 !image-20240921203309754 在创建readview后，可以将记录中的trxid划分为这三种情况 !image-20240921203721971 !img 这样，对于当前事务的启动瞬间来说，一个数据版本的 row trxid，有以下几种可能： 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的； 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的； 如果落在黄色部分，那就包括两种情况 a. 若 row trxid 在mids数组中，表示这个版本是由还没提交的事务生成的，不可见； b. 若 row trxid 不在mids数组中，表示这个版本是已经提交了的事务生成的，可见。 当它要去更新数据的时候，就不能再在历史版本上更新了，否则事务 C 的更新就丢失了。 因此，事务 B 此时的 set k=k+1 是在（1,2）的基础上进行的操作。 所以，这里就用到了这样一条规则：更新数据都是先读后写的，而这个读，只能读==当前的值==，称为“当前读”（current read）。 这里我们提到了一个概念，叫作当前读。其实，==除了 update 语句外，select 语句如果加锁，也是当前读。== 强一致性要求：在某些情况下，应用程序需要确保数据的一致性，如在金融交易中，多个操作必须以原子方式完成。仅使用 MVCC 可能无法保证在并发环境下的强一致性。 防止脏读和不可重复读：虽然 MVCC 可以帮助避免脏读，但在某些隔离级别下（如可重复读），仍可能需要锁来防止不可重复读和幻读。 长事务：当一个事务持有长时间的锁时，MVCC 可能无法有效管理版本。长事务可能导致数据版本太多，增加内存和存储的负担。 事务的回滚：在发生错误或需要回滚时，MVCC 无法处理某些复杂的场景，比如在多个事务之间的依赖关系，需要锁来提供清晰的事务边界。 写冲突：在高竞争的情况下，如果多个事务尝试同时更新同一行数据，仅依赖 MVCC 可能导致大量的版本产生，进而影响性能。使用锁可以有效控制对同一资源的访问，从而减少写冲突的发生。 锁的优先级：在一些场景中，开发者可能希望通过锁来优先处理某些重要的事务，而 MVCC 无法提供这样的控制。 DDL 操作：在执行数据定义语言（DDL）操作时，通常需要加锁以防止其他事务同时修改结构，这在 MVCC 中是无法处理的。 复杂查询：某些复杂的查询可能需要在读取数据的一致性上依赖于锁，以确保在查询过程中数据不会被修改。 实现复杂度：完全依赖 MVCC 可能会导致实现复杂性增加，尤其是在处理死锁、回滚和锁竞争等问题时。锁机制提供了相对简单而有效的方式来管理并发。",
    "url": "/mysql相关/说一下MVCC.html",
    "lang": ""
  },
  {
    "title": "count()和count(1)区别",
    "content": "--- title: count()和count(1)区别 --- !image-20240921200832115 count(1)、 count()、 count(主键字段)在执行的时候，如果表里存在二级索引，优化器就会选择二级索引进行扫描。 所以，如果要执行 count(1)、 count()、 count(主键字段) 时，尽量在数据表上建立二级索引，这样优化器会自动采用 keylen 最小的二级索引进行扫描，相比于扫描主键索引效率会高一些。 再来，就是不要使用 count(字段) 来统计记录个数，因为它的效率是最差的，会采用全表扫描的方式来统计。如果你非要统计表中该字段不为 NULL 的记录个数，建议给这个字段建立一个二级索引。 在 SQL 中，COUNT 函数用于计算行数，但不同的用法会产生不同的结果。以下是 COUNT(1)、COUNT()、COUNT(主键字段) 和 COUNT(字段) 之间的区别： 定义：计算结果集中所有行的总数，包括 NULL 值。 特点： 不管行中的数据是否为 NULL，COUNT() 都会计数。 通常是最常用的计数方式，因为它不涉及特定字段的值，速度较快。 定义：计算结果集中所有行的总数，其中每一行都被视为 1。 特点： 与 COUNT() 类似，COUNT(1) 也会计算所有行，包括 NULL 值。 实际上，在大多数数据库实现中，COUNT(1) 和 COUNT() 的执行效率是相似的，但其内部实现可能略有不同。 定义：计算结果集中指定主键字段不为 NULL 的行数。 特点： 只有当主键字段的值不是 NULL 时，才会被计数。 如果主键字段是非 NULL 的（因为主键设计的特性），这通常会计数所有行。 使用主键字段进行计数可以在一定条件下提高代码的可读性，但在主键总是有值的情况下，可以与 COUNT() 得到相同的结果。 定义：计算结果集中指定字段不为 NULL 的行数。 特点： 只有当指定字段的值不是 NULL 时，才会被计数。 如果某些行的该字段值为 NULL，这些行将不会被计数。 适合用于检查某个特定字段的非空记录数量。",
    "url": "/mysql相关/count()和count(1)区别.html",
    "lang": ""
  },
  {
    "title": "MySQL如何避免死锁",
    "content": "--- title: MySQL如何避免死锁 --- 打破 互斥 ，请求与保持，不可剥夺，循环等待 这四个条件即可、 在数据库层面，有两种策略通过打破循环等待条件来解除死锁状态 1.设置事务等待锁的超时时间 innodblockwaittimeout。当一个事务的等待时间超过该值后，就对这个事务进行回滚，于是锁就释放了，另一个事务就可以执行了。 2.开启主动死锁检测 。主动死锁检测在发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodbdeadlockdetect 设置为 on，表示开启这个逻辑，默认就开启。",
    "url": "/mysql相关/MySQL如何避免死锁.html",
    "lang": ""
  },
  {
    "title": "sql查询慢，除了索引还有什么原因",
    "content": "--- title: sql查询慢，除了索引还有什么原因 --- 连接数过小，应用连接池 bufferpool过小",
    "url": "/mysql相关/sql查询慢，除了索引还有什么原因.html",
    "lang": ""
  },
  {
    "title": "MySQL更新一条记录会发生什么",
    "content": "--- title: MySQL更新一条记录会发生什么 --- !image-20240923091240024",
    "url": "/mysql相关/MySQL更新一条记录会发生什么.html",
    "lang": ""
  },
  {
    "title": "Mysql索引优化",
    "content": "--- title: Mysql索引优化 --- 根据数据量大的情况： 按需查询字段,也能减少网络IO消耗 前缀索引优化： 主键索引最好是自增的：如果使用非自增的话，插入的索引值是随机的，会经常出现需要数据移动甚至从一个页移动到另一个页的情况（页分裂）。导致索引结构不紧凑，影响查询效率。 注意索引失效的情况 避免使用select distinct 选择区分度高的列作为索引 不要使用select ，减少Mysql优化器负担 查询的字段尽量保证索引覆盖：避免回表 索引最好设置为not null：索引列存在Null使得优化器做索引选择更加复杂，并且会多花存储空间来存储null值列表 借助Redis等nosql缓存数据缓解Mysql的压力",
    "url": "/mysql相关/Mysql索引优化.html",
    "lang": ""
  },
  {
    "title": "mysql面经",
    "content": "--- title: mysql面经 --- mysql执行语句的过程 口述实现一个负载均衡组件，阐明实现要点 MySQL隔离级别&索引注意事项、 了解哪些数据库&归纳讲讲 MVSQL的优势 聚簇索引非聚簇索引区别 如何设计索引 nosql数据库和mysql的区别 \\9. mysql你用什么存储引擎，用到InnoDB的什么特性，mvcc是怎么实现的，没有mvcc会产生什么问题 事务隔离级别 MYSQL的MYISAM和INNODB有什么区别 redolog,undolog,binlog的顺序呢 为什么要设置逻辑外键 什么是分布式事务 分布式事务就是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上，以上是百度百科的解释。简单的说，就是一次大的操作由不同的小操作组成，这些小的操作分布在不同的服务器上，且属于不同的应用，分布式事务需要保证这些小操作要么全部成功，要么全部失败。本质上来说，分布式事务就是为了保证数据库中的数据一致性。 （1）在分布式系统中一次操作由多个系统协同完成，这种一次事务操作涉及多个系统通过网络协同完成的过程称为分布式事务。这里强调的是多个系统通过网络协同完成一个事务的过程，并不强调多个系统访问了不同的数据库，即使多个系统访问的是同一个数据库也是分布式事务 2）另外一种分布式事务的表现是，一个应用程序使用了多个数据源连接了不同的数据库，当一次事务需要操作多个数据源，此时也属于分布式事务，当系统作了数据库拆分后会出现此种情况。 6)联合索引注意事项 (7)B+树索引结构优点覆盖索引 讲一下索引是什么? 13.讲讲聚集索引和非聚集索引的区别? 14.讲讲索引为什么要用 B+树而不用 B树或红黑树? 15.什么是事务? 16.事务的四个特性? 17.Mysql的四种隔离级别? 18.什么是幻读?可重复读能解决幻读吗? 19.InnoDB 和 Mylsam 的区别? 20.如果想要查询某个表中有多少行，InnoDB 和 Mylsam 哪个比较快 二叉查找树、二叉平衡树、红黑树的区别和联系 \\4. 覆盖索引是什么 \\5. 聚集索引和非聚集索引区别 \\6. 给几个SQL，看会不会走索引（注意order by id） \\7. 如果要优化SQL，用什么方法 \\8. EXPLAIN需要关注那些字段 \\9. EXPLAIN的Extra列中出现的内容，Using Index和Using Where的区别 \\10. 数据库用来做分布式锁怎么做 \\11. 什么SQL会加排他锁 .mysql事务的隔离级别 3.脏读、不可重复度、幻读?问到不回滚会导致脏读问题么?只事务中只修改一次会导致不可重复读么?幻读具体场景? 4.Innodb引擎的默认隔离级别?怎么实现的?MVCC机制?MVCC一条记录有几个版本? 5.讲讲索引?联合索引听过么?场景(a=1&b>2&c=2)和(a=1b=2|c=3)索引失效问题?追问用or一定会索引失:么?怎么看sq1是不是走索引了? 1.redo log是什么? 为什么需要redo log? redo log 是什么呢? 为什么需要 redo log? 2.什么是 WAL技术,好处是什么3.redo log的写入方式 4.redo log的执行流程 5.redo log 为什么可以保证 crash safe机制呢? 6.binlog的概念是什么,起到什么作用,可以保证crash-safe吗? binlog和 redolog的不同点有哪些?8.执行器和innoDB在执行update语句时候的流程是什么样的? 9.如果数据库误操作,如何执行数据恢复?10.binlog日志三种格式11.什么是MySQL两阶段提交,为什么需要两阶段提交? 12.如果不是两阶段提交,先写redo log和先写 binlog两种情况各会遇到什么问题? 13.binlog刷盘机制14.undo log 是什么?它有什么用15.说说 Redo log的记录方式 9.关于数据库建索引，where a== xxx 还有a!=xxx，哪个会用索引为什么。!=为什么不会用索引10.关于组合索引，a,b,c三个字段，对a和b做组合索引，有个sal语句是a== ? And c ==?请问这个场景会不会用到索引?那么b == ?and c ==?会不会用到索引呢 6.脏读 如何解决7.3范式 8.B+树 优点",
    "url": "/mysql相关/mysql面经.html",
    "lang": ""
  },
  {
    "title": "基本",
    "content": "--- title: 基本 --- 1.缓存 2.排行榜zset 3.消息队列，利用Lists或者Stream来实现一个简单消息队列 4.分布式锁，利用redisson框架使用redis实现了一套分布式锁解决方案，但是存在一定限制，业务超时解锁，导致并发问题，集群脑裂问题。 5.计数器，Redis的命令是原子性的，可以利用INCR和DECR来构建计数器系统,也可以用来做限流 6.连续签到，利用string，将key值设为ccuid，然后设置每日零点过期，value为连续签到天数. 基于内存实现：速度比磁盘IO块很多 使用IO多路复用模型。：可以同时监听多个连接 单线程模型。：不会因为创建线程消耗性能，避免上下文切换引起的CPU消耗，没有多线程切换的开销。避免了线程之间的竞争问题，不需要考虑各种锁问题。 高效的底层数据结构：为了衡量空间和时间，有些数据类型甚至用了多种数据结构。 全局哈希表 单线程指的是Redis的网络IO以及field-value pairs命令读/写是由一个线程来执行的。Redis的持久化、集群数据同步、异步删除等操作都是其他线程执行的。 不过Redis从6.0版本开始支持多线程模型，多I/O线程模型只用来处理网络读写请求，而Redis的读写命令依然是单线程处理的。 在使用redis时，几乎不存在CPU成为瓶颈的情况，Redis的性能瓶颈主要受限于内存。",
    "url": "/redis相关/基本.html",
    "lang": ""
  },
  {
    "title": "介绍",
    "content": "--- title: 非关系型数据库 --- 关系型数据库：MySQL就是关系型数据库的一种，支持表结构。 非关系型数据库：Redis为例，以键值对的形式。 基于键值对 key-value类型：Redis，memcached 列存储数据库 Column-oriented Graph：HBase 图形数据库 Graphs based：Neo4j 文档型数据库： MongoDB MongoDB是一个基于分布式文件存储的数据库，主要用来处理大量的文档。 远程字典服务器。开源的、C语言编写的、高性能。数据库、缓存和消息中间件。 redis中文官网：http://www.redis.cn/ redis英文官网：https://redis.io/ 1、支持持久化。redis是内存数据库，数据是存在内存中，断电重启之后，数据会丢失，持久化可以将数据被备份到磁盘上面，断电重启之后，可以将数据加载到内存中。 2、支持丰富的数据类型。支持：==string、list、set、sort set（zset）、hash== 3、支持数据的备份，也就是支持主从复制。 1、性能高。读11w/s,写8.1w/s 2、丰富的数据类型，支持：string、list、set、sort set、hash 3、原子 4、丰富的特性，支持发布订阅，key过期。",
    "url": "/redis相关/非关系型数据库.html",
    "lang": ""
  },
  {
    "title": "redis的命令执行过程",
    "content": "--- title: redis的命令执行过程 --- 客户端请求 通过IO多路复用，调用readQueryFromClient将可读事件的client放入redisServer.clientspendingread列表（IO队列） 主线程调用handleClientsWithPendingReadsUsingThreads将client分配给线程绑定的队列。 iothreadlist[0] ----- iothreadlist[n] 主线程和IO线程多线程去调用readQueryFromClient来执行命令解析 但是执行命令的时候，还是单线程模型，等待所有IO线程解析命令完成，并执行命令请求。 将待响应client添加到clientspendingwrite队列，并将队列中client分配给IO线程并通知线程此次处理的是可写事件。 主线程和IO线程多线程调用writeToClient发送数据响应客户端。 主线程阻塞等待所有IO线程将响应发给客户端，再检查clientspendingwrite是否还有数据要返回给客户端，如果有将剩余数据响应给客户端。",
    "url": "/redis相关/redis的命令执行过程.html",
    "lang": ""
  },
  {
    "title": "常用数据类型",
    "content": "--- title: 常用数据类型 --- shell select index #index是数据库的编号 keys del key flushdb flushall move key db #db目标数据库的下标 expire key second #second过期的时间 ttl key | 类型 | 简介 | 特性 | 场景 | | --------------------- | ------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | | String(字符串) | 二进制安全 | 可以包含任何数据,比如jpg图片或者序列化的对象,一个键最大能存储512M | --- | | Hash(字典) | 键值对集合,即编程语言中的Map类型 | 适合存储对象,并且可以像数据库中update一个属性一样只修改某一项属性值(Memcached中需要取出整个字符串反序列化成对象修改完再序列化存回去) | 存储、读取、修改用户属性 | | List(列表) | 链表(双向链表) | 增删快,提供了操作某一段元素的API | 1,最新消息排行等功能(比如朋友圈的时间线) 2,消息队列 | | Set(集合) | 哈希表实现,元素不重复 | 1、添加、删除、查找的复杂度都是O(1) 2、为集合提供了求交集、并集、差集等操作 | 1、共同好友 2、利用唯一性,统计访问网站的所有独立ip 3、好友推荐时,根据tag求交集,大于某个阈值就可以推荐 | | Sorted Set(有序集合) | 将Set中的元素增加一个权重参数score,元素按score有序排列 | 数据插入集合时,已经进行天然排序 | 1、排行榜 2、带权重的消息队列 | string是二进制安全的，可以存放任何数据。 shell set key value get key mset k1 value1 k2 value2... mget k1 k2 ... GETRANGE key start end #start开始的下标的位置0开始，end结束的下标的位置 -1代表倒数第一个 SETRANGE k2 offset value #从offset的位置开始，字符串被设置为value值 getset key newValue #获取oldValue，并且设置key的newValue setex key seconds value incr key #每次增加单位1 incrby key num #增加num大小 SETNX key value set key value ex 100 nx 127.0.0.1:6379> set k1 hello\\\\0wuhan #不会截断\\0后面的字符串 OK 127.0.0.1:6379> get k1 \"hello\\\\\\\\0wuhan\" 127.0.0.1:6379> set k2 hello\\0hhh OK 127.0.0.1:6379> get k2 \"hello\\\\0hhh\" 127.0.0.1:6379> 双向链表。 shell lpush/rpush key value1 value2 lpop/rpop key lrange key start stop #获取指定范围的元素，start就是从下标为0开始 lset key index newValue #index就是下标的含义。 lindex mylist index #index还是下标的含义 lrem mylist count value LTRIM mylist 0 5 LINSERT key before/after value newValue LINSERT mylist before 4 1000 元素是唯一的，但是没有顺序，底层使用的哈希表。（与STL中的set不一样） shell sadd myset value1 value2 value3 ... scard key #获取set的成员数 smembers key srandmember key num #随机选择num个元素，但是元素还在集合set中 spop key num #随机删除num个元素 srem key member 移除 sismember key member #判断member元素是否是集合key的成员 smove source destination member #将member元素从source集合移动到destination集合 srem key value #删除集合中值为value的元素 SDIFF/SINTER/SUNION key1 key2 可以给每个元素添加double类型的分数（权重），根据分数进行排序。 shell zadd key double1 value1 double2 value2 double value3 .... zadd myzset 10 h 30 j 20 w zcard key #获取zset的成员数 ZCOUNT myzset -inf +inf #-inf，负无穷 +inf，正无穷 zrevrange key start stop #从大道小返回索引区间 (递减) 0 -1 -1代表最后一个成员 ZRANGE key start stop [WITHSCORES] #通过索引区间返回有序集合指定区间内的成员 ZRANGEBYLEX key min max [LIMIT offset count] #通过字典区间返回有序集合的成员（分数要一致） ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT] #通过分数返回有序集合指定区间内的成员 ZSCORE key member #返回有序集中，成员的分数值 ZREVRANGEBYSCORE key max min [WITHSCORES] #返回有序集中指定分数区间内的成员，分数从高到低排序 等价于是STL的map<key, map<key2, value>> shell hset key field value hget key field hmset key field1 value1 field2 value2 ... hmset people name h3rwau sex man age 90 hmget key field1 field2 ... hmget people name sex age",
    "url": "/redis相关/常用数据类型.html",
    "lang": ""
  },
  {
    "title": "String数据结构",
    "content": "--- title: String数据结构 --- 为了支持丰富和高性能的字符串操作函数、保存二进制格式数据、节省内存，以及实现“既要又还要”的目标，使用了SDS，中文叫做简单动态字符串。 C语言字符串的缺点： 没有长度信息，要获取长度信息只能遍历找到'\\0'。 无法做到安全的二进制格式数据存储，图片等二进制格式数据无法保存。无法存储\\0这种特殊字符是因为\\0在C语言字符串中表示结尾。 字符串的扩容和缩容，char数组的长度在创建字符串的时候就确定下来。如果要追加数据，则要重新申请一块空间，把追加的字符串内容拷贝进去，再释放旧空间看，十分消耗资源 指针向右移是数据buf，指针向左移是元信息，数组总长度，使用的长度，flags表示SDS类型 SDS一共设计了5种类型，sdshdr5, sdshdr8，sdshdr16，sdshdr32，sdshdr64 c++ struct attribute ((packed)) sdshdr8{ //使用了专门的编译优化手段来节省空间。这个作用就是告诉编译器，不要使用字节对齐的方式，而是采用紧凑的方式分配内存。 uint8t len; uint8t alloc; unsigned char flags; char buf[]; }; 当对SDS进行缩短操作时，程序并不会回收多余的内存空间，如果后面需要append追加操作，则可以直接使用数组中未使用的空间。 为了防止重启数据丢失，开启AOF，但是还是有可能丢失1秒的数据。 所以使用一个异步机制将生成的最大ID持久化到一个Mysql 当应用服务启动时先从Mysql中查询出最大值M，然后执行EXISTS counter:order判断是否存在键 如果不存在这个键，则SET counter:order M将值写入Redis中 如果存在这个键 值为N，则比较N和M的值，SET counter:order max(M,N)将最大值写入Redis中，如果相等则不操作。 应用服务启动完毕后，在每次需要生成ID时，应用程序就向Redis服务器发送INCR counter:order命令 应用将获取到的ID发送到MQ中，消费者监听队列把值持久化到Mysql.",
    "url": "/redis相关/String数据结构.html",
    "lang": ""
  },
  {
    "title": "List数据结构",
    "content": "--- title: List数据结构 --- 早期作为Lists的底层实现的linkedlist(双端链表)和ziplist(压缩列表)，Redis3.2引入的由linkedlist和ziplist组成的quicklist，以及7.0版本中取代了ziplist的listpack。 linkedlist由于是链表，所以每个节点内存不连续，并且每个节点都有一个pre指针和一个next指针。 当链表中的每个元素占用的字节数小于或等于64, 当链表的元素数量小于512个 就会使用ziplist存储链表，ziplist内存是连续的，并使用元数据来记录了总字节数和最后一个entry的偏移量，和entry总数，能够以O(1)的时间找到ziplist中第一个元素或最后一个元素。entry中还存储了前一个entry占用的字节数。 但是ziplist也有缺点，不能保存过多元素，否则查询性能会大大降低，导致O(N)时间复杂度。 ziplist的存储空间是连续的，当插入新的entry时，内存空间不足就需要重新分配一块连续内存空间，引发连锁更新问题。 于是Redis3.2引入了quicklist quicklist本质还是一个链表，只不过链表的每个节点都是一个ziplist。 每个节点还是有前序指针和后序指针，由于每个节点都是ziplist，所以还有一个指向ziplist的指针。 但由于有ziplist，所以连锁更新的问题还是存在。 后序7.0版本使用listpack替换掉ziplist。listpack不记录前一个元素的长度，而是记录自身的encoding-type和encoding-data的长度。 消息队列：异步的服务间通信方式（异步解耦，流量削峰）",
    "url": "/redis相关/List数据结构.html",
    "lang": ""
  },
  {
    "title": "Hash数据结构",
    "content": "--- title: Hash数据结构 --- 散列表是一种field-value pairs集合类型，Redis的散列表底层数据结构就是dict，由数组和链表构成。 散列表的底层数据结构实际上有两种。 dict数据结构 listpack数据结构 通常情况下，使用dict存储，每个field-value pairs构成一个dictEntry节点。 只有同时满足以下两个条件，才会使用listpack来代替dict 每个field-value pairs中的field和value的字符串的字节数都小于或等于hash-max-listpack-value配置的值(默认64) field-value pairs的数量小于hash-max-listpack-entries配置的值(默认512) dic同时使用httable[0]和httable[1]两个散列表，采用渐进式rehash !image-20250322090908847 购物车场景 购物车hash：key为 shoppingCart:用户ID dictEntry存储 filed: 商品的名字或者编号 value:数量 还可以用一个hash来存储商品明细信息 商品信息表hash：key为 goods:info dictEntry存储 filed: 商品的名字或者编号 value:信息的json",
    "url": "/redis相关/Hash数据结构.html",
    "lang": ""
  },
  {
    "title": "Set数据结构",
    "content": "--- title: Set数据结构 --- Sets是字符串类型的无序集合，集合中的元素是唯一的，不会出现重复的数据。底层数据结构是采用哈希表实现的，value存Null就行。 当元素内容都是64位以内的十进制整数，并且元素个数不超过set-max-intset-entries配置的值(默认512)时，Sets会使用更加省内存的intset（整形数组）来存储。 intset是有序的，但是set是无序的，为什么？ 数组有序有助于使用二分法提高查找元素效率 共同关注：通过集合交集实现。 每日新增关注数：对近两天的总注册用户量集合取差集。 打标签：为每篇收藏的文章打标签。 共同好友：",
    "url": "/redis相关/Set数据结构.html",
    "lang": ""
  },
  {
    "title": "为每个用户/IP/API创建一个zset",
    "content": "--- title: ZSet数据结构 --- zset中的元素由两部分组成，分别是member和score。 zset底层使用两种方式存储数据。 listpack(在7.0版本之前是ziplist)：使用条件是集合元素个数小于或等于某个配置值(默认128)，且member占用字节数小于或等于zset-max-listpack-value的配置值(默认64)。将member和score紧凑排列作为listpack的一个元素存储。 skiplist+dict：当不满足上述条件时，将数据分别存储在skiplist和dict中，是一种空间换时间的思想。散列表的key存储的是元素的member，value存储的是member关联的score。 listpack适合元素个数不多且元素占用空间不大的场景。 skiplist用来根据score进行范围查询或者单个查询，dict则用于实现以o(1)时间复杂度查询单个元素。 skiplist本质是一种可以进行二分查找的有序链表。增加了多级索引，通过索引来实现快速查找。 排行榜，维护游戏中根据分数排名的top10-有序列表。 设置分数为=玩家游戏分+[(基准时间-玩家获得分数时间)/基准时间] 这样在分数相同时，越早获得该分数的排名更前。 速率限流器，根据排序集合构建滑动窗口速率限制器。 c++ ZADD user:123:requests <currenttimestamp> <requestid> ZREMRANGEBYSCORE user:123:requests 0 <currenttimestamp - 60000> ZCARD user:123:requests 延迟队列，使用score存储过期时间，从小到大排序，最靠前的就是最先到期的数据。",
    "url": "/redis相关/ZSet数据结构.html",
    "lang": ""
  },
  {
    "title": "Stream数据结构",
    "content": "--- title: Stream数据结构 --- 使用Lists实现消息队列有很多局限性。 没有ACK机制 没有类似Kafka的消费者组的概念 消息堆积 Lists是线性结构，查询指定数据需要遍历整个列表 Stream是Redis5.0专门为消息队列设计的数据类型，提供消费者组的概念，同时提供消息的持久化和主从复制机制。 主要特性： 使用Radix Tree（压缩前缀树）和listpack结构来存储消息。 序列化生成消息ID 将多个消费者划分到不同的消费者组中。 支持多播、阻塞和非阻塞读取。 ACK确认机制，保证了消息至少被消费一次。 可配置消息保存上限阈值，将历史消息丢弃，防止内存占用过大。 但是Redis Stream是一种超轻量级的MQ，并没有完全实现消息队列的所有设计要点。",
    "url": "/redis相关/Stream数据结构.html",
    "lang": ""
  },
  {
    "title": "为什么zset底层是跳表而不是红黑树或者b+树",
    "content": "--- title: 为什么zset底层是跳表而不是红黑树或者b+树 --- zset是redis的有序集合，是需要支持范围查找的，而红黑树是弱平衡二叉查找树，范围查找开销比较大，需要不断的遍历节点。而对于跳表的话，只需要从上层链表的索引就能确定最底层的范围了，然后从最底层取数据就行了。 那为什么不用b+树呢。b+树的叶子节点是存储了数据的，而非叶子节点只存储索引，并且b+树是多路平衡查找二叉树。并且叶子结点每一个节点都是挨个链接起来的是一个双向链表。所以说b+树也是可以做范围查找的。 但是b+树去查找一个节点的时间复杂度是要比跳表高的。是hlog2n。在每一个非叶子节点中，需要利用二分查找来找到对应的索引，然后再往子节点去搜索，所以说还要乘上一个树的高度。 所以说跳表是适合用来组织内存数据的。而B+树适合组织磁盘数据。 因为B+树的磁盘IO次数是根据树的高度来算的。每次访问一个节点就是一次磁盘IO，它是一个扁平的结构，意味着更少的磁盘IO。跳表中的比较次数就比较多了。",
    "url": "/redis相关/为什么zset底层是跳表而不是红黑树或者b+树.html",
    "lang": ""
  },
  {
    "title": "配置",
    "content": "--- title: 配置 --- 配置文件的路径 /etc/redis/6379.conf 关闭redis服务器的命名，在客户端中执行shutdown。 启动redis服务器的命令，sudo redis-server /etc/redis/6379.conf",
    "url": "/redis相关/配置.html",
    "lang": ""
  },
  {
    "title": "持久化",
    "content": "--- title: 持久化 --- redis是属于内存数据库，数据是存放在内存中的，如果断电之后，数据就会丢失，为了防止这个现象，reids会定期的将内存中的数据备份到磁盘中，断电重启之后，将数据从磁盘加载到内存中。 RDB持久化：将当前数据保存到硬盘（原理是将Reids在内存中的数据库记录定时dump到磁盘上） AOF持久化：将每次执行的写命令保存到硬盘（原理是将Reids的操作日志以追加的方式写入文件，类似于MySQL的binlog） 默认的持久化方案。在指定的时间间隔内，执行指定次数的写操作，则会将内存中的数据写入到磁盘中。即在指定目录下生成一个dump.rdb文件。Redis 重启会通过加载dump.rdb文件恢复数据。（/var/lib/redis/6379） 1、执行shutdown命令，会触发快照 2、flushall命令，也会触发快照。 3、手动执行save命令，也可以触发快照。（bgsave （异步）命令） 4、在指定的时间间隔内，执行指定次数的写操作。 shell save 30 2 #该命令的含义是，在30s以内，执行两次写操作，写的次数要有2次以上，并且时间要达到30s，才能触发快照。如果时间达到了30s，但是写次数没有达到2次以上，就不会触发快照；如果写次数达到了2次以上，但是时间没有达到30s，也不会触发快照（时间与写次数要同时达到）xx:yy:40 save set set xx:y(y+1):01 优点： 1、适合大规模的数据恢复，与AOF相比,在恢复大的数据集的时候，RDB方式会更快一些 2、如果业务对数据完整性和一致性要求不高，RDB是很好的选择。 缺点： 1、对数据的完整性与一致性不高。 2、备份时占用内存（备份时会独立创建一个fork子进程，将数据写入到一个临时文件，最后再将临时文件替换之前的备份文件） AOF方式是将执行过的写指令记录下来，在数据恢复时按照从前到后的顺序再将指令都执行一遍。主流的持久化方案。 !image-20230704161852601 redis set k1 1 incy k1 #将该命令执行99次。 k1 = 100; set k1 100 因为 AOF 的运作方式是不断地将命令追加到文件的末尾， 所以随着写入命令的不断增加， AOF 文件的体积也会变得越来越大。举个例子， 如果你对一个计数器调用了 100 次 INCR ， 那么仅仅是为了保存这个计数器的当前值， AOF 文件就需要使用 100 条记录（entry）。然而在实际上，只使用一条 SET 命令已经足以保存计数器的当前值了， 其余 99 条记录实际上都是多余的。 为了处理这种情况， Redis 支持一种有趣的特性： 可以在不打断服务客户端的情况下， 对 AOF 文件进行重建（rebuild）。执行 BGREWRITEAOF命令， Redis 将生成一个新的 AOF 文件， 这个文件包含重建当前数据集所需的最少命令。Redis 2.2 需要自己手动执行 BGREWRITEAOF 命令； Redis 2.4 则可以自动触发 AOF 重写， 具体信息请查看 配置文件。 redis Redis会记录上次重写时的AOF大小，默认配置是当AOF文件大小是上次rewrite后大小的一倍且文件大于64M时触发 auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb BGREWRITEAOF手动触发重写。 优点 1、对数据的完整性与一致性比较高。 缺点 1、执行写命令，花费时间比较久。 2、文件追加的形式，可能导致文件体积变大。 1、如果对数据的完整性与一致性要求比较高，可以选择aof持久化；如果想让数据恢复快一些，可以使用rdb，默认情况使用的是rdb持久化方式 2、两种持久化的方式是可以一起使用的，可以起到互补的作用。 3、如果只有aof持久化方式，是可以启动redis服务器的。如果aof是唯一的持久化方式，但是aof文件被损坏了，那么redis服务器是启动不成功的。 4、如果aof持久化是唯一的方式，那么aof文件损坏之后，服务器不能正常启动，但是可以通过命令修复aof文件。 !image-20230704163821497",
    "url": "/redis相关/持久化.html",
    "lang": ""
  },
  {
    "title": "事务",
    "content": "--- title: 事务 --- 也是一组命令的集合。 事务会有三个步骤，开始事务、命令入队、执行事务。 ==redis中的事务是没有原子性的。== shell multi exec discard watch unwatch 测试 shell 127.0.0.1:6379> MULTI OK 127.0.0.1:6379> 127.0.0.1:6379> lpush mylist 1 3 5 7 9 QUEUED 127.0.0.1:6379> get k3 QUEUED 127.0.0.1:6379> set k4 400 QUEUED 127.0.0.1:6379> exec 1) (integer) 5 2) \"hello\" 3) OK 127.0.0.1:6379> !image-20230704171355049 !image-20230704172020686 1、悲观锁：顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，当其他线程想要访问数据时，都需要阻塞挂起。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁、表锁，读锁，写锁等，都是在操作之前先上锁。linux互斥锁就是这种机制 2、乐观锁：【冲突检测和数据更新】 乐观锁（Optimistic Lock），顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是==在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。==乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库如果提供类似于writecondition机制的其实都是提供的乐观锁。 乐观锁策略：提交版本必须大于记录当前版本才能执行更新，一般会使用版本号机制和CAS操作实现： ==version方式==：一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。 CAS（Check And Set【先检查再设置】） ==CAS操作方式==：即 Compare And Swap，CAS是乐观锁技术，涉及到三个操作数，数据所在的内存值V，预期值A，新值B。当需要更新时，判断当前内存值V与之前取到的值A是否相等，若相等，则用新值更新，若失败则重试，一般情况下是一个自旋操作，即不断的重试。 两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下，即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果经常产生冲突，上层应用会不断的进行retry，这样反倒是降低了性能，所以这种情况下用悲观锁就比较合适。 单独的隔离级别 事务是串行执行的，天然保证了隔离性 没有隔离级别的概念 不保证原子性",
    "url": "/redis相关/事务.html",
    "lang": ""
  },
  {
    "title": "主从复制",
    "content": "--- title: 主从复制 --- 持久化侧重解决的是Redis数据的单机备份问题（从内存到硬盘的备份）；而主从复制则侧重解决数据的多机热备。此外，主从复制还可以实现负载均衡和故障恢复 主从复制，是指将一台Redis服务器的数据，复制到其他的Redis服务器。前者称为主节点(master)，后者称为从节点(slave)；==数据的复制是单向的，只能由主节点到从节点。== 默认情况下，每台Redis服务器都是主节点；且一个主节点可以有多个从节点(或没有从节点)，但一个从节点只能有一个主节点。 ==1.数据冗余==：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。 ==2.故障恢复==：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务的冗余。 ==3.读写分离==：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务（即写Redis数据时应用连接主节点，读Redis数据时应用连接从节点），分担服务器负载；尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高Redis服务器的并发量。 ==4.高可用基石==：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是Redis高可用的基础。 1、在/etc/redis路径下面，将6379.conf文件拷贝两份，分别取名6380.conf与6381.conf。 2、将6380.conf文件与6381.conf文件中的数据进行修改，修改五个 redis port 6380 6381 pidfile \"/var/run/redis6380.pid\" \"/var/run/redis6381.pid\" logfile \"/var/log/redis6380.log\" \"/var/log/redis6381.log\" dbfilename \"dump6380.rdb\" \"dump6381.rdb\" appendfilename \"appendonly6380.aof\" \"appendonly6380.aof\" 3、分别启动三个配置文件,然后使用ps命令查看三个进程是否启动了 sudo redis-server /etc/redis/6379.conf sudo redis-server /etc/redis/6380.conf sudo redis-server /etc/redis/6381.conf !image-20230705094811490 4、分别启动三个客户端， redis-cli -p 6379 redis-cli -p 6380 redis-cli -p 6381 5、在每个客户端下面查看主从复制的信息info replication，发现都是master的身份 6、如果想将6380与6381作为6379的从机，可以在6380与6381下面执行，slaveof 127.0.0.1 6379,就可以看到主从复制的信息。 !image-20230705095105788 !image-20230705095122275 总结 1、==从机上是不能进行写操作的。== 2、==如果主机挂掉了， 那么从机会立马感知到主机的掉线，会记录主机的状态为down，但是从机不会成为新的主机，只会默默等待主机的上线。== 3、==如果不想将某台机器作为任何机器的从机，可以执行slaveof no one.== redis的传统的主从复制，当主机挂掉之后，从机不会成为主机，那么如果有新的写操作，是无法进行的，所以可以使用哨兵模式。 哨兵模式会执行两个协议，流言协议（造谣）、投票协议（少数服从多数） 1、在/etc/redis下面添加一个文件，sentinel.conf,然后添加一行命令 redis sentinel monitor master6379 127.0.0.1 6379 1 2、启动哨兵的配置文件sudo redis-sentinel /etc/redis/sentinel .conf，哨兵就启动了，会监视主机 3、将主机断掉之后， 然后哨兵会进行留言协议与投票协议，然后将这一票投递给6380或者6381， 谁得到这一票谁就是主机。 问题：==大量的数据在同一时间失效了，导致本来可以在缓存中查找的数据现在找不到，就只能到底层数据库中进行查找，那么底层数据库的压力会比较大。== 解决方案：==1、可以让key不过期 2、将数据分散时间失效== !image-20230705102745061 问题：==当热点数据key从缓存内失效时，大量访问同时请求这个数据，就会将查询下沉到数据库层，此时数据库层的负载压力会骤增，我们称这种现象为\"缓存击穿\"。== 解决问题：==1、延长热点数据的过期时间或者设置为永不过期 2、加锁，让大量请求互斥访问数据库== !image-20230705105357866 问题：==要访问的数据在缓存中没有，并且在底层数据库中也不存在该数据，那么当大量请求过来之后，会造成底层数据库的压力比较大，但是数据在数据库依旧没有。== 解决方案：==可以设置key与空的键值对，存在缓存中map<key,null>，查询的时候，可以通过value进行体现== !image-20230705110139641 redis redisCommand(pc, \"set %s %s\", key, value); redisCommand(pc, \"lpush %s %d %d %d\", mylist, 1,2,3); redisCommand(pc, \"get %s \", key); set key value lpush mylist 1 2 3 redis redisContext redisConnect(const char ip, int port); void redisCommand(redisContext c, const char format...); void freeReplyObject(void reply) void redisFree(redisContext c) 在编译的时候，g++ xxx.cc -lhiredis",
    "url": "/redis相关/主从复制.html",
    "lang": ""
  },
  {
    "title": "其他",
    "content": "--- title: 其他 --- > 为什么 Redis 是单线程的而不用多线程并行执行充分利用 CPU 呢？ 我们要明确的是：Redis 的单线程指的是 Redis 的网络 IO 以及键值对指令读写是由一个线程来执行的。 对于 Redis 的持久化、集群数据同步、异步删除等都是其他线程执行。 至于为啥用单线程，我们先了解多线程有什么缺点。 使用多线程，通常可以增加系统吞吐量，充分利用 CPU 资源。 但是，使用多线程后，没有良好的系统设计，可能会出现如下图所示的场景，增加了线程数量，前期吞吐量会增加，再进一步新增线程的时候，系统吞吐量几乎不再新增，甚至会下降！ !image-20240915193309572 在运行每个任务之前，CPU 需要知道任务在何处加载并开始运行。也就是说，系统需要帮助它预先设置 CPU 寄存器和程序计数器，这称为 CPU 上下文。 这些保存的上下文存储在系统内核中，并在重新计划任务时再次加载。这样，任务的原始状态将不会受到影响，并且该任务将看起来正在连续运行。 切换上下文时，我们需要完成一系列工作，这是非常消耗资源的操作。 另外，当多线程并行修改共享数据的时候，为了保证数据正确，需要加锁机制就会带来额外的性能开销，面临的共享资源的并发访问控制问题。 引入多线程开发，就需要使用同步原语来保护共享资源的并发读写，增加代码复杂度和调试难度。 不会因为线程创建导致的性能消耗； 避免上下文切换引起的 CPU 消耗，没有多线程切换的开销； 避免了线程之间的竞争问题，比如添加锁、释放锁、死锁等，不需要考虑各种锁问题。 代码更清晰，处理逻辑简单。 单线程是否没有充分利用 CPU 资源呢？ 官方答案：因为 Redis 是基于内存的操作，CPU 不是 Redis 的瓶颈，Redis 的瓶颈最有可能是机器内存的大小或者网络带宽。既然单线程容易实现，而且 CPU 不会成为瓶颈，那就顺理成章地采用单线程的方案了。原文地址：https://redis.io/topics/faq。 纯内存操作，一般都是简单的存取操作，线程占用的时间很多，时间的花费主要集中在 IO 上，所以读取速度快。 整个 Redis 就是一个全局哈希表，他的时间复杂度是 O(1)，而且为了防止哈希冲突导致链表过长，Redis 会执行 rehash 操作，扩充 哈希桶数量，减少哈希冲突。并且防止一次性 重新映射数据过大导致线程阻塞，采用 渐进式 rehash。巧妙的将一次性拷贝分摊到多次请求过程后总，避免阻塞。 Redis 使用的是非阻塞 IO：IO 多路复用，使用了单线程来轮询描述符，将数据库的开、关、读、写都转换成了事件，Redis 采用自己实现的事件分离器，效率比较高。 采用单线程模型，保证了每个操作的原子性，也减少了线程的上下文切换和竞争。 Redis 全程使用 hash 结构，读取速度快，还有一些特殊的数据结构，对数据存储进行了优化，如压缩表，对短数据进行压缩存储，再如，跳表，使用有序的数据结构加快读取的速度。 根据实际存储的数据类型选择不同编码",
    "url": "/redis相关/其他.html",
    "lang": ""
  },
  {
    "title": "过期删除策略",
    "content": "--- title: 过期删除策略 --- !image-20240924110647379 当我们对一个key设置了过期时间，Redis会把该key带上过期时间存储到一个过期字典中，也就是说==过期字典保存了数据中所有key的过期时间== 当我们查询一个key时候，Redis首先检查该key是否存在于过期字典中： 1.如果不在 ，则正常读取键值 2.如果存在，则会获取该key的过期时间，然后与当前系统时间进行对比，如果比系统时间大，那就没过期。 · ==定时删除==：在设置 key 的过期时间时，同时创建一个定时事件，当时间到达时，由事件处理器自动执行 key 的删除操作。 ​ 优点：可以保证过期 key 会被尽快删除，也就是内存可以被尽快地释放。因此，定时删除对内存是最友好的 ​ 缺点：在过期 key 比较多的情况下，删除过期 key 可能会占用相当一部分 CPU 时间，在内存不紧张但 CPU 时间紧张的情况下，将 CPU 时间用于删除和当前任务无关的过期键上，无疑会对服务器的响应时间和吞吐量造成影响。所以，定时删除策略对 CPU 不友好。 · ==惰性删除==：如果过期不主动删除过期键，每次从数据库访问 key 时，都检测 key 是否过期，则删除该 key。 ​ 优点：因为每次访问时，才会检查 key 是否过期，所以此策略只会使用很少的系统资源，因此，惰性删除策略对 CPU 时间最友好。 ​ 缺点：如果一个 key 已经过期，而这个 key 又仍然保留在数据库中，那么只要这个过期 key 一直没有被访问，它所占用的内存就不会释放，造成了一定的内存空间浪费。所以，惰性删除策略对内存不友好 · ==定期删除==：每隔一段时间「随机」从数据库中取出一定数量的 key 进行检查，并删除其中的过期key。 ​ 优点：通过限制删除操作执行的时长和频率，来减少删除操作对 CPU 的影响，同时也能删除一部分过期的数据减少了过期键对空间的无效占用。 ​ 缺点:内存清理方面没有定时删除效果好，同时没有惰性删除使用的系统资源少，难以确定删除操作执行的时长和频率。如果执行的太频繁，定期删除策略变得和定时删除策略一样，对CPU不友好:如果执行的太少，那又和惰性删除一样了，过期 key 占用的内存不会及时得到释放。 redis的定期删除的流程： 从数据库中随机抽取20个key，检查这20个key是否过期，并删除已过期的key，如果检查的过期key数量超过了5个，那么就继续抽查，如果没有等待下一轮定期删除。循环流程有个上限不超过25ms",
    "url": "/redis相关/过期删除策略.html",
    "lang": ""
  },
  {
    "title": "内存淘汰策略",
    "content": "--- title: 内存淘汰策略 --- 当Redis的运行内存已经超过Redis设置的最大内存后，会使用内存淘汰策略删除符合条件的key，以此来保障Redis高效的运行 这个最大内存，64位操作系统没有内存大小限制，32位是3G。 ==noeviction==(Redis3.0之后，默认的内存淘汰策略):它表示当运行内存超过最大设置内存时，不淘汰任何数据，这时如果有新的数据写入，会报错通知禁止写入，不淘汰任何数据，但是如果没用数据写入的话，只是单纯的查询或者删除操作的话，还是可以正常工作。 针对「进行数据淘汰」这一类策略，又可以细分为「在设置了过期时间的数据中进行淘汰」和「在所有数据范围内进行淘汰」这两类策略。 ==在设置了过期时间的数据中进行淘汰==: 随机淘汰：·volatile-random:随机淘汰设置了过期时间的任意键值 更早过期淘汰·volatile-ttl:优先淘汰更早过期的键值。 LRU淘汰·volatile-lru(Redis3.0 之前，默认的内存淘汰策略):淘汰所有设置了过期时间的键值中，最久未使用的键值; LFU淘汰volatile-lfu(Redis 4.0 后新增的内存淘汰策略):淘汰所有设置了过期时间的键值中，最少使用的键值; ==在所有数据范围内进行淘汰:== 随机淘汰：allkeys-random:随机淘汰任意键值; LRU淘汰alkeys-lru:淘汰整个键值中最久未使用的键值, ·LFU淘汰allkeys-lfu(Redis 4.0 后新增的内存淘汰策略):淘汰整个键值中最少使用的键值,",
    "url": "/redis相关/内存淘汰策略.html",
    "lang": ""
  },
  {
    "title": "RDB和AOF高可用",
    "content": "--- title: RDB和AOF高可用 --- 定时执行RDB快照，既实现了块，也实现了持久化。 有两种情况会触发RDB快照持久化。 手动触发：执行save（主线程执行会阻塞）或bgsave（fork一个子进程用于写入临时RDB文件，持久化交给子进程来处理，阻塞只会发生在fork阶段，生成RDB文件的默认配置使用的就是该命令） 自动触发： 在redis.conf配置save m n，在m秒内至少有n个key更改，自动触发bgsave 主从复制：从节点需要从主节点进行全量复制时会触发bgsave操作，把生成的RDB文件发送给从节点。 shutdown命令，如果没有开启AOF持久化，那么也会触发bgsave 执行debug reload命令重新加载Redis会触发bgsave。 如果配置 save \"\"，则表示关闭RDB快照 在对内存数据做RDB快照时，并不会暂停写操作。 使用了多进程写时复制技术（COW）来实现RDB快照持久化。（通过fork产生子进程） 在子进程产生时，它和父进程共享内存里面的代码段和数据段。 bgsave的子进程可以共享主线程的所有内存数据，所以能读取主线程的数据并写入RDB文件。 优点： RDB文件采用二进制格式数据和数据压缩的方式写磁盘，文件体积远小于内存大小，适合备份和全量复制。 RDB文件加载恢复数据的速度远远快于AOF文件。 缺点： 实时性不够，无法做到秒级持久化。 通过bgsave调用fork函数创建子进程，子进程属于重量级操作，频繁执行成本高。 AOF持久化记录的是服务器接收的每个写操作，在服务器启动，重放还原数据集。 AOF采用的是先写内存，后写日志。 在MYSQL InnoDB引擎，在实际修改数据前先记录修改redolog，再修改数据，这种是先写日志（WAL） appendonly yes AOF的配置项： always:同步写回，写命令执行完毕立刻将aofbuf缓冲区的内容写到aof文件。 everysec：每秒写回，写命令执行完，日志只会写到AOF文件缓冲区，每隔一秒就把缓冲区的内容同步到磁盘。 no:操作系统控制，写命令执行完，把日志写到AOF文件内存缓冲区，由操作系统决定何时同步到磁盘。 AOF可以将多个命令压缩成一条。 通过主线程fork出一个bgrewriteaof 且7.0之后，主线程将新来的写命令写入到新的INCR AOF文件，子进程在cow写时复制得到内存快照数据，写入到新的Base AOF文件，重写结束后，将两个文件合并并记录为新的，原来的记录为history。 优点： 持久化实时性高 是一种追加日志，不会出现随机磁盘读写。 易于理解和解析的格式，包含所有操作的日志。 写操作执行成功才记录日志，避免了命令语法检查开销，同时不会阻塞当前写命令。 缺点： 由于AOF记录的是一个个命令，因此故障恢复时要执行所有命令，如果命令太大，那么整个恢复过程会很缓慢。 文件系统对文件大小有限制，且追加效率随文件大小增加而降低。 写日志之前宕机，那么会丢失数据 混合方式： RDB文件以一定的频率执行，使用AOF文件记录两次快照之间的所有写操作。",
    "url": "/redis相关/RDB和AOF高可用.html",
    "lang": ""
  },
  {
    "title": "主从复制高可用",
    "content": "--- title: 主从复制高可用 --- 高可用有两个含义：一是数据尽量不丢失；二是尽可能提供服务 主从复制作用： 增加副本 故障恢复 负载均衡，读写分离 高可用基石：是哨兵和集群实施的基础。 Redis集群主要解决大数据量存储导致的响应变慢问题，同时便于横向扩展。 垂直扩展：升级单个Redis的硬件配置 水平扩展：横向增加Redis的实例个数，每个节点负责一部分数据。 Redis集群并没有使用一致性hash算法，而是将数据划分为16384个slot，每个节点负责一部分slot，slot的信息存储在节点中。",
    "url": "/redis相关/主从复制高可用.html",
    "lang": ""
  },
  {
    "title": "redis的应用案例",
    "content": "--- title: redis的应用案例 --- !image-20241118144847945 !image-20241118145445823 !image-20241118150057279 !image-20241118150936804 !image-20241118151628453 !image-20241118151903607 !image-20241118152151191 !image-20241118152359276 !image-20241118152409905 !image-20241118152611028 将一个大 Key 按规则分解为多个小 Key，实现： 负载分散：小 Key 分布在多个节点（如 Redis Cluster 分片）。 并行处理：单一操作变多线程/多节点协同。 精准读写：按需访问子 Key，减少数据传输量。 !image-20241118153447662 !image-20241118153524686 !image-20241118153847627 !image-20241118153830944 !image-20241118153950018 !image-20241118154110630 !image-20241118154200528 !image-20241118154331401 !image-20241118154513767",
    "url": "/redis相关/redis的应用案例.html",
    "lang": ""
  },
  {
    "title": "redis数据结构",
    "content": "--- title: redis数据结构 --- !image-20241118145420830 !image-20241118145752966 !image-20241118145955569 !image-20241118150619486 !image-20241118151248909 !image-20241118151424234",
    "url": "/redis相关/redis数据结构.html",
    "lang": ""
  },
  {
    "title": "setnx为什么不是高可用的分布式锁实现",
    "content": "--- title: setnx为什么不是高可用的分布式锁实现 --- !image-20241118152151191 用setnx实现分布式锁的问题： （1）业务超时解锁，导致并发问题。业务执行时间超过锁超时时间 （2）redis主从切换临界点问题。主从切换后,A持有的锁还没有同步到新的主节点时，B可在新主节点获取分布式锁 （3）redis集群脑裂，导致出现多个主节点。 高可用的分布式锁是分布式系统中协调资源访问的关键组件，其设计需兼顾容错性、一致性和性能。以下是常见的实现方案及其特点： --- 单节点 Redis：使用 SET key value NX PX timeout 命令实现锁的互斥性和自动超时释放。 RedLock 算法：在多个独立 Redis 节点上同时申请锁，半数以上节点成功时视为获取锁，避免单点故障。 性能高，适用于高并发场景。 RedLock 通过多节点部署提升可用性。 RedLock 存在争议（如时钟漂移、GC 停顿可能导致锁失效）。 单节点 Redis 存在单点故障风险。 Redisson（Java 客户端实现 RedLock）。 --- 利用 临时顺序节点（Ephemeral Sequential Nodes） 实现锁： 客户端在锁路径下创建临时顺序节点。 判断自己是否为最小序号节点，若是则获取锁。 非最小节点监听前序节点的删除事件（Watcher 机制）。 强一致性保障，避免锁冲突。 临时节点自动删除，防止死锁。 适合对一致性要求高的场景。 写性能低于 Redis。 频繁的 Watcher 通知可能带来性能开销。 Curator（ZooKeeper 客户端库，提供现成的锁实现）。 --- 利用 etcd 的 Lease（租约） 和 Revision 机制： 客户端创建带 Lease 的键值对（Lease 自动过期）。 通过事务（Transaction）比较 Revision 号，确保锁的互斥性。 基于 Raft 协议，强一致性高可用。 Lease 机制自动释放锁，避免死锁。 性能略低于 Redis，但高于 ZooKeeper。 etcd 官方客户端库（如 etcdv3）。 --- 唯一索引/乐观锁：通过数据库的唯一约束或版本号实现锁竞争。 专用表结构：例如记录锁名称、持有者、超时时间等字段。 实现简单，无需额外组件。 性能差（数据库连接开销大）。 需处理锁超时和死锁问题。 高可用依赖数据库主从同步（可能引入延迟）。 --- 利用 Session 机制： 创建 Session 并与键关联。 Session 失效时自动释放锁。 内置服务发现和健康检查，适合微服务架构。 支持 ACL 增强安全性。 性能低于 Redis 和 etcd。 --- AWS DynamoDB Lock Client：基于 DynamoDB 的托管锁服务。 阿里云 Distributed Lock：集成于阿里云产品中。 Google Cloud Memorystore (Redis)：托管 Redis 服务实现锁。 免运维，高可用性由云厂商保障。 开箱即用。 绑定特定云环境，可能产生成本。 --- 强一致性场景：选 ZooKeeper 或 etcd。 高性能场景：选 Redis（需评估 RedLock 风险）。 云原生环境：优先使用托管服务（如 AWS/Aliyun）。 简单场景：数据库锁（适用于低频访问）。 --- 锁自动释放：通过超时机制（TTL）避免死锁。 可重入性：需记录锁持有者（如线程/进程 ID）及重入次数。 公平性：ZooKeeper 的临时顺序节点天然支持公平锁。 网络分区（脑裂）：需结合业务容忍度选择 CP 或 AP 模型。 --- 以上方案各有优劣，需结合业务场景的 一致性需求、性能要求 和 运维成本 综合选择。",
    "url": "/redis相关/setnx为什么不是高可用的分布式锁实现.html",
    "lang": ""
  },
  {
    "title": "Redis有什么优缺点？为什么用Redis查询会比较快",
    "content": "--- title: Redis有什么优缺点？为什么用Redis查询会比较快 --- (1) Redis有什么优缺点？ Redis 是一个基于内存的数据库，读写速度非常快，通常被用作缓存、消息队列、分布式锁和键值存储数据库。它支持多种数据结构，如字符串、哈希表、列表、集合、有序集合等， Redis 还提供了分布式特性，可以将数据分布在多个节点上，以提高可扩展性和可用性。但是Redis 受限于物理内存的大小，不适合存储超大量数据，并且需要大量内存，相比磁盘存储成本更高。 (2）为什么Redis查询快 基于内存操作： 传统的磁盘文件操作相比减少了IO，提高了操作的速度。 高效的数据结构：Redis专门设计了STRING、LIST、HASH等高效的数据结构，依赖各种数据结构提升了读写的效率。 单线程：单线程操作省去了上下文切换带来的开销和CPU的消耗，同时不存在资源竞争，避免了死锁现象的发生。 I/O多路复用：采用I/O多路复用机制同时监听多个Socket，根据Socket上的事件来选择对应的事件处理器进行处理。",
    "url": "/redis相关/Redis有什么优缺点？为什么用Redis查询会比较快.html",
    "lang": ""
  },
  {
    "title": "Redis的数据类型有哪些？",
    "content": "--- title: Redis的数据类型有哪些？ --- Redis 常见的五种数据类型：String（字符串），Hash（哈希），List（列表），Set（集合）及 Zset(sorted set：有序集合)。 字符串STRING：存储字符串数据，最基本的数据类型。 哈希表HASH：存储字段和值的映射，用于存储对象。 列表LIST：存储有序的字符串元素列表。 集合SET：存储无序的不相同的字符串元素。 有序集合ZSET：类似于集合，但每个元素都关联一个分数，可以按分数进行排序。 Redis版本更新，又增加了几种数据类型， BitMap: 存储位的数据结构，可以用于处理一些位运算操作。 HyperLogLog：用于基数估算的数据结构，用于统计元素的唯一数量。 GEO： 存储地理位置信息的数据结构。 Stream：专门为消息队列设计的数据类型。",
    "url": "/redis相关/Redis的数据类型有哪些？.html",
    "lang": ""
  },
  {
    "title": "Redis是单线程的还是多线程的，为什么？",
    "content": "--- title: Redis是单线程的还是多线程的，为什么？ --- Redis在其传统的实现中是单线程的(网络请求模块使用单线程进行处理，其他模块仍用多个线程)，这意味着它使用单个线程来处理所有的客户端请求。这样的设计选择有几个关键原因： 简化模型：单线程模型简化了并发控制，避免了复杂的多线程同步问题。 性能优化：由于大多数操作是内存中的，单线程避免了线程间切换和锁竞争的开销。 原子性保证：单线程执行确保了操作的原子性，简化了事务和持久化的实现。 顺序执行：单线程保证了请求的顺序执行。 但是Redis的单线程模型并不意味着它在处理客户端请求时不高效。实际上，由于其操作主要在内存中进行，Redis能够提供极高的吞吐量和低延迟的响应。 此外，Redis 6.0 引入了多线程的功能，用来处理网络I/O这部分，充分利用CPU资源，减少网络I/O阻塞带来的性能损耗。",
    "url": "/redis相关/Redis是单线程的还是多线程的，为什么？.html",
    "lang": ""
  },
  {
    "title": "说一说Redis持久化机制有哪些",
    "content": "--- title: 说一说Redis持久化机制有哪些 --- AOF 日志：每执行一条写操作命令，就把该命令以追加的方式写入到一个文件里； RDB 快照：将某一时刻的内存数据，以二进制的方式生成一个dump.rdb文件写入磁盘，但是rdb方式是通过在指定的时间间隔内，执行指定次数的写操作或者是通过其他命令来触发，不能确保数据的完整性和一致性； 混合持久化方式：Redis 4.0 新增的方式，集成了 AOF 和 RBD 的优点；",
    "url": "/redis相关/说一说Redis持久化机制有哪些.html",
    "lang": ""
  },
  {
    "title": "介绍一下Redis缓存雪崩和缓存穿透，如何解决这些问题？",
    "content": "--- title: 介绍一下Redis缓存雪崩和缓存穿透，如何解决这些问题？ --- 缓存雪崩是指在某个时间点，大量缓存同时失效，导致请求直接访问数据库，增加了系统负载。对于缓存雪崩， 可以通过合理设置缓存的过期时间，分散缓存失效时间点，或者采用永不过期的策略，再结合定期更新缓存。 设置缓存Redis集群，避免单机宕机的情况。 缓存击穿是指一个缓存中不存在但是数据库中存在的数据，当有大量并发请求查询这个缓存不存在的数据时，导致请求直接访问数据库，增加数据库的负载。典型的场景是当一个缓存中的数据过期或被清理，而此时有大量请求访问这个缓存中不存在的数据，导致大量请求直接访问底层存储系统。 对于缓存击穿，可以采用互斥锁（例如分布式锁setnx）或者在查询数据库前先检查缓存是否存在，如果不存在再允许查询数据库，并将查询结果写入缓存。 缓存预热，提前加载数据 逻辑过期，存储一个过期时间字段（用户感知丝滑，数据一致性较弱，维护异步逻辑） 缓存穿透是指查询一个在缓存和数据库都不存在的数据，这个数据始终无法被缓存，导致每次请求都直接访问数据库，增加数据库的负载。典型的情况是攻击者可能通过构造不存在的 key 大量访问缓存，导致对数据库的频繁查询。 设置空值为value 对于缓存穿透，可以采用布隆过滤器等手段来过滤掉恶意请求（存储合法key），或者在查询数据库前先进行参数的合法性校验。",
    "url": "/redis相关/介绍一下Redis缓存雪崩和缓存穿透，如何解决这些问题？.html",
    "lang": ""
  },
  {
    "title": "如何保证数据库和缓存的一致性",
    "content": "--- title: 如何保证数据库和缓存的一致性 --- 删除缓存的方法： 读取数据的逻辑： 去缓存查数据 如果缓存查询数据失败，就查询数据库 数据库获取到数据，再更新缓存 再返回 写数据的逻辑：有两种思路，都是保证最终一致性。 先删除缓存再更新数据库 可能会造成数据不一致，假设线程1先删除缓存，然后在更新数据库的过程中发生了网络延迟。这个时候还有线程2进行读操作，首先由于缓存已经被删了，所以缓存未命中，然后读数据库，将旧的数据更新到了缓存。线程1的网络好了，然后更新数据库。但是这个时候缓存和数据库的数据发生了不一致。 解决办法：使用延迟双删，写操作更新了数据库后停顿一个时间再进行删除缓存。这样子最多其他线程在这个时间段里读到的是脏数据。 先更新数据库再删除缓存（cache aside） 假如在删除缓存时失败，那就用重试机制。 在高并发的时候，重试最好的方法是异步，比如发送消息给mq中间件，实现异步解耦。 用redis一定是为了提升性能，所以这里所有的方案都是最终一致性，允许短暂的不同，如果非要强一致，那就不要用缓存或者加锁或者通过分布式读写锁。 Cache Aside（先更新数据库再删除缓存） Cache Aside 是最基本的缓存模式，在这个模式下，业务代码就是==把缓存看成是和数据库一样的独立的数据源==，然后业务代码控制怎么写入缓存，怎么写入数据库。一般来说，都是优先写入数据库的。 不管是先写数据库还是先写缓存，Cache Aside 都不能解决数据一致性问题。 原理： 读：==先从缓存中读取数据，如果没有就再去数据库里面读数据，然后把数据放回缓存中，如果缓存中可以找到数据就直接返回数据；== 写：==更新数据的时候先把数据持久化到数据库（先），然后再让缓存失效（后）。== 问题：假如有两个操作一个更新一个查询，第一个操作先更新数据库，还没来及删除缓存，查询操作可能拿到的就是旧的数据；更新操作马上让缓存失效了，所以后续的查询可以保证数据的一致性；还有的问题就是有一个是读操作没有命中缓存，然后就到数据库中取数据，此时来了一个写操作，写完数据库后，让缓存失效，然后，之前的那个读操作再把老的数据放进去，也会造成脏数据，出现了缓存中数据和数据库数据不一致。 可行性：出现上述问题的概率其实非常低，需要同时达成读缓存时缓存失效并且有并发写的操作。数据库读写要比缓存慢得多，所以读操作在写操作之前进入数据库，并且在写操作之后更新，概率比较低。 解决方法依旧使用延迟双删 Read/Write Through（将缓存作为整个存储的代理） read through它的核心是==当缓存里面没有数据的时候，缓存会代替你去数据库里面把数据加载出来==，并且缓存起来。可以异步更新缓存 Write Through 就是在==写入数据的时候，只写入缓存，然后缓存会代替我们的去更新数据库==。但是，Write Through 没有要求先写数据库还是先写缓存，不过一般也是先写数据库。 原理：Read/Write Through原理是把更新数据库（Repository）的操作由缓存代理，应用认为后端是一个单一的存储，而存储自己维护自己的缓存。 Read Through：就是在查询操作中更新缓存，也就是说，当缓存失效的时候，==Cache Aside策略是由调用方负责==把数据加载入缓存，而==Read Through则用缓存服务自己来加载，从而对调用方是透明的==。 Write Through：当有数据更新的时候，如果没有命中缓存，直接更新数据库，然后返回。如果命中了缓存，则更新缓存，然后再由缓存自己更新数据库（这是一个同步操作）。 Write Behind 原理：在更新数据的时候，只更新缓存，不更新数据库，而缓存会异步地批量更新数据库或者缓存过期的时候再刷到数据库。这个设计的好处就是让数据的I/O操作非常快，带来的问题是，数据不是强一致性的，而且可能会丢。 第二步失效问题：这种可能性极小，缓存删除只是标记一下无效的软删除，可以看作不耗时间。如果会出问题，一般程序在写数据库那里就没有完成：故意在写完数据库后，休眠很长时间再来删除缓存。",
    "url": "/redis相关/如何保证数据库和缓存的一致性.html",
    "lang": ""
  },
  {
    "title": "RDB和AOF",
    "content": "--- title: RDB和AOF --- RDB持久化：将当前数据保存到硬盘（原理是将Reids在内存中的数据库记录定时dump到磁盘上的RDB持久化） RDB持久化也是redis默认的一个持久化方案，在指定的时间间隔内，执行指定次数的写操作，会将内存中的数据快照写入到磁盘中，在指定目录下生成一个dump.rdb文件。Redis 重启的时候会通过加载dump.rdb文件恢复数据。 RDB触发快照一共有四种方式，第一种是执行shutdown命令，第二种是flushall命令，第三种是执行save命令，以及在指定的时间间隔内，执行指定次数的写操作也是可以触发快照的 RDB的优点是适合大规模的数据恢复，并且如果业务对数据完整性和一致性要求不高的话，RDB是一个不错的选择。 RDB的缺点自然就是对数据完整性和一致性不高，并且备份的时候是占用内存，fork一个子进程，将数据写入到一个临时文件中，最后再将临时文件替换之前的备份文件。 AOF持久化：将每次执行的写命令保存到硬盘（原理是将Reids的操作日志以追加的方式写入文件，类似于MySQL的binlog），数据恢复的按照从前往后的顺序再将指令都执行一遍。 AOF的同步策略可以在配置文件中可以修改 ： appendfsync always , appendfsync everysec, appendfsync no 此外，AOF还会有一个重写的机制，因为AOF的运作方式是将写命令不断的追加到文件末尾，而在其中一些命令是可以重写成一条命令的。所以redis可以在不打断服务客户端的情况下，对aof文件进行rebuild。 AOF的优点就是数据的完整性与一致性比较高 AOF的缺点就是需要执行写命令，数据恢复时间比较久，然后通过命令追加的形式，AOF文件的体积会比较大。 AOF和RDB持久化方式是可以同时使用起到互补的作用的。",
    "url": "/redis相关/RDB和AOF.html",
    "lang": ""
  },
  {
    "title": "redis为何如此快",
    "content": "--- title: redis为何如此快 --- 1.Redis速度快的第一个原因是它是一个内存级数据库。内存访问比随机磁盘I/O快几个数量级。纯内存访问提供高读写吞吐量和低延迟。权衡是数据集不能大于内存。 2.Redis快的另外一个原因是它主要是单线程的，避免了并发使用的锁和其他同步机制。 3.Redis使用IO多路复用，允许单个线程同时等待多个套结字链接。使用的是epoll。",
    "url": "/redis相关/redis为何如此快.html",
    "lang": ""
  },
  {
    "title": "redis的5种常见使用场景",
    "content": "--- title: redis的5种常见使用场景 --- 1.Redis的第一个用例是缓存对象以加速Web应用程序 2.第二个就是用redis来存储session,意在无状态服务器之间共享会话数据。 2.Redis的第二个用例是分布式锁，当应用程序中的多个节点需要协调对某些共享资源的访问时，就会使用分布式锁。Redis用作分布式锁，其原子命令如SETNX SETNX lock \"1234abcd\" EX 3 如果尚未设置key，则SETNX命令返回1，表示客户端1已获取锁 客户端1完成工作并通过删除key来释放锁 如果key已被设置，SETNX命令则返回0，表示锁已被另一个客户端持有 在这种情况下，客户端1等待并重试SETNX操作，直到另一个客户端释放锁。 3.第三个是速率限制器，通过在某些计数器上使用其增量命令并在这些计数器上设置过期时间，Redis可以用作速率限制器，通过INCR命令来增加对key的请求数量，将当前计数与允许的速率限制进行比较。 4.游戏排行榜可以用zset实现 Redis 是一个高性能的内存数据库，广泛用于各种实时数据处理场景。以下是 Redis 在一些具体功能场景中的应用和实现方式： Top-K 元素： 实现：可以使用 Redis 的 Sorted Set 数据结构来实现 Top-K 元素查询。Sorted Set 会为每个元素关联一个得分，并自动根据得分进行排序。 用法：使用 ZADD 来添加元素及其得分，使用 ZREVRANGE 或 ZREVRANGEBYSCORE 获取得分最高的前 K 个元素。 关注列表： 实现：可以利用 Redis 的 Set 数据结构来存储用户的关注关系。 用法：每个用户的关注列表可以用一个 Set 来表示，使用 SADD 添加关注关系，SREM 移除关注关系，SMEMBERS 获取关注列表。 排行榜： 实现：同样可以使用 Sorted Set 实现排行榜功能，因其支持按得分排序的特性。 用法：使用 ZADD 将用户及其得分添加到排行榜，ZRANK 获取用户排名，ZREVRANGE 获取前 N 名用户。 会话存储： 实现：可以使用 Redis 的 String 数据结构来存储用户会话信息。 用法：将会话 ID 作为键，相关信息作为值，使用 SET 和 GET 存取会话信息；可以利用 EXPIRE 设置过期时间来实现自动过期。 分布式锁： 实现：可以使用 Redis 的 SETNX 命令来实现简单的分布式锁。 用法：使用 SETNX 尝试获取锁，成功则设置一个过期时间以防死锁，使用 DEL 释放锁。 计数器： 实现：利用 Redis 的 String 数据结构，可以很方便地实现计数功能。 用法：使用 INCR 或 INCRBY 增加计数，DECR 或 DECRBY 减少计数。 消息队列： 实现：Redis 的 List 数据结构可用于实现简单的消息队列。 用法：LPUSH 将消息放入队列，RPOP 消费消息。 连续签到： 实现：Redis 的 string将key作为用户的id，value是当前连续签到的天数，设置过期时间为明天的0点 这些应用场景只是 Redis 功能的一部分，实际上，凭借 Redis 的灵活性和丰富的数据结构，开发者可以根据需求设计出更多的高效解决方案。在实现具体功能时，也可以结合 Lua 脚本、事务等特性增强系统的处理能力和可靠性。",
    "url": "/redis相关/redis的5种常见使用场景.html",
    "lang": ""
  },
  {
    "title": "redis的hotkey和大key",
    "content": "--- title: redis的hotkey和大key --- 在拥有大量并发用户的系统中，HotKey一直以来都是一个不可避免的问题。 比如秒杀活动、热点微博、热评，某件商品被数万次点击浏览或购买时，就会造成热点问题 比如大量发布、浏览的热点新闻、热点评论等读多写少场景也会产生热点问题 比如是 瞬间大量开启的爬虫用户， 突发大批机器人以远超正常用户的速度发起极其密集的请求，这些机器人只需要很小的代价，就能发出百倍于普通用户的请求量，从而大幅挤占正常用户的资源。 以京东为例的这些头部互联网公司，动辄某个爆品，会瞬间引入每秒上百万甚至数百万的请求，当然流量多数会在几秒内就消失。 但就是这短短的几秒的HotKey，就会瞬间造成其所在redis分片集群瘫痪。 原因也很简单，redis作为一个单线程的结构，所有的请求到来后都会去排队，当请求量远大于自身处理能力时，后面的请求会陷入等待、超时。 由于该redis分片完全被这个key的请求给打满，导致该分片上所有其他数据操作都无法继续提供服务，也就是HotKey不仅仅影响自己，还会影响和它合租的数据。 这样，redis 缓存没有响应之后，相当于 redis 击穿， 请求直接转向DB DB的吞吐量，比如会低很多，DB 就会雪崩。 HotKey占用大量的Redis CPU时间，使其性能变差并影响其它请求； Redis Cluster中各node流量不均衡造成Redis Cluster的分布式优势无法被Client利用，一个分片负载很高而其它分片十分空闲从而产生读/写热点问题； 在抢购、秒杀活动中，由于商品对应库存Key的请求量过大，超出Redis处理能力造成超卖； HotKey的请求压力数量超出Redis的承受能力造成缓存击穿，此时大量请求将直接指向后端存储，将后端存储打挂并影响到其它业务； 流量过于集中，突破物理网卡的极限 请求过多，缓存分片服务被打垮 穿透DB 1.在应用层引入本地缓存（如Guava Cache、Caffeine、nginx share dict等），将热点数据缓存在本地内存中，减少对Redis的直接访问，从而降低Redis的压力。 优点：减少了对Redis的读请求，降低了热点Key带来的负载压力。 2.通过改变Key的结构（如添加随机前缀），将同一个热点Key拆分成多个Key，使其分布在不同的Redis节点上，从而避免所有流量集中在一个节点上，访问的时候可以访问多个key。 优点：有效避免了单点瓶颈，提高了Redis集群的整体吞吐量。 如果是用 ： redis 主从架构，可以通过增加Redis集群中的从节点，增加 多个读的副本。 3.通过对读流量进行 负载均衡， 将读流量 分散到更多的从节点 上，减轻单个节点的压力。 优点：通过水平扩展，Redis可以处理更大的负载，特别是针对高并发的读请求。 4.实现一个redis代理，结合两个功能：热点数据发现，然后存在本地缓存。 !image-20241118152359276 !image-20241118152409905 !image-20241118152611028 将一个大 Key 按规则分解为多个小 Key，实现： 负载分散：小 Key 分布在多个节点（如 Redis Cluster 分片）。 并行处理：单一操作变多线程/多节点协同。 精准读写：按需访问子 Key，减少数据传输量。 !image-20241118153447662 !image-20241118153524686 !image-20241118153847627 !image-20241118153830944 !image-20241118153950018 !image-20241118154110630",
    "url": "/redis相关/redis的hotkey和大key.html",
    "lang": ""
  },
  {
    "title": "什么是脑裂",
    "content": "--- title: 什么是脑裂 --- 在分布式系统中，因为网络分区或节点故障导致节点之间的通信断开，进而导致数据一致性问题。 在一个高可用集群中，当多个服务器在指定的时间内，由于网络的原因无法互相检测到对方，而各自形成一个新的小规模集群，并且各小集群当中，会选举新的master节点，都对外提供独立的服务。 由于网络断裂的原因，一个高可用集群中，实际上分裂为多个小的集群，这种情况就称为裂脑。 脑裂的严重后果是：数据一致性的丧失，这对于依赖精确数据进行操作的系统来说是致命的。 脑裂导致的三大问题，这里再总结一下： 数据不一致：不同子集之间可能对同一数据进行不同的写入，导致数据不一致。 重复写入：在脑裂问题解决后，不同子集可能尝试将相同的写操作应用到主节点上，导致数据重复。 数据丢失：新Matser会向slave 实例发送slave of命令，让所有slave 重新进行全量同步，在此之前会先清空实例上的数据，所以在主从切换期间，原slave上执行的写命令也会被清空。",
    "url": "/redis相关/什么是脑裂.html",
    "lang": ""
  },
  {
    "title": "redis的四大集群模式",
    "content": "--- title: redis的四大集群模式 --- Redis的部署模式主要包括以下几种： 一、单机 二、主从复制：通过主从复制可以实现数据备份、读写分离、故障恢复等功能。 三、哨兵：哨兵模式是在主从复制基础上加入了哨兵节点，实现了自动故障转移。同时，所有的数据都存放在一台机器（没有进行分片），使得存储的容量也有了限制。 四、集群：Redis集群使用哈希槽的方式将数据进行分片，分开存放在不同的机器上，这样就大大提升了系统存储的容量和性能。每个master还可以有多个从节点，如果master宕机，从节点自动提升为master。 在Redis集群中，数据通过哈希槽（hash slot）进行分片： 哈希槽（Hash Slot）：Redis集群将数据键的哈希值分配到0到16383之间的哈希槽中，每个哈希槽唯一对应一个分片。 数据分片（Shard）：每个主节点管理若干哈希槽，即若干数据分片。 在高并发、高可用的应用场景下，Redis集群是一个更为“高端”的部署方案，可以较方便地进行水平扩展。 Cluster模式应用场景 大规模数据存储：通过数据分片，突破单节点内存限制。 高性能要求场景：通过负载均衡，提高系统性能。 高可用性要求场景：通过自动故障转移，确保服务的持续可用。 总结： Cluster模式在提供高可用性的同时，实现了数据分片和负载均衡，适用于大规模数据存储和高性能要求的场景。然而，它的配置和管理相对复杂，且某些复杂的多键操作可能受到限制。",
    "url": "/redis相关/redis的四大集群模式.html",
    "lang": ""
  },
  {
    "title": "哨兵模式",
    "content": "--- title: 哨兵模式 --- 哨兵 对 Redis 实例（master、从节点）运行状态的监控，并能够在master发生故障时通过一系列的机制实现选主及主从切换，实现故障转移，确保整个 Redis 系统的可用性 。 Redis 哨兵具备的能力有如下几个： 监控：持续监控 master 、slave 是否处于预期工作状态。 自动切换主库：当 Master 运行故障，哨兵启动自动故障恢复流程：从 slave 中选择一台作为新 master。 通知：让 slave 执行 replicaof ，与新的 master 同步；并且通知客户端与新 master 建立连接。 哨兵利用 PING 命令来检测master、 slave 的生命状态。 如果是无效回复，哨兵就把这个 slave 标记为 主观下线 。 哨兵集群中，如果有过半的哨兵判断 master 已经 主观下线 ，这时候才能将 master 标记为 客观下线 。",
    "url": "/redis相关/哨兵模式.html",
    "lang": ""
  },
  {
    "title": "UUID",
    "content": "--- title: UUID --- UUID 的应用场景 UUID 在许多场景中都有广泛应用，包括： 数据库主键：在数据库中，使用 UUID 作为主键可以避免主键冲突，特别是在分布式系统中。 分布式系统：在分布式系统中，使用 UUID 可以确保生成的标识符在全局范围内唯一。 日志记录：在日志记录中，使用 UUID 可以唯一标识每条日志记录。 UUID 是 128 位（16 字节） 的数字，通常表示为 32 个十六进制字符（如 550e8400-e29b-41d4-a716-446655440000），其结构包含： 时间戳（60 位）：记录生成时的精确时间（到 100 纳秒级） 时钟序列（14 位）：应对时钟回拨的计数器 节点标识（48 位）：网卡 MAC 地址或随机数 版本标识（4 位）：标明 UUID 类型（如版本 1、4） | UUID 版本 | 理论重复概率（每秒生成 10 亿个） | | --------- | --------------------------------------------- | | v1 | 约需 89 年 才会出现 50% 碰撞概率 | | v4 | 需要 生成 2.71×10^18 个 才有 50% 碰撞概率 | <think> 好的，用户问到了UUID的应用场景有哪些。我需要先回顾一下UUID的基本概念和特性，然后思考它在不同领域中的实际应用。首先，UUID是通用唯一识别码，版本主要有1、3、4、5，每个版本生成方式不同，比如版本1基于时间和MAC地址，版本4是随机的，版本3和5基于命名空间和散列。它的主要特点是全局唯一性，通常由128位组成，以32位十六进制数表示，格式如550e8400-e29b-41d4-a716-446655440000。 接下来，用户可能需要了解在不同场景下为什么选择UUID而不是自增ID或者其他标识符。比如在分布式系统中，自增ID可能需要中央协调，而UUID可以在本地生成，避免单点故障。此外，UUID无需数据库预生成，可以在客户端生成，减少数据库压力。安全性方面，版本4的随机性较高，适合需要不可预测性的场景，比如临时令牌或一次性URL。 不过，UUID也存在一些缺点，比如长度较长，存储和索引效率可能不如自增整数。在数据库主键中使用UUID可能会影响插入性能和查询性能，尤其是在没有适当优化的情况下。因此，在具体应用时需要权衡利弊。 然后，我需要列举具体的应用场景。比如分布式系统唯一标识，数据库主键，文件命名，会话标识，安全性相关场景如临时令牌，消息队列的消息ID，日志跟踪中的请求ID，设备标识，URL短链中的匿名链接，跨系统数据同步时的主键等等。每个场景需要说明为什么UUID适合，比如唯一性、无需中央协调、安全性等。 同时，需要指出不同版本UUID的适用场景。比如版本1可能因为包含MAC地址而存在隐私问题，适合内部系统；版本4随机性强，适合需要匿名的场景；版本3和5适用于需要基于名称生成唯一标识的情况，比如内容寻址。 最后，可能还需要提到一些注意事项，如存储优化策略（例如使用二进制存储而不是字符串），索引优化，以及UUID的可读性等问题。如果用户是开发人员，可能还关心如何在具体编程语言中生成UUID，以及性能方面的考虑。 总结来说，回答需要涵盖UUID的特性，各个应用场景的具体例子，版本选择的建议，以及使用时需要注意的问题。这样用户不仅能了解UUID的应用场景，还能根据自身需求做出合适的选择。 </think> UUID（Universally Unique Identifier）是一种全局唯一标识符，通常由 32 位十六进制字符 表示（如 550e8400-e29b-41d4-a716-446655440000），其核心价值在于分布式环境下无需中央协调即可生成唯一值。以下是其典型应用场景及技术细节分析： --- | 特性 | 说明 | | ---------------- | ------------------------------------------------------------ | | 全局唯一性 | 重复概率极低（版本4的冲突概率约为 $10^{-36}$） | | 去中心化生成 | 无需依赖数据库或中央服务器分配，客户端可自主生成 | | 无意义性 | 大多数版本（如v4）不包含业务信息，避免信息泄露 | | 标准化 | 遵循 RFC 4122，跨平台兼容性高 | v1：基于时间戳 + MAC地址（可能泄露设备隐私，适合内部系统）。 v4：完全随机生成（最常用，安全性高）。 v3/v5：基于命名空间和散列（如用 URL 生成固定 UUID）。 --- 场景：微服务架构中生成订单号、用户ID、日志追踪ID等。 优势： 各服务节点独立生成ID，避免主键冲突（如分库分表场景）。 对比雪花算法（Snowflake）：无需维护机器ID，适合无状态服务。 场景：代替自增整数（Auto Increment）作为主键。 优势： 防止爬虫通过ID递增猜测数据规模（如用户表主键暴露业务增长量）。 数据合并时无需处理ID冲突（如多分支数据库同步到中心库）。 注意： 建议存储为 BINARY(16) 而非字符串，节省空间（字符串占36字节，二进制仅16字节）。 若需范围查询，可额外添加自增辅助字段。 场景：用户上传文件命名（如 f3a7b21c-45d6-4e8f.jpg）、云存储对象键。 优势： 避免文件名冲突（如多个用户上传同名文件）。 隐藏文件路径信息（对比自增ID，无法通过文件名推测存储结构）。 场景：一次性密码（OTP）、API密钥、访问令牌（Token）。 优势： v4 UUID 随机性高，难以预测（如生成 tmp550e8400e29b41d4a716446655440000 作为临时令牌）。 结合过期时间（TTL）和黑名单机制，提升安全性。 场景：客户端本地存储的匿名用户ID、设备指纹、浏览器会话ID。 优势： 无需用户登录即可生成唯一标识（符合 GDPR 匿名性要求）。 避免 Cookie 被清除后无法追踪用户行为（结合 LocalStorage 持久化 UUID）。 场景：Kafka/RabbitMQ 消息唯一ID，防止重复消费。 实现： python messageid = uuid.uuid4().hex producer.send(topic, key=messageid, value=data) # 消费者通过 messageid 幂等处理 场景：多个独立系统间同步数据（如ERP与CRM系统用户数据互通）。 优势： 通过 UUID 关联同一实体（如用户），避免不同系统的自增ID冲突。 --- | 优化策略 | 说明 | | -------------- | ------------------------------------------------------------ | | 二进制存储 | 将 UUID 转为 BINARY(16) 存储，空间节省55%（36字节 → 16字节）。 | | 索引优化 | 若作为主键，优先使用自增主键 + UUID 组合（避免 B+ 树页分裂导致的插入性能下降）。 | | 生成器选择 | 高并发场景使用更快的生成算法（如 uuidv7 时间有序优化查询性能）。 | 避免使用 v1 UUID：因包含 MAC 地址和时间戳，可能泄露设备信息。 敏感数据脱敏：在日志中掩码部分UUID（如 550e8400--446655440000）。 缺点：UUID 无业务含义，调试时难以记忆（如对比订单号 ORD-20230901-0001）。 解决方案：组合使用（如 USER-{uuid} 或前缀标识类型 CUST550e8400e29b）。 --- | 方案 | 适用场景 | 劣势 | | ------------ | ------------------------ | ----------------------------------------- | | 自增ID | 单机数据库、强顺序需求 | 分库分表时需中央协调，暴露业务规模 | | 雪花算法 | 分布式系统、时间有序需求 | 依赖机器ID配置，时钟回拨问题 | | ULID | 需要时间有序且可读的UUID | 128位兼容性不如 UUID | | NanoID | 短链、需要更紧凑的字符串 | 长度可变（默认21字符），安全性低于v4 UUID | --- UUID 是分布式系统、高安全场景、匿名标识等领域的基础工具，选择时需注意： 版本选择：优先使用 v4（随机）或 v7（时间有序）。 存储优化：二进制存储 + 索引优化提升性能。 安全合规：避免隐私泄露，必要时进行脱敏。",
    "url": "/消息队列相关/UUID.html",
    "lang": ""
  },
  {
    "title": "如何保证消息不丢失（可靠性）",
    "content": "--- title: 如何保证消息不丢失（可靠性） --- 消息的可靠性由三个部分保证： ==生产者保证可靠性：== 首先生产者在网络连接这块就有个连接重试机制，当连接超时的时候，会根据间隔进行重试。但是这个重试是阻塞的重试，会对性能会有影响，一般不建议打开。 为了保证消息不丢失，RabbitMQ提供了消息确认机制。一个是Publisher Return，一个是Publisher confirm publisher return是表示路由失败的时候，返回ack和异常消息。 publisher confirm是通过ack和nack来表示投递成功的信息 临时消息投递到了MQ，并且入队成功，会返回ACK。 持久消息投递到了MQ，并且入队成功并完成持久化，会返回ACK。 其他情况会返回NACK,告知投递失败。 ==MQ保证可靠性：== 设置消息的持久化属性，使消息 能够保存在磁盘上。在服务器宕机恢复之后重新恢复消息。并且使用事务来确保消息的持久化和发布的原子性。只有消息被完全写入到磁盘上才确认发布成功。 可以设置队列为持久化，没有被消费的消息也会被持久化到磁盘中。 ==消费者保证可靠性==： 消费者可以通过确认机制，ack表示处理成功，nack表示处理失败，reject表示处理失败并拒绝消息。 ACK处理模式可以设置三种模式： none表示不处理。消息投递给消费者后立刻ack,消息会立刻从MQ中删除。非常不安全 manual手动模式。主动在代码中调用api发送ack或者reject，存在业务入侵，但是更加安全。 auto自动模式。业务正常执行时自动返回ack,业务异常的话根据异常判断返回不同结果。 还存在失败本地重试机制，处理失败不需要将消息再次入队再次发送，而是直接在本地重试。重试达到最大次数后，此时会返回reject。",
    "url": "/消息队列相关/如何保证消息不丢失（可靠性）.html",
    "lang": ""
  },
  {
    "title": "幂等",
    "content": "--- title: 幂等 --- 幂等是 多次执行的影响和一次执行的影响是一样的。 !image-20241018152437048 方案一： 数据库select +insert+主键唯一索引冲突 根据唯一流水号bizSeq字段（uuid），先select一下数据库的流水表 如果数据已经存在，就拦截是重复请求，直接返回成功 如果数据不存在，就执行insert插入，如果Insert成功，则直接返回成功，如果insert产生主键冲突，则捕获异常，接着直接返回成功。 （存在业务侵入，需要提供写数据库操作和查数据库操作，影响性能 ） 方案二： 方案一会查一下流水表，但是如果重复的概率比较低，我们可以直接插入请求，利用主键/唯一索引冲突，去判断是重复请求。 方案三： 状态机幂等 直接通过业务来判断，给表中加一个状态字段，写sql的时候带上这个状态利用where语句设置 0-待处理，1-处理中，2-成功，3-失败 ==sql语句： update transferflow set status=2 where bizseq='666' and status=1;== 影响行数是否等于1？如果等于1代表正常处理请求，如果不等于1，也返回成功 方案四： 使用token机制，服务器在表单提交前生成一个token，发给前端，前端提交的时候携带这个token，然后到服务器中的redis中进行判断是否存在，如果存在，将这个token删掉，那么之后的重复提交由于这个token已经被删除，就不能往后执行了。 方案五： 单独搞个防重表，利用主键索引唯一性，如果插入防重表冲突直接返回成功，如果插入成功，去处理请求。 方案六： 利用数据库悲观锁，select ... for update （不推荐的思路） 如果查到的状态是处理中，就处理完业务，再更新状态为完成，如果不是处理中，就直接返回。 或者使用乐观锁的思想，利用版本号，第一次请求版本号一定是1. 方案七： 在后端利用redis使用setnx，设置key，当设置过了就会返回false，并且设置过期时间。",
    "url": "/消息队列相关/幂等.html",
    "lang": ""
  },
  {
    "title": "MQ消息积压",
    "content": "--- title: MQ消息积压 --- 消息积压的原因是==生产者的消息生产速度大于消费者的消费速度==，遇到这个问题的时候，需要排查具体的原因再提出解决方案 流量变大，而RabbitMQ服务器配置偏低，导致消息产生速度大于消费速度； 扩容即可。可以纵向扩容，即增加服务器资源，该加内存加内存，该加CPU加CPU。如果纵向扩容不方便，那就横向扩容，即将单机改为集群模式，增加集群节点，并且增加消费者数量，让消费速度快起来！原来是5个消费者，现在变成50个消费者！ 通过异步的方式来处理消息、或者通过批量处理的方式来消费。 采用==惰性队列==扩大交换器和消费者之间的消息可积压空间，正常队列把消息存放在内存中，可利用空间较小，==惰性队列接收到消息后直接存入磁盘而非内存==，要消费消息时才会从磁盘中读取并加载到内存，支持数百万数据的存储 消费者故障，从而消息只增不减； 通过查看日志搞清楚为什么消费者会故障，据我多年经验，发生此类问题大概率是程序代码写的不够完美，跑着跑着导致内存溢出，然后消费者进程被杀。要想永久解决此问题，需要结合日志分析程序代码，优化代码。临时解决方法是写监控脚本，如果发现消费者进程中断，需要重启服务！ 程序逻辑设计有问题，导致生产者持续生产消息，而消费者不消费或者消费慢； 总之就是程序逻辑问题，判断的方法也很简单，持续观察服务器的资源耗费情况，如果内存、CPU一切都正常，但就是队列持续增长，而消费速度非常慢。此时，就需要好好查查程序代码了。当然，可以尝试增加消费者数量，看看是否有好转。 相信，当我们发现消息积压时，想必问题已经比较严重了，或者说已经影响到业务正常运转了，那么当务之急肯定是需要先将业务恢复正常。对于上面第二种情况，直接重启相关服务，让消费者恢复正常。 ​ 除此之外，还有一种“断尾求生”的骚操作，就是新开一个队列，将新产生的消息到新队列里，消费者也到新队列里消费。而老的队列，则需要做一个异步处理，慢慢消费掉即可。 当然，如果积压的消息不怎么重要，可有可无的话，那干脆直接删除掉，这样大家都省事不是。",
    "url": "/消息队列相关/MQ消息积压.html",
    "lang": ""
  },
  {
    "title": "MQ消息重复消费",
    "content": "--- title: MQ消息重复消费 --- 在消费者这边做消息去重：在消费者端对接收到的消息进行去重操作，可以通过维护一个消息ID（这个消息ID是一个全局唯一的）的集合（去重表）或者使用唯一标识（查数据库）来判断是否已经消费过该消息。如果已经消费过，则不进行处理，避免重复消费。 消息确认机制：RabbitMQ提供了消息确认机制，可以确保消息被正确地消费。消费者在消费完消息之后，可以手动向RabbitMQ发送确认消息，告知RabbitMQ该消息已经成功消费，RabbitMQ会将该消息标记为已确认，然后删除消息队列中的该消息。",
    "url": "/消息队列相关/MQ消息重复消费.html",
    "lang": ""
  },
  {
    "title": "MQ消息顺序性",
    "content": "--- title: MQ消息顺序性 --- 业务中可能会存在多个消息需要顺序处理的情况，比如生成订单和扣减库存消息，那肯定先执行生成订单的操作，再执行扣减库存操作。但我们的项目一般都是集群部署的，一个队列就会有多个消费者，怎么实现一个队列中所有顺序消息只能有一个消费者消费呢？ RabbitMQ本身并不能直接保证消息的消费顺序，因为其是基于 AMQP 协议 的，它使用多个消费者在多个线程上并行地消费消息。但是可以采取以下方法间接实现消息的有序消费： !image-20241020144106735 单消费者对单队列：可以给 RabbitMQ 创建多个queue， 每个消费者只消费一个queue， 生产者==根据订单号，把订单号相同的消息放入同一个queue==。这样同一个订单号的消息就只会被同一个消费者顺序消费。 使用多个队列和分区键：可以将相同类型的消息发送到多个队列，并使用分区键 routingKey（例如基于消息的某个属性）决定消息要发送到哪个队列。然后，在消费者端，针对每个队列都使用单个消费者来保证消息的消费顺序。 消费者内部排序：为了减少网络延迟、消费者运行速度等影响，在消费者端，可以对接收到的消息进行排序，以确保按照特定的顺序进行处理。消费者可以在==内部维护一个消息缓冲区==，按照==某个属性或序列号对消息进行排序后再进行处理==。",
    "url": "/消息队列相关/MQ消息顺序性.html",
    "lang": ""
  },
  {
    "title": "重试机制为什么要mq",
    "content": "--- title: 重试机制为什么要mq --- 在系统设计中，重试机制的最佳实践之所以推荐通过异步消息队列（MQ）实现，核心原因在于解耦业务逻辑、避免资源阻塞、保证可靠性。以下是其原理、优势及实际场景的详细分析： ------ 场景示例：用户支付订单后，系统需要调用第三方支付接口，若调用失败需重试。 同步重试的缺陷： 阻塞主流程 ：重试逻辑嵌入业务代码，占用主线程资源，导致接口响应延迟。 java复制// 同步重试伪代码（问题：阻塞用户请求线程） boolean success = false; int retry = 0; while (!success && retry < 3) { success = callPaymentAPI(); retry++; } 重试风暴：高并发下，大量重试请求可能导致下游服务被压垮（如支付接口超载）。 状态维护复杂：需在业务代码中管理重试次数、间隔、回滚逻辑，代码臃肿。 ------ 核心思想：将失败操作发送到消息队列，由独立消费者异步处理，达到业务逻辑与容错机制的物理分离。 主业务逻辑： 调用支付接口失败后，立即将失败信息（订单号、错误原因）发送到MQ（如 RabbitMQ 死信队列、RocketMQ 定时消息）。 快速释放主线程资源，用户端立刻收到“支付处理中”响应，提升体验。 异步消费者： 从MQ拉取消息，按策略（如指数退避）重试。 成功：标记任务完成；失败：重新入队或进入死信队列人工处理。 | 维度 | 同步重试 | 异步重试（MQ） | | -------------- | ------------------------------ | ------------------------------ | | 响应速度 | 延迟高（用户需等待重试完成） | 延迟低（主线程快速返回） | | 系统耦合度 | 高（重试逻辑与业务代码强绑定） | 低（MQ隔离业务与重试） | | 下游保护 | 易引发重试风暴（并发冲击下游） | 消费速率可控（削峰填谷） | | 可维护性 | 修改重试策略需重启服务 | 动态调整消费者并发数和重试策略 | ------ 消息持久化： MQ 提供磁盘持久化，即使服务崩溃，重试任务不丢失。 流量控制： 通过消费者并发数和拉取速率限制，避免下游服务过载（如限制每秒查询支付接口 100 次）。 灵活路由： 支持优先级队列：优先处理重要订单（如VIP用户）。 死信队列（DLQ）：收集多次重试失败的任务，触发告警或人工介入。 跨服务解耦： 支付服务升级或替换时，只需调整消费者逻辑，无需修改订单系统代码。 ------ java复制// 订单服务发送重试消息 Message msg = new Message(\"PAYRETRYTOPIC\", orderId, JSON.toBytes(paymentInfo)); // 设置首次重试延迟10秒，后续逐步递增 msg.setDelayTimeLevel(3); // Level 3对应10秒 producer.send(msg); // 消费者重试逻辑 consumer.subscribe(\"PAYRETRYTOPIC\", (msg) -> { boolean success = retryPayment(msg); if (!success) { // 失败则重新投递（RocketMQ 自动递增延迟级别） msg.setReconsumeTimes(msg.getReconsumeTimes() + 1); return ConsumeConcurrentlyStatus.RECONSUMELATER; } }); 数据库表：存储需重试的任务（字段：任务ID、状态、下次重试时间、重试次数）。 定时调度：每隔5秒扫描待重试任务，调用支付接口。 缺点： 依赖数据库性能，高并发下可能成为瓶颈。 无原生流量控制，需自行实现速率限制。 ------ 对实时性要求不高（如订单支付、短信通知）。 下游服务不稳定（如第三方接口偶发超时）。 需严格避免资源竞争（如库存扣减的重试需保证幂等性）。 幂等设计：重试可能导致重复调用，下游服务需支持幂等（如订单ID去重）。 监控告警：对死信队列中的任务设置监控，避免任务堆积无人处理。 最终一致性：异步重试不保证严格实时，业务需容忍短暂不一致。 ------ 通过 MQ 实现异步重试的本质是将“错误处理”转化为“事件驱动”，其优势源于： 架构解耦：隔离业务逻辑与容错机制。 资源隔离：通过消息队列缓冲压力，保护核心服务。 弹性设计：可动态调整重试策略，适配不同故障场景。 这一模式是构建高可靠分布式系统的基石，尤其适合电商、金融等对稳定性和最终一致性要求较高的领域。",
    "url": "/消息队列相关/重试机制为什么要mq.html",
    "lang": ""
  },
  {
    "title": "CAP",
    "content": "--- title: CAP --- 以下来自 分布式的 CAP 定理和一致性模型 | 春水煎茶 (writings.sh) 对于一个数据存储系统，它具有以下美好的性质： 可用性： 收到请求总会响应。 一致性： 两次读请求，总会返回一致的结果。 在单节点系统中，达到这两条性质并不难 然而，单节点系统有以下问题： 服务能力有限。 单点风险： 一旦这个节点故障，整个系统将不可用。 我们可以使用主从结构，从节点不提供服务，只作为数据备份 网络分区发生的时候，无论同步还是异步的方式，如果不放弃可用性，都不能保证一致性。 CAP 定理 经过前面的一番讨论之后，所谓「CAP 定理」已经呼之欲出了。 CAP 定理又叫做布鲁斯定理, 该定理源于埃里克·布鲁尔在2000年的分布式计算原理研讨会上提出的一个猜想， 在2002年麻省理工学院的Gilbert和Lynch发表了证明[1]，使之成为一个定理： 这就是所谓「CAP」的名字由来，我们只能在 C,A,P 中三个性质中选两个。 这个定理也可以表达为： 在网络分区发生的情况下，分布式系统不能同时保证一致性和可用性。 除非是单节点系统， 否则无法同时保证 CA 这个结论在分布式领域已经耳熟能详， 不过， 关于 CAP 定理，需要特别注意搞清楚以下两点： CAP 定理范畴下的一致性、可用性、分区的概念都是 非常狭义的 。 CAP 所说的一致性其实是 线性一致性。 CAP 所说的可用性其实是非常高的可用性。 分区现象在现实网络中不可避免，也就是说，现实中我们只能在 C和 A中做选择。 CAP 其实在说 强一致性和高可用性，二者择其一 。 在前面的讨论中，我们曾多次提到「分区现象」， 正是这个问题导致我们无法同时具备一致性和可用性上。 要了解分区容忍性，就要先了解什么是分区现象。 当两个节点之间的网络不再连通，相当于分成了几块分区，所以叫做分区现象。 分区所强调的是 节点间的不连通问题，即使每个节点都可以工作 。 简单来说，Partition != Crash。 节点故障一般会造成节点的无响应，导致分区出现， 但是分区的出现不一定代表节点真的故障（宕机）了。 现实中的分布式网络，丢包、延迟、中断都是存在的， 所以在实际的系统设计中，分区容忍性是必选的。 也就是说，我们只能在可用性和一致性这两个性质中做选择。 先用最直观地方式理解下其含义： Availability = Reads and writes always succeed. 可用性并没有要求读到的数据的正确性，只描述了系统可以总可以非错的工作的能力。 也就是说，在 CAP 的范畴下，如果一个系统没有做好故障转移的逻辑， 那么这个系统不具备可用性。 CAP 定理中的可用性的定义，可以说很强，也可以说很弱： 很强，是因为要求必须响应 100% 的请求。 很弱，是因为并没有要求响应的时限，只需要最终返回。 而通常我们对系统的响应时限是有要求的， 所以说 CAP 定理中的可用性的条件很强， 在 CAP 的理论范畴下，没有「可用性的强弱程度」一说。 综合看来， CAP 定理中对可用性的定义是狭义的。 在实际中，可用性却不是一个非黑即白的简单判定， 和工程上对一致性的概念类似，存在不同可用性高低之分的说法。 我们常说的高可用性 high availability， 一般是指部分节点损失后整个系统仍可以正常工作。 要达成高可用性，就必须做好故障转移。 所以我们也可以说， CAP 定理中的可用性其实是我们所说的高可用性，而且是最高的可用性 。 简单直观地理解是： 分布式系统中多个节点的数据返回始终一致的性质。 Consistency = two reads return the same value. 所有节点在同一时刻的看到的数据是一样的。 CAP 定理中的一致性，要求所有节点都可以看到最新修改的数据。 其实这个要求是非常强的， 我们接下来会讨论一致性的模型， CAP 中的 C 就是强一致性中的线性一致性 。 一致性的分类 实际中，一致性也不是非黑即白的性质， 而是有强弱之分。 人们对一致性的研究和实践中， 按照强弱程度建立了一致性模型。 一致性按强弱程度可以分为三类： After a write, reads may or may not see it. 向系统写入一个值后，后续的读操作可能读出来，也可能读不出来。 After a write, reads will eventually see it. 向系统写入一个值后，后续立刻的读操作可能读不出来，但是在某个时间段后，读取一定成功。 例如，读写分离的关系型数据库。 After a write, reads will always see it. 向系统写入一个值后，后续任意时刻的读取一定成功。 在实际的分布式系统设计中，我们无法同时达成强一致性和高可用性。 CAP 定理是我们在设计一个分布式系统之初时的一个有益参考， 它让我们在设计系统的时候不必浪费时间去寻求理论上都达不成的目标。 CAP 范畴下的一致性和可用性的定义其实是非常狭义的、非常精确的， 它所讨论的是非常极限的情况， CAP 只否定了我们没办法同时达成最强的一致性和最高的可用性。 选择 C 多一些的情况 为了实现更强的一致性，这个系统可能会在某些情况下拒绝服务、甚至直接关闭节点。 实例中，比较偏好 C 的有 Zookeeper，Hbase，MongoDB 等。 选择 A 多一些的情况 为了实现更高的可用性，这个系统必须做好故障转移，尽可能地响应所有请求，但是有可能返回不一致的结果。 实例中，比较偏好A的是有D ynamo，CouchDB，Riak，Cassandra 等。 没有绝对的选择和放弃，只有更偏好哪个和弱化哪个的说法。",
    "url": "/分布式/CAP.html",
    "lang": ""
  },
  {
    "title": "分布式事务",
    "content": "--- title: 分布式事务 --- 分布式的意义在哪？ 分治，将并发量从一台机子分散到多台机子，间接的达到速度上的提升 一把锁也没有性能的提升（就算是分布式也还是串行化） 所以一般是一个订单业务对应一把锁 多把锁在多机情况下会有并行提升，不同的业务订单之间是并行的。 处理场景： 电商秒杀，外卖员接单，司机抢单",
    "url": "/分布式/分布式事务.html",
    "lang": ""
  },
  {
    "title": "分布式锁",
    "content": "--- title: 分布式锁 --- 两大类分布式锁 类cas自旋，询问方式，尝试加锁 event事件通知后续锁变化 分布式事务和分布式锁是分布式系统中两个重要的概念，它们在处理数据一致性和并发控制时发挥着关键作用。以下是对这两个概念的解释，以及在什么情况下应考虑使用它们。 分布式事务是指在多个数据库或服务之间执行的一组操作，这些操作需要保证原子性、一致性、隔离性和持久性（通常称为 ACID 属性）。在分布式环境中，由于涉及到多个参与者（如不同的数据库或微服务），确保事务的一致性和完整性就变得更加复杂。 两阶段提交协议（2PC）：这是最常用的分布式事务协议。它分为两个阶段： 准备阶段：协调者向所有参与者询问是否准备提交事务。 提交阶段：如果所有参与者都回复准备，就会提交事务；否则，事务将被回滚。 三阶段提交协议（3PC）：是为了提高 2PC 的可靠性而设计的第三种阶段，增加了一个不确定状态，从而减少了阻塞的可能性。 最终一致性：在某些场景中，可以接受最终一致性，而不是强一致性。这种情况下，可以使用异步的方法来处理事务，比如通过消息队列。 分布式锁是一种控制多个进程或线程对共享资源（如数据库、文件等）访问的机制。它确保在分布式系统中，同一时间只有一个进程或线程可以访问某个资源，从而避免并发冲突和数据不一致性。 基于 Redis：使用 Redis 的 SETNX 命令（即“如果不存在则设置”）来实现分布式锁。 Zookeeper：Zookeeper 提供了高可用的分布式锁实现，可以通过临时节点和序列节点来控制锁的获取和释放。 数据库悲观锁：通过数据库的锁机制来实现，但可能会影响性能。 需要强一致性的操作：当操作涉及多个服务或数据库，且需要保证数据在所有参与者中保持一致时，如金融交易、库存管理等。 复杂业务场景：在需要跨多个系统的复杂业务逻辑中，确保操作的原子性和一致性至关重要。 数据完整性要求高：在涉及用户数据、财务数据等敏感信息的场景中，确保准确性和一致性是必须的。 控制并发访问：当多个进程或线程需要访问共享资源时，使用分布式锁可以确保同一时间只有一个进程可以访问该资源。 避免数据冲突：在需要对同一数据进行写操作的场景中，分布式锁可以防止由于并发导致的数据不一致。 长时间执行的任务：如果某些操作需要较长时间完成，为了防止其他操作干预，可以使用分布式锁锁定资源。 分布式事务用于确保在多个服务或数据库之间的操作具有一致性和原子性，适合于强一致性的场景。 分布式锁用于控制对共享资源的并发访问，以避免冲突和不一致，适合于高并发和资源争用的场景。 在设计分布式系统时，选择使用分布式事务还是分布式锁取决于具体的业务需求、数据一致性要求和系统架构。",
    "url": "/分布式/分布式锁.html",
    "lang": ""
  },
  {
    "title": "一致性哈希",
    "content": "--- title: 一致性哈希 ---",
    "url": "/分布式/一致性哈希.html",
    "lang": ""
  },
  {
    "title": "两阶段提交和三阶段提交",
    "content": "--- title: 两阶段提交和三阶段提交 ---",
    "url": "/分布式/两阶段提交和三阶段提交.html",
    "lang": ""
  },
  {
    "title": "LCN解决方案",
    "content": "--- title: LCN解决方案 ---",
    "url": "/分布式/LCN解决方案.html",
    "lang": ""
  },
  {
    "title": "TCC解决方案",
    "content": "--- title: TCC解决方案 --- TCC采用的其实就是一种补偿机制 TRY CONFIRM CANCEL",
    "url": "/分布式/TCC解决方案.html",
    "lang": ""
  },
  {
    "title": "分布式ID生成器",
    "content": "--- title: 分布式ID生成器 --- 在我们业务数据量不大的时候，单库单表完全可以支撑现有业务，数据再大一点搞个MySQL主从同步读写分离也能对付。 但随着数据日渐增长，主从同步也扛不住了，就需要对数据库进行分库分表，但分库分表后需要有一个唯一ID来标识一条数据，数据库的自增ID显然不能满足需求；特别一点的如订单、优惠券也都需要有唯一ID做标识。此时一个能够生成全局唯一ID的系统是非常必要的。那么这个全局唯一ID就叫分布式ID。 全局唯一：必须保证ID是全局性唯一的，基本要求 高性能：高可用低延时，ID生成响应要快，否则反倒会成为业务瓶颈 高可用：100%的可用性是骗人的，但是也要无限接近于100%的可用性 好接入：要秉着拿来即用的设计原则，在系统设计和实现上要尽可能的简单 趋势递增：最好趋势递增，如果ID不是趋势递增，那么B+树为了维护ID的有序性，会频繁地在索引的中间位置插入节点，从而影响后面节点的位置，甚至频繁会导致页分裂，这对于性能的影响是极大的。这个要求就得看具体业务场景了，一般不严格要求 存储空间小，因为对于innodb引擎来说，普通索引会存储主键值，主键越大，每个page可以存储的数据就越少，访问磁盘IO次数就会增加。 UUID 数据库自增ID 数据库多主模式 号段模式 Redis 雪花算法（SnowFlake） 滴滴出品（TinyID） 百度 （Uidgenerator） 美团（Leaf） 在Java的世界里，想要得到一个具有唯一性的ID，首先被想到可能就是UUID，毕竟它有着全球唯一的特性。那么UUID可以做分布式ID吗？答案是可以的，但是并不推荐！ javascript public static void main(String[] args) { String uuid = UUID.randomUUID().toString().replaceAll(\"-\",\"\"); System.out.println(uuid); } UUID的生成简单到只有一行代码，输出结果 c2b8c2b9e46c47e3b30dca3b0d447718，但UUID却并不适用于实际的业务需求。像用作订单号UUID这样的字符串没有丝毫的意义，看不出和订单相关的有用信息；而对于数据库来说用作业务主键ID，它不仅是太长还是字符串，存储性能差查询也很耗时，所以不推荐用作分布式ID。 优点： 生成足够简单，本地生成无网络消耗，具有唯一性 缺点： 无序的字符串，不具备趋势自增特性 没有具体的业务含义 长度过长16 字节128位，36位长度的字符串，存储以及查询对MySQL的性能消耗较大，MySQL官方明确建议主键要尽量越短越好，作为数据库主键 UUID 的无序性会导致数据位置频繁变动，严重影响性能。 基于数据库的autoincrement自增ID完全可以充当分布式ID，具体实现：需要一个单独的MySQL实例用来生成ID，建表结构如下： javascript CREATE DATABASE SEQID; CREATE TABLE SEQID.SEQUENCEID ( id bigint(20) unsigned NOT NULL autoincrement, value char(10) NOT NULL default '', PRIMARY KEY (id), ) ENGINE=MyISAM; insert into SEQUENCEID(value) VALUES ('values'); 当我们需要一个ID的时候，向表中插入一条记录返回主键ID，但这种方式有一个比较致命的缺点，访问量激增时MySQL本身就是系统的瓶颈，用它来实现分布式服务风险比较大，不推荐！ 优点： 实现简单，ID单调自增，数值类型查询速度快 缺点： DB单点存在宕机风险，无法扛住高并发场景 前边说了单点数据库方式不可取，那对上边的方式做一些高可用优化，换成主从模式集群。害怕一个主节点挂掉没法用，那就做双主模式集群，也就是两个Mysql实例都能单独的生产自增ID。 那这样还会有个问题，两个MySQL实例的自增ID都从1开始，会生成重复的ID怎么办？ 解决方案：设置起始值和自增步长 MySQL1 配置： javascript set @@autoincrementoffset = 1; -- 起始值 set @@autoincrementincrement = 2; -- 步长 MySQL2 配置： javascript set @@autoincrementoffset = 2; -- 起始值 set @@autoincrementincrement = 2; -- 步长 这样两个MySQL实例的自增ID分别就是： > 1、3、5、7、9 > 2、4、6、8、10 那如果集群后的性能还是扛不住高并发咋办？就要进行MySQL扩容增加节点，这是一个比较麻烦的事。 !在这里插入图片描述 从上图可以看出，水平扩展的数据库集群，有利于解决数据库单点压力的问题，同时为了ID生成特性，将自增步长按照机器数量来设置。 增加第三台MySQL实例需要人工修改一、二两台MySQL实例的起始值和步长，把第三台机器的ID起始生成位置设定在比现有最大自增ID的位置远一些，但必须在一、二两台MySQL实例ID还没有增长到第三台MySQL实例的起始ID值的时候，否则自增ID就要出现重复了，必要时可能还需要停机修改。 优点： 解决DB单点问题 缺点： 不利于后续扩容，而且实际上单个数据库自身压力还是大，依旧无法满足高并发场景。 号段模式是当下分布式ID生成器的主流实现方式之一，号段模式可以理解为从数据库批量的获取自增ID，每次从数据库取出一个号段范围，例如 (1,1000] 代表1000个ID，具体的业务服务将本号段，生成11000的自增ID并加载到内存。表结构如下： javascript CREATE TABLE idgenerator ( id int(10) NOT NULL, maxid bigint(20) NOT NULL COMMENT '当前最大id', step int(20) NOT NULL COMMENT '号段的布长', biztype int(20) NOT NULL COMMENT '业务类型', version int(20) NOT NULL COMMENT '版本号', PRIMARY KEY (id) ) biztype ：代表不同业务类型 maxid ：当前最大的可用id step ：代表号段的长度 version ：是一个乐观锁，每次都更新version，保证并发时数据的正确性 | id | biztype | maxid | step | version | | ---- | -------- | ------ | ---- | ------- | | 1 | 101 | 1000 | 2000 | 0 | 等这批号段ID用完，再次向数据库申请新号段，对maxid字段做一次update操作，update maxid= maxid + step，update成功则说明新号段获取成功，新的号段范围是(maxid ,maxid +step]。 javascript update idgenerator set maxid = #{maxid+step}, version = version + 1 where version = # {version} and biztype = XXX 由于==多业务端可能同时操作，所以采用版本号version乐观锁方式更新==，这种分布式ID生成方式不强依赖于数据库，不会频繁的访问数据库，对数据库的压力小很多。 Redis也同样可以实现，原理就是利用redis的 incr命令实现ID的原子性自增。 javascript 127.0.0.1:6379> set seqid 1 // 初始化自增ID为1 OK 127.0.0.1:6379> incr seqid // 增加1，并返回递增后的数值 (integer) 2 用redis实现需要注意一点，要考虑到redis持久化的问题。redis有两种持久化方式RDB和AOF RDB会定时打一个快照进行持久化，假如连续自增但redis没及时持久化，而这会Redis挂掉了，重启Redis后会出现ID重复的情况。 AOF会对每条写命令进行持久化，即使Redis挂掉了也不会出现ID重复的情况，但由于incr命令的特殊性，会导致Redis重启恢复的数据时间过长。 雪花算法（Snowflake）是twitter公司内部分布式项目采用的ID生成算法，开源后广受国内大厂的好评，在该算法影响下各大公司相继开发出各具特色的分布式生成器。 !在这里插入图片描述 > 以上图片源自网络，如有侵权联系删除 Snowflake生成的是Long类型的ID，一个Long类型占8个字节，每个字节占8比特，也就是说一个Long类型占64个比特。 Snowflake ID组成结构：正数位（占1比特）+ 时间戳（占41比特）+ 机器ID（占5比特）+ 数据中心（占5比特）+ 自增值（占12比特），总共64比特组成的一个Long类型。 第一个bit位（1bit）：Java中long的最高位是符号位代表正负，正数是0，负数是1，一般生成ID都为正数，所以默认为0。 时间戳部分（41bit）：毫秒级的时间，不建议存当前时间戳，而是用（当前时间戳 - 固定开始时间戳）的差值，可以使产生的ID从更小的值开始；41位的时间戳可以使用69年，(1L << 41) / (1000L 60 60 24 365) = 69年 工作机器id（10bit）：也被叫做workId，这个可以灵活配置，机房或者机器号组合都可以。 序列号部分（12bit），自增值支持同一毫秒内同一个节点可以生成4096个ID 根据这个算法的逻辑，只需要将这个算法用Java语言实现出来，封装为一个工具方法，那么各个业务应用可以直接使用该工具方法来获取分布式ID，只需保证每个业务应用有自己的工作机器id即可，而不需要单独去搭建一个获取分布式ID的应用。 uid-generator是由百度技术部开发，项目GitHub地址 https://github.com/baidu/uid-... uid-generator是基于Snowflake算法实现的，与原始的snowflake算法不同在于，uid-generator支持自定义时间戳、工作机器ID和 序列号 等各部分的位数，而且uid-generator中采用用户自定义workId的生成策略。 uid-generator需要与数据库配合使用，需要新增一个WORKERNODE表。当应用启动时会向数据库表中去插入一条数据，插入成功后返回的自增ID就是该机器的workId数据由host，port组成。 对于uid-generator ID组成结构： workId，占用了22个bit位，时间占用了28个bit位，序列化占用了13个bit位，需要注意的是，和原始的snowflake不太一样，时间的单位是秒，而不是毫秒，workId也不一样，而且同一应用每次重启就会消费一个workId。 > 参考文献 > https://github.com/baidu/uid-... Leaf由美团开发，github地址：https://github.com/Meituan-Di... Leaf同时支持号段模式和snowflake算法模式，可以切换使用。 先导入源码 https://github.com/Meituan-Di... ，在建一张表leafalloc javascript DROP TABLE IF EXISTS leafalloc; CREATE TABLE leafalloc ( biztag varchar(128) NOT NULL DEFAULT '' COMMENT '业务key', maxid bigint(20) NOT NULL DEFAULT '1' COMMENT '当前已经分配了的最大id', step int(11) NOT NULL COMMENT '初始步长，也是动态调整的最小步长', description varchar(256) DEFAULT NULL COMMENT '业务key的描述', updatetime timestamp NOT NULL DEFAULT CURRENTTIMESTAMP ON UPDATE CURRENTTIMESTAMP COMMENT '数据库维护的更新时间', PRIMARY KEY (biztag) ) ENGINE=InnoDB; 然后在项目中开启号段模式，配置对应的数据库信息，并关闭snowflake模式 javascript leaf.name=com.sankuai.leaf.opensource.test leaf.segment.enable=true leaf.jdbc.url=jdbc:mysql://localhost:3306/leaftest?useUnicode=true&characterEncoding=utf8&characterSetResults=utf8 leaf.jdbc.username=root leaf.jdbc.password=root leaf.snowflake.enable=false 启动leaf-server 模块的 LeafServerApplication项目就跑起来了 号段模式获取分布式自增ID的测试url ：http：//localhost：8080/api/segment/get/leaf-segment-test 监控号段模式：http://localhost:8080/cache Leaf的snowflake模式依赖于ZooKeeper，不同于原始snowflake算法也主要是在workId的生成上，Leaf中workId是基于ZooKeeper的顺序Id来生成的，每个应用在使用Leaf-snowflake时，启动时都会都在Zookeeper中生成一个顺序Id，相当于一台机器对应一个顺序节点，也就是一个workId。 javascript leaf.snowflake.enable=true leaf.snowflake.zk.address=127.0.0.1 leaf.snowflake.port=2181 snowflake模式获取分布式自增ID的测试url：http://localhost:8080/api/snowflake/get/test",
    "url": "/分布式/分布式ID生成器.html",
    "lang": ""
  },
  {
    "title": "分布式幂等性",
    "content": "--- title: 分布式幂等性 ---",
    "url": "/分布式/分布式幂等性.html",
    "lang": ""
  },
  {
    "title": "BASE理论",
    "content": "--- title: BASE理论 --- 由于CAP中一致性C和可用性A无法兼得，eBay的架构师，提出了BASE理论，它是通过牺牲数据的强一致性，来获得可用性。 它由于如下3种特征。 Basically Available(基本可用):分布式系统在出现不可预知故障的时候，允许损失部分可用性，保证核心功能的可用。 Soft state(软状态):软状态也称为弱状态，和硬状态相对，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。 Eventually consistent(最终一致性):最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。 BASE理论并没有要求数据的强一致性，而是允许数据在一定的时间段内是不一致的，但在最终某个状态会达到一致。在生产环境中，很多公司，会采用BASE理论来实现数据的一致，因为产品的可用性相比强一致性来说，更加重要。例如在电商平台中，当用户对一个订单发起支付时，往往会调用第三方支付平台，例如支付宝支付或者微信支付，调用第三方成功后，第三方并不能及时通知我方系统，在第三方没有通知我方系统的这段时间内，给用户的订单状态显示支付中，等到第三方回调之后，再将状态改成已支付。虽然订单状态在短期内存在不一致，但是用户却获得了更好的产品体验。",
    "url": "/分布式/BASE理论.html",
    "lang": ""
  },
  {
    "title": "消息队列加本地事件表",
    "content": "--- title: 消息队列加本地事件表 --- !image-20241027144520016",
    "url": "/分布式/消息队列加本地事件表.html",
    "lang": ""
  },
  {
    "title": "可靠消息最终一致性方案",
    "content": "--- title: 可靠消息最终一致性方案 --- 可靠消息最终一致性方案。它是保证事务最终一致性的一种方案，允许数据在业务中出现短暂的不一致状态。 可靠消息最终一致性方案是指，当事务的发起方(事务参与者，也就是消息发送者)执行完本地事务后，同时发出一条消息，事务参与方(事务参与者，也就是消息的消费者)一定能够接收消息并可以成功处理自己的事务 这里面强调两点。 可靠消息:发起方一定得把消息传递到消费者 最终一致性:最终发起方的业务处理和消费方的业务处理得完成，达成最终一致。 上图是一个可靠消息服务最终一致性方案的流程图，从图中可以看出事务发起方将消息发送给消息中间件，事务消费方从消息中间件接收消息，事务发起方和消息中间件之间，事务消费方和消息中间件之间，都有网络通信,由于网络通信的不确定性，这块会导致数据的问题。下面针对导致的问题来分别进行解决。 (1)事务发起方本地事务和消息发送之间的原子性问题。 此问题是本地事务执行成功，消息必须发出去，否则丢弃消息，即本地事务执行成功和消息的发送成功，要么都成功，要么都失败。 来一段伪代码参考一下。 begin transaction; 发送消息; 操作数据库; commit transaction; 这种情况下，如果发送消息成功，数据库操作失败，则无法保证原子性。调换一下顺序。 begin transaction; 操作数据库; 发送消息; commit transaction: 这种情况下，如果操作数据库出错，回滚，不影响数据;如果发送消息出错，也回滚，不影响数据。这么一看似乎可以保证原子性，但是会有一种情况，发送消息响应超时，导致数据库回滚，但是消息已经发送成功了。这时原子性还是无法保证的，这个时候就需要人工补偿了。 (2)事务消费方和消息消费的原子性问题。 此时要保证事务消费方必须能接受到消息，如果由于程序故障，导致事务消费方重启，那么需要消息中间件要有消息重发机制;由于网络延时的存在，当事务消费方消费消息成功，没有向消息中间件响应（或者响应包丢失了）时，而消息中间件由于重发机制，会再次投递消息，就导致了消息重复消费的问题。此时在消费要有幂等性解决方案。",
    "url": "/分布式/可靠消息最终一致性方案.html",
    "lang": ""
  },
  {
    "title": "负载均衡",
    "content": "--- title: 负载均衡 --- 负载均衡（Load Balancing）是一种分布式处理技术，其主要目的是将来自客户端的请求均匀分配到多个服务器上，以提高整体系统的性能、可靠性和可用性。通过负载均衡，可以避免单个服务器成为性能瓶颈或单点故障。 随机方法： 随机中也可以设置加权随机 轮询方法：通过轮询的调度，但是不同的服务机可能性能不同。可能会出现资源浪费 所以引出了加权轮询算法，相当于a ,b c 三个实例 权值为 5,1,1.那么调度实例序列为{a,a,a,a,a,b,c} 但是这个调度不均匀，有平滑加权轮询， aa b aa c a 哈希取模和一致性hash：普通的哈希取模会出现增加节点后，原先映射的服务器节点会发生变化。有利于session的维护 一致性hash可以理解为一个环，这个环上放置不同的服务器节点，在哪个区间就打到哪个服务器节点上。 但可能造成不均匀的分布，大量请求集中某一个节点上。数据倾斜：大量的请求集中在少数的服务器上。 解决方法：新建更多的虚拟节点在中间，每个对应一个真实节点的映射。",
    "url": "/分布式/负载均衡.html",
    "lang": ""
  },
  {
    "title": "限流算法",
    "content": "--- title: 限流算法 --- 将计数器算法中的时间周期切分成多个小的窗口",
    "url": "/分布式/限流算法.html",
    "lang": ""
  },
  {
    "title": "数据库如何处理大数据",
    "content": "--- title: 数据库如何处理大数据 --- 对数据库进行： 分区，分库分表，主从架构（读写分离 分区是在同一张表中通过逻辑上的划分，将数据分散到不同的存储区域。 分表是将一个大表的数据拆分到多个物理表中，每个表存储一部分数据。",
    "url": "/分布式/数据库如何处理大数据.html",
    "lang": ""
  },
  {
    "title": "熔断和降级",
    "content": "--- title: 熔断和降级 --- 大家看一下下面的服务调用场景。C服务和D服务调用B服务，B服务调用A服务。在下面情况1中，服务正常调用。服务在运行过程中，A服务发生故障(网络延时，服务异常，负载过大无法及时响应)，系统变成了情况2由于B服务调用A服务，A服务出故障，导致B服务调用A的代码处也出故障，此时B服务也出故障了，系统变成了情况3。以此类推，系统最终发展成A、B、C、D所有的服务都出错了，整个系统崩塌了。这就是雪崩，如图所示。 雪崩效应 微服务系统之间通过互相调用来实现业务功能，但每个系统都无法百分之百保证自身运行不出问题。在服务调用中，很可能面临依赖服务失效的问题(网络延时，服务异常，负载过大无法及时响应)，导致服务雪崩，这对于个系统来说是灾难性的。因此需要一个组件，能提供强大的容错能力，当服务发生异常时，能提供保护和控制，把影响控制在较小范围内，不要造成所有服务的雪崩。 熔断开关： 开（走降级的方法，返回一个错误）关闭（正常的调用）半开（测试服务有没有恢复，将半开转为关闭） 从两方面来阐述。 (1)相似性: ①==目的一致==:都是从可用性和可靠性着想，为防止系统的整体响应缓慢甚至崩溃，而采用的技术手段。 ②==最终表现类似==:对于两者来说，最终让用户体验到的是某些功能暂时不可达或不可用。 ③粒度一致:都是服务级别的。 ④自治性要求很高:熔断模式一般都是服务基于策略的自动触发，降级虽说可人工干预，但在微服务架构下，完全靠人显然不可能，开关预置、配置中心都是必要手段。 (2)区别: ①触发原因不一样:服务熔断一般是某个服务(下游服务)故障引起，而服务降级一般是从整体负荷考虑，某些并发量非常大的时期，可以将某些不核心的服务暂时性的关闭或者使用静态方法替代，就是牺牲一些服务而使得性能提高。 ②管理目标的层次不一样:熔断是一个框架级的处理，每个服务都需要，而降级一般有业务层级之分，例如，降级一般在服务调用的上层处理。 !image-20241027164745118",
    "url": "/分布式/熔断和降级.html",
    "lang": ""
  },
  {
    "title": "注册中心",
    "content": "--- title: 注册中心 --- 服务名 服务信息 服务的IP地址，服务端口，服务对外提供的URL等order-service(订单服务)那在程序中如何存储呢? 从上面的存储内容和存储方式，是不是能想到Java中常用的数据结构Map?打开Eureka的源码，印证一下，在AbstractInstanceRegistryjava 中,有一个属性。private final ConcurrentHashMap<String, Map<String, Lease <InstanceInfo>>> registrynew ConcurrentHashMap<String, Map<String, Lease<InstanceInfo>>>();它的存储结构是用的ConcurrentHashMap，它的键值是服务名，值是个Map，而值的Map中键是服务实例ID值是租约(租约里面包括服务信息)。 key:value < 服务名:< 服务实例:ip+port，服务配置等等。> > 大家想一下，服务启动后，是否需要向注册中心进行注册?所以注册中心需要提供一个接口，让服务调用它来进行服务的登记注册，这就是注册中心的第一个功能，接受服务注册 服务注册完成后，注册中心得知道这个服务是否还是有效服务?所以此时需要服务定期地告诉注册中心，自己的工作状态(是否可用)。此时需要注册中心提供第二个功能，接受服务心跳。 当服务下线时，要通知注册中心自己要下线，注册中心需要提供对应的接口，来让服务调用。此时需要注册中心的第三个功能，接受服务下线。 如果服务挂了，没有及时通知注册中心，此时注册中心也发现服务，最近没有发送心跳。注册中心要主动剔除挂了的服务。此时需要注册中心第四个功能，服务剔除。 注册表中存储的信息，是要供其他服务查询的，就像通讯录一样，是要供主人查阅的，所以注册中心还需要第五个功能， 查询注册表中的服务信息。 一般微服务中，每个服务都要避免单点故障，注册中心也要做集群，所以还要涉及到注册中心间，注册信息的同步。这就是注册中心的第六个功能，注册中心集群间注册表的同步 如果让你开发，是不是也能开发出一个注册中心呢?其实本质就是一个web服务，提供上面分析的5个接口，供服务调用。这就是平时所说的 注册中心服务端。那对应的调用注册中心的服务业务服务)，一般称之为 注册中心客户端。",
    "url": "/分布式/注册中心.html",
    "lang": ""
  },
  {
    "title": "如何提高系统的并发能力",
    "content": "--- title: 如何提高系统的并发能力 --- 分流：负载均衡，消息队列，数据库拆分 导流：缓存，cdn 并行处理： 看具体业务",
    "url": "/分布式/如何提高系统的并发能力.html",
    "lang": ""
  },
  {
    "title": "如何进行服务划分",
    "content": "--- title: 如何进行服务划分 --- 如何准确识别系统的隔离点，也就是系统的边界。 单一职责原则。让每个服务能独立，有界限的工作，每个服务只关注自己的业务。做到高内聚，服务和服务之间做到低耦合。 服务自治原则。每个服务要能做到独立开发、独立测试、独立构建、独立部署，独立运行，与其他服务进 行解耦。 轻量级通信原则。让每个服务之间的调用是轻量级，并且能够跨平台、跨语言。例如采用RESTful风格，利用消息队列进行通信等。 粒度进化原则。对每个服务的粒度把控，其实没有统一的标准，这个得结合解决的具体业务问题。不要过度设计。服务的粒度随着业务和用户的发展而发展。 还有一句话别忘了:软件是为业务服务的，好的系统不是设计出来的，而是进化出来的。",
    "url": "/分布式/如何进行服务划分.html",
    "lang": ""
  },
  {
    "title": "服务治理",
    "content": "--- title: 服务治理 --- 服务治理是分布式系统（尤其是微服务架构）中的核心管理机制，其本质是通过一系列技术手段，确保服务间的协作高效、可靠、可观测，解决服务规模扩大后衍生的复杂性难题。以下是服务治理的核心概念、关键能力及实践场景的详解： ------ 可控性：确保服务调用的稳定性（如流量控制、熔断降级）。 可观测性：实时监控服务状态（如链路追踪、日志聚合）。 可维护性：简化服务管理（如动态配置、版本灰度发布）。 !服务治理核心功能示意图 ------ 问题背景：微服务动态扩缩容时，如何自动感知服务实例的IP和端口？ 解决方案 ： 服务启动时向注册中心（如 Nacos、Consul）注册自身信息。 调用方通过注册中心查询目标服务的可用实例列表。 示例 ： java复制// 服务注册（Nacos 示例） @Service public class UserService { @PostConstruct public void register() { Nacos.register(\"user-service\", \"192.168.1.100:8080\"); } } 策略类型 ： 随机轮询（Random） 加权轮询（根据服务器性能分配权重） 最小连接数（Least Connections） 实现方式 ： 客户端负载均衡（如 Ribbon 根据服务列表本地计算路由）。 服务端负载均衡（如 Nginx 反向代理）。 场景：防止突发流量压垮下游服务（如秒杀活动）。 工具 ： 限流：通过 Sentinel 或 Hystrix 限制 QPS（每秒请求数）。 熔断：当错误率超过阈值时，暂时阻断请求（类似电路保险丝）。 常见模式： 重试机制：对临时性错误（如网络抖动）自动重试。 降级策略：返回缓存数据或默认值，替代真实服务响应。 超时控制：避免长时间等待拖垮系统（如设置HTTP请求超时2秒）。 动态配置：无需重启服务，实时更新参数（如数据库连接池大小）。 工具：Apollo、Nacos Config。 示例 ： yaml复制# 应用从配置中心获取数据库地址 spring.datasource.url = ${nacos.config:mysql-url} 问题：跨服务调用复杂，如何快速定位性能瓶颈？ 方案 ： 通过 TraceID 串联全链路日志（如 OpenTelemetry、SkyWalking）。 可视化展示调用拓扑和耗时（如下图）。 !链路追踪可视化示例 ------ 需求：新版本服务上线前，先让部分用户试用。 实现：通过流量染色（如HTTP Header标记测试用户），将特定请求路由到新版本。 场景：A服务调用B服务，B服务宕机导致A服务线程池耗尽。 治理手段 ： 熔断器：快速失败，避免A服务资源耗尽。 服务隔离：为不同服务分配独立线程池（如Hystrix线程池隔离）。 问题：开发、测试、生产环境需切换不同配置。 方案 ： 使用配置中心按环境（Profile）分发配置。 敏感数据（如密码）通过加密存储。 ------ | 类别 | 开源方案 | 商业产品 | | -------- | ------------------------ | ------------- | | 注册中心 | Nacos、Consul、Zookeeper | AWS Cloud Map | | 配置中心 | Apollo、Nacos Config | AWS AppConfig | | 流量治理 | Sentinel、Istio | F5 BIG-IP | | 链路追踪 | SkyWalking、Jaeger | Datadog APM | ------ 随着微服务数量的增长，系统会面临三大核心挑战： 服务定位难：实例动态变化，手动维护IP列表不可行。 故障扩散快：单个服务宕机可能引发级联故障。 运维复杂度高：数百个服务的配置、监控需统一管理。 服务治理通过自动化和标准化解决上述问题，是构建高可用分布式系统的基石。 ------ 服务治理不是单一工具，而是一套覆盖服务全生命周期的管理体系，其价值体现在： 对开发人员：降低跨服务协作的复杂性。 对运维人员：提供故障快速定位和恢复能力。 对业务：保障用户体验，提升系统稳定性。 无论是自研中间件还是采用云原生方案（如Istio Service Mesh），理解服务治理的核心逻辑都是架构设计的必备能力。",
    "url": "/分布式/服务治理.html",
    "lang": ""
  },
  {
    "title": "seata",
    "content": "--- title: seata ---",
    "url": "/分布式/seata.html",
    "lang": ""
  },
  {
    "title": "付钱和扣库存的一致性",
    "content": "--- title: 付钱和扣库存的一致性 --- 在电子商务系统中，确保用户付款后再扣减库存的一致性是非常重要的。为了实现这一目标，可以考虑以下几种方法： 使用数据库事务可以确保在一个原子操作中完成付款和扣库存的操作。这意味着要么两者都成功，要么两者都失败，从而保持一致性。 步骤： 开始一个数据库事务。 检查库存是否充足。 扣除库存。 记录交易（付款信息）。 提交事务。 注意：要确保整个过程中没有其他操作干扰，比如并发请求的处理。 如果系统架构是分布式的，可以使用两阶段提交（2PC）或其他分布式事务管理方案来保证一致性。 步骤 ： 在付款服务和库存服务中分别进行准备操作。 如果两者都准备好，提交事务；否则，回滚。 在高并发场景中，可以采用最终一致性模型，例如使用消息队列来解耦付款和库存扣减的操作。 步骤： 用户付款后，发送一个消息到消息队列（例如“用户付款成功”）。 库存服务监听该消息队列，接收到消息后进行库存扣减。 处理失败的情况，如库存扣减失败时，可以通过重试机制或补偿机制进行处理。 在库存表中使用乐观锁控制并发更新。可以在库存记录中增加一个版本号，进行更新时检查版本号是否一致。 步骤 ： 用户发起付款，查询库存并获取当前版本号。 扣减库存时同时检查版本号，如果一致则更新；如果不一致，则说明有其他操作已修改库存，需要重试。 在一些情况下，可以采用补偿机制来处理不一致的状态。例如，在付款成功后，如果库存扣减失败，可以通过通知用户进行退款。 在处理过程中，可以引入“待处理”状态。例如，支付成功后，将订单状态设置为“待处理”，然后在后台异步处理库存扣减。 选择合适的方法取决于系统的架构、并发量以及业务需求。无论采用哪种方式，确保在设计时考虑到一致性、可用性和系统的复杂性都是非常重要的。",
    "url": "/分布式/付钱和扣库存的一致性.html",
    "lang": ""
  },
  {
    "title": "分库分表面经",
    "content": "--- title: 分库分表面经 --- 分库分表的常用中间件有哪些？ 有哪些问题中间件无法提供帮助，只能改写业务代码的场景？ 使用了什么中间件？ 分库分表的实现场景和方式有哪些？ 分表之后，要查询两个表的数据要怎么查？ 分库分表的优缺点是什么？ 分库分表业界有哪些替代方案？（提示：分布式文件系统，因为分库分表会出现降低QPS，比如range查询失效） 为什么做了分库分表后分页比较困难了？ 如果10亿数据要分表，要怎么分？业务怎么切？ 分库分表怎么保证数据一致性？ 选的什么分片键？什么分片算法？ 分库分表后的分布式ID怎么做？ （2025年目前为止的牛客面经关于分库分表的问题收集） 总结： 分布式事务一致性问题 跨节点关联查询JOIN问题（解决方案：1.全局表 2.冗余字段 3.建立1：1的ER实体关系分片） 非分片键的查询问题 全局分布式ID问题 跨库跨节点分页查询问题 参考面试回答：（吟唱） <strong>面试官：分库分表后、如何解决跨节点JOIN查询问题</strong> <span> <code>&amp;lt;参考回答：&amp;gt;</code></span> 分库分表后、跨节点 JOIN 查询会带来性能问题。 为了解决这个问题主要有以下几种方案： \\1. 全局表： 如果是一些数据量小、变动不频繁的基础数据（比如权限表、配置表、商品分类表）可以将它们复制到每个数据库节点。 这样查询时可以直接在本地 JOIN、避免跨库。 但需要保证全局表的数据同步。 \\2. 冗余字段： 如果经常需要 JOIN 某些字段、可以将这些字段冗余存储到需要查询的表中。 比如在订单表中冗余存储用户的姓名和地址。 这样查询订单信息时、就不用 JOIN 用户表了。 但需要保证冗余字段的数据一致性。 \\3. ER 分片： 如果表之间存在很强的关联关系、比如订单表和订单详情表、可以按照相同的规则进行分片、保证它们在同一个数据库节点上。 这样就可以避免跨库 JOIN。 （ER: 例如将订单表 和订单详情表按照 订单ID进行分片） 使用一致性哈希算法、将 订单ID映射到不同的数据库节点上。 关键： 保证具有相同 订单ID 的订单表记录和订单详情表记录、始终被分配到同一个数据库节点上。)",
    "url": "/分布式/分库分表面经.html",
    "lang": ""
  }
]